{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cbbada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.calibration import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Data\\k2pandc final.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bbcf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Load dataset\n",
    "# -------------------------------\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    print(df.info())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513772e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Handle missing values\n",
    "# -------------------------------\n",
    "def fill_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "\n",
    "    # Numerical → median\n",
    "    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "    # Categorical → mode\n",
    "    for col in cat_cols:\n",
    "        mode = df[col].mode()\n",
    "        df[col] = df[col].fillna(mode[0] if not mode.empty else 'Unknown')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Scale numerical features\n",
    "# -------------------------------\n",
    "def scale_numerical(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_scaled = df.copy()\n",
    "    num_cols = df_scaled.select_dtypes(include=['float64', 'int64']).columns\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled[num_cols] = scaler.fit_transform(df_scaled[num_cols])\n",
    "    return df_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee3046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. Encode categorical features\n",
    "# -------------------------------\n",
    "\n",
    "def encode_categorical(df: pd.DataFrame):\n",
    "    df_encoded = df.copy()\n",
    "    cat_cols = df_encoded.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "\n",
    "    le_dict = {}\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[col] = le.fit_transform(df_encoded[col].fillna('NaN_Label'))\n",
    "        le_dict[col] = le\n",
    "    \n",
    "    return df_encoded, le_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5. Feature selection (correlation + importance)\n",
    "# -------------------------------\n",
    "def select_features(df_encoded: pd.DataFrame, target_col: str, N: int = 20):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    X = df_encoded.drop(columns=[target_col])\n",
    "    y = df_encoded[target_col]\n",
    "\n",
    "    X = pd.get_dummies(X)  # one-hot if needed\n",
    "    X_train, _, y_train, _ = train_test_split(X, y, random_state=42)\n",
    "\n",
    "    # Correlation\n",
    "    corr_with_target = df_encoded.corr(numeric_only=True)[target_col].drop(target_col).abs()\n",
    "    top_corr = corr_with_target.sort_values(ascending=False).head(N).index.tolist()\n",
    "\n",
    "    # Random Forest importance\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    top_importance = importances.head(N).index.tolist()\n",
    "\n",
    "    # Combine\n",
    "    selected_features = list(set(top_corr + top_importance))\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41633fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 6. Build preprocessed dataframe\n",
    "# -------------------------------\n",
    "def build_preprocessed_df(df_encoded: pd.DataFrame, selected_features: list, target_col: str):\n",
    "    X = pd.get_dummies(df_encoded.drop(columns=[target_col]))\n",
    "    valuable_features_df = X[selected_features]\n",
    "\n",
    "    # Add target\n",
    "    preprocessed_df = valuable_features_df.copy()\n",
    "    preprocessed_df[target_col] = df_encoded[target_col]\n",
    "    return preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfca81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 7. Downsample classes\n",
    "# -------------------------------\n",
    "def downsample_classes(df: pd.DataFrame, target_col: str):\n",
    "    from sklearn.utils import resample\n",
    "\n",
    "    df_downsample = df.copy()\n",
    "    class_counts = df_downsample[target_col].value_counts()\n",
    "    min_count = class_counts.min()\n",
    "\n",
    "    dfs = []\n",
    "    for cls in class_counts.index:\n",
    "        cls_df = df_downsample[df_downsample[target_col] == cls]\n",
    "        cls_downsampled = resample(cls_df, replace=False, n_samples=min_count, random_state=42)\n",
    "        dfs.append(cls_downsampled)\n",
    "\n",
    "    df_downsampled = pd.concat(dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return df_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Data/cumulative_2025.10.03_07.59.03.csv\"\n",
    "target_col = \"koi_disposition\"\n",
    "\n",
    "df = load_dataset(path)\n",
    "df_filled = fill_missing_values(df)\n",
    "df_scaled, scaler = scale_numerical(df_filled)\n",
    "df_encoded, le_dict = encode_categorical(df_scaled)\n",
    "\n",
    "selected_features = select_features(df_encoded, target_col, N=20)\n",
    "preprocessed_df = build_preprocessed_df(df_encoded, selected_features, target_col)\n",
    "\n",
    "df_downsampled = downsample_classes(preprocessed_df, target_col)\n",
    "\n",
    "print(\"Final preprocessed dataset shape:\", df_downsampled.shape)\n",
    "print(df_downsampled.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0155321",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_downsampled.drop(columns=['koi_pdisposition']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=[target_col]),  # Features\n",
    "    df[target_col],                 # Target\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[target_col]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fit a Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c87e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Assume you have these (replace with your actual data)\n",
    "# X_train: np.array or torch.Tensor, shape [n_samples, input_size]\n",
    "# y_train: np.array or torch.Tensor, shape [n_samples], values 0-2\n",
    "# X_test, y_test: similar\n",
    "\n",
    "# Convert to tensors if not already\n",
    "X_train = torch.tensor(x_train.to_numpy(), dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_test = torch.tensor(x_test.to_numpy(), dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 32  # Adjust based on your data size and memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Compute class weights for imbalance (optional but recommended for recall)\n",
    "class_counts = np.bincount(y_train.numpy())\n",
    "class_weights = 1. / class_counts\n",
    "class_weights = torch.tensor(class_weights / class_weights.sum(), dtype=torch.float32)  # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805675d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_rate=0.3):  # <-- fixed __init__\n",
    "        super(ResidualBlock, self).__init__()  # <-- fixed __init__\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.ln = nn.LayerNorm(out_features)  # Layer norm for better stability\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Projection if dimensions differ\n",
    "        self.proj = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = self.proj(x)\n",
    "        x = F.silu(self.ln(self.fc(x)))  # Swish activation\n",
    "        x = self.dropout(x)\n",
    "        return x + residual  # Skip connection\n",
    "\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, input_size=16, num_classes=3):  # <-- fixed __init__\n",
    "        super(ResidualMLP, self).__init__()  # <-- fixed __init__\n",
    "        self.entry = nn.Linear(input_size, 256)  # Entry layer\n",
    "        \n",
    "        # Residual blocks for depth\n",
    "        self.res_block1 = ResidualBlock(256, 256)\n",
    "        self.res_block2 = ResidualBlock(256, 128)\n",
    "        self.res_block3 = ResidualBlock(128, 128)\n",
    "        self.res_block4 = ResidualBlock(128, 64)\n",
    "        \n",
    "        self.dropout_final = nn.Dropout(0.2)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.silu(self.entry(x))\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.res_block4(x)\n",
    "        \n",
    "        x = self.dropout_final(x)\n",
    "        x = self.fc_out(x)  # Logits output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current Device Index:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef817bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train.shape[1]  # number of features\n",
    "model = ResidualMLP(input_size=input_size, num_classes=3).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.005, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler (ReduceLROnPlateau equivalent)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.8, patience=6, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Early stopping setup\n",
    "patience = 20\n",
    "best_accuracy = 0.0\n",
    "epochs_no_improve = 0\n",
    "best_model_path = \"best_residual_mlp_model.pth\"\n",
    "\n",
    "# Training loop\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        \n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    # --- Evaluation on test set ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += batch_y.size(0)\n",
    "            correct_test += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "\n",
    "    # --- Scheduler step ---\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(test_accuracy)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr < old_lr:\n",
    "        print(f\"📉 Learning rate reduced from {old_lr:.6f} to {new_lr:.6f}\")\n",
    "\n",
    "    # --- Save best model ---\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ Saved new best model at epoch {epoch+1} with Test Accuracy: {best_accuracy:.2f}%\")\n",
    "        epochs_no_improve = 0  # reset patience counter\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # --- Logging ---\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} | \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n",
    "        f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"\\n🎯 Training finished. Best Test Accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- Evaluate on test set and generate classification report ---\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Generate report\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
