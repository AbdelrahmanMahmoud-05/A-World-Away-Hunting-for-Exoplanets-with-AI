{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2b88ca",
   "metadata": {},
   "source": [
    "# Kepler KOI Dataset Columns\n",
    "\n",
    "- **kepid**: KepID  \n",
    "- **kepoi_name**: KOI Name  \n",
    "- **kepler_name**: Kepler Name  \n",
    "- **koi_disposition**: Exoplanet Archive Disposition  \n",
    "- **koi_pdisposition**: Disposition Using Kepler Data  \n",
    "- **koi_score**: Disposition Score  \n",
    "- **koi_fpflag_nt**: Not Transit-Like False Positive Flag  \n",
    "- **koi_fpflag_ss**: Stellar Eclipse False Positive Flag  \n",
    "- **koi_fpflag_co**: Centroid Offset False Positive Flag  \n",
    "- **koi_fpflag_ec**: Ephemeris Match Indicates Contamination False Positive Flag  \n",
    "- **koi_period**: Orbital Period [days]  \n",
    "- **koi_period_err1**: Orbital Period Upper Uncertainty [days]  \n",
    "- **koi_period_err2**: Orbital Period Lower Uncertainty [days]  \n",
    "- **koi_time0bk**: Transit Epoch [BKJD]  \n",
    "- **koi_time0bk_err1**: Transit Epoch Upper Uncertainty [BKJD]  \n",
    "- **koi_time0bk_err2**: Transit Epoch Lower Uncertainty [BKJD]  \n",
    "- **koi_impact**: Impact Parameter  \n",
    "- **koi_impact_err1**: Impact Parameter Upper Uncertainty  \n",
    "- **koi_impact_err2**: Impact Parameter Lower Uncertainty  \n",
    "- **koi_duration**: Transit Duration [hrs]  \n",
    "- **koi_duration_err1**: Transit Duration Upper Uncertainty [hrs]  \n",
    "- **koi_duration_err2**: Transit Duration Lower Uncertainty [hrs]  \n",
    "- **koi_depth**: Transit Depth [ppm]  \n",
    "- **koi_depth_err1**: Transit Depth Upper Uncertainty [ppm]  \n",
    "- **koi_depth_err2**: Transit Depth Lower Uncertainty [ppm]  \n",
    "- **koi_prad**: Planetary Radius [Earth radii]  \n",
    "- **koi_prad_err1**: Planetary Radius Upper Uncertainty [Earth radii]  \n",
    "- **koi_prad_err2**: Planetary Radius Lower Uncertainty [Earth radii]  \n",
    "- **koi_teq**: Equilibrium Temperature [K]  \n",
    "- **koi_teq_err1**: Equilibrium Temperature Upper Uncertainty [K]  \n",
    "- **koi_teq_err2**: Equilibrium Temperature Lower Uncertainty [K]  \n",
    "- **koi_insol**: Insolation Flux [Earth flux]  \n",
    "- **koi_insol_err1**: Insolation Flux Upper Uncertainty [Earth flux]  \n",
    "- **koi_insol_err2**: Insolation Flux Lower Uncertainty [Earth flux]  \n",
    "- **koi_model_snr**: Transit Signal-to-Noise  \n",
    "- **koi_tce_plnt_num**: TCE Planet Number  \n",
    "- **koi_tce_delivname**: TCE Delivery  \n",
    "- **koi_steff**: Stellar Effective Temperature [K]  \n",
    "- **koi_steff_err1**: Stellar Effective Temperature Upper Uncertainty [K]  \n",
    "- **koi_steff_err2**: Stellar Effective Temperature Lower Uncertainty [K]  \n",
    "- **koi_slogg**: Stellar Surface Gravity [log10(cm/s²)]  \n",
    "- **koi_slogg_err1**: Stellar Surface Gravity Upper Uncertainty [log10(cm/s²)]  \n",
    "- **koi_slogg_err2**: Stellar Surface Gravity Lower Uncertainty [log10(cm/s²)]  \n",
    "- **koi_srad**: Stellar Radius [Solar radii]  \n",
    "- **koi_srad_err1**: Stellar Radius Upper Uncertainty [Solar radii]  \n",
    "- **koi_srad_err2**: Stellar Radius Lower Uncertainty [Solar radii]  \n",
    "- **ra**: Right Ascension [decimal degrees]  \n",
    "- **dec**: Declination [decimal degrees]  \n",
    "- **koi_kepmag**: Kepler-band Magnitude [mag]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1070a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8666a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f4de7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_rowid</th>\n",
       "      <th>kepid</th>\n",
       "      <th>kepoi_name</th>\n",
       "      <th>kepler_name</th>\n",
       "      <th>koi_disposition</th>\n",
       "      <th>koi_pdisposition</th>\n",
       "      <th>koi_score</th>\n",
       "      <th>koi_fpflag_nt</th>\n",
       "      <th>koi_fpflag_ss</th>\n",
       "      <th>koi_fpflag_co</th>\n",
       "      <th>...</th>\n",
       "      <th>koi_steff_err2</th>\n",
       "      <th>koi_slogg</th>\n",
       "      <th>koi_slogg_err1</th>\n",
       "      <th>koi_slogg_err2</th>\n",
       "      <th>koi_srad</th>\n",
       "      <th>koi_srad_err1</th>\n",
       "      <th>koi_srad_err2</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>koi_kepmag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10797460</td>\n",
       "      <td>K00752.01</td>\n",
       "      <td>Kepler-227 b</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-81.0</td>\n",
       "      <td>4.467</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>291.93423</td>\n",
       "      <td>48.141651</td>\n",
       "      <td>15.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10797460</td>\n",
       "      <td>K00752.02</td>\n",
       "      <td>Kepler-227 c</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-81.0</td>\n",
       "      <td>4.467</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>291.93423</td>\n",
       "      <td>48.141651</td>\n",
       "      <td>15.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10811496</td>\n",
       "      <td>K00753.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-176.0</td>\n",
       "      <td>4.544</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>297.00482</td>\n",
       "      <td>48.134129</td>\n",
       "      <td>15.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10848459</td>\n",
       "      <td>K00754.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-174.0</td>\n",
       "      <td>4.564</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>285.53461</td>\n",
       "      <td>48.285210</td>\n",
       "      <td>15.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10854555</td>\n",
       "      <td>K00755.01</td>\n",
       "      <td>Kepler-664 b</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-211.0</td>\n",
       "      <td>4.438</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>1.046</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>288.75488</td>\n",
       "      <td>48.226200</td>\n",
       "      <td>15.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9559</th>\n",
       "      <td>9560</td>\n",
       "      <td>10090151</td>\n",
       "      <td>K07985.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-166.0</td>\n",
       "      <td>4.529</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>297.18875</td>\n",
       "      <td>47.093819</td>\n",
       "      <td>14.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9560</th>\n",
       "      <td>9561</td>\n",
       "      <td>10128825</td>\n",
       "      <td>K07986.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-220.0</td>\n",
       "      <td>4.444</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>1.031</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>286.50937</td>\n",
       "      <td>47.163219</td>\n",
       "      <td>14.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9561</th>\n",
       "      <td>9562</td>\n",
       "      <td>10147276</td>\n",
       "      <td>K07987.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-236.0</td>\n",
       "      <td>4.447</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>1.041</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>294.16489</td>\n",
       "      <td>47.176281</td>\n",
       "      <td>15.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9562</th>\n",
       "      <td>9563</td>\n",
       "      <td>10155286</td>\n",
       "      <td>K07988.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-128.0</td>\n",
       "      <td>2.992</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>7.824</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-1.896</td>\n",
       "      <td>296.76288</td>\n",
       "      <td>47.145142</td>\n",
       "      <td>10.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9563</th>\n",
       "      <td>9564</td>\n",
       "      <td>10156110</td>\n",
       "      <td>K07989.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-225.0</td>\n",
       "      <td>4.385</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>1.193</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>297.00977</td>\n",
       "      <td>47.121021</td>\n",
       "      <td>14.826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9564 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      loc_rowid     kepid kepoi_name   kepler_name koi_disposition  \\\n",
       "0             1  10797460  K00752.01  Kepler-227 b       CONFIRMED   \n",
       "1             2  10797460  K00752.02  Kepler-227 c       CONFIRMED   \n",
       "2             3  10811496  K00753.01           NaN       CANDIDATE   \n",
       "3             4  10848459  K00754.01           NaN  FALSE POSITIVE   \n",
       "4             5  10854555  K00755.01  Kepler-664 b       CONFIRMED   \n",
       "...         ...       ...        ...           ...             ...   \n",
       "9559       9560  10090151  K07985.01           NaN  FALSE POSITIVE   \n",
       "9560       9561  10128825  K07986.01           NaN       CANDIDATE   \n",
       "9561       9562  10147276  K07987.01           NaN  FALSE POSITIVE   \n",
       "9562       9563  10155286  K07988.01           NaN       CANDIDATE   \n",
       "9563       9564  10156110  K07989.01           NaN  FALSE POSITIVE   \n",
       "\n",
       "     koi_pdisposition  koi_score  koi_fpflag_nt  koi_fpflag_ss  koi_fpflag_co  \\\n",
       "0           CANDIDATE      1.000              0              0              0   \n",
       "1           CANDIDATE      0.969              0              0              0   \n",
       "2           CANDIDATE      0.000              0              0              0   \n",
       "3      FALSE POSITIVE      0.000              0              1              0   \n",
       "4           CANDIDATE      1.000              0              0              0   \n",
       "...               ...        ...            ...            ...            ...   \n",
       "9559   FALSE POSITIVE      0.000              0              1              1   \n",
       "9560        CANDIDATE      0.497              0              0              0   \n",
       "9561   FALSE POSITIVE      0.021              0              0              1   \n",
       "9562        CANDIDATE      0.092              0              0              0   \n",
       "9563   FALSE POSITIVE      0.000              0              0              1   \n",
       "\n",
       "      ...  koi_steff_err2  koi_slogg  koi_slogg_err1  koi_slogg_err2  \\\n",
       "0     ...           -81.0      4.467           0.064          -0.096   \n",
       "1     ...           -81.0      4.467           0.064          -0.096   \n",
       "2     ...          -176.0      4.544           0.044          -0.176   \n",
       "3     ...          -174.0      4.564           0.053          -0.168   \n",
       "4     ...          -211.0      4.438           0.070          -0.210   \n",
       "...   ...             ...        ...             ...             ...   \n",
       "9559  ...          -166.0      4.529           0.035          -0.196   \n",
       "9560  ...          -220.0      4.444           0.056          -0.224   \n",
       "9561  ...          -236.0      4.447           0.056          -0.224   \n",
       "9562  ...          -128.0      2.992           0.030          -0.027   \n",
       "9563  ...          -225.0      4.385           0.054          -0.216   \n",
       "\n",
       "      koi_srad  koi_srad_err1  koi_srad_err2         ra        dec  koi_kepmag  \n",
       "0        0.927          0.105         -0.061  291.93423  48.141651      15.347  \n",
       "1        0.927          0.105         -0.061  291.93423  48.141651      15.347  \n",
       "2        0.868          0.233         -0.078  297.00482  48.134129      15.436  \n",
       "3        0.791          0.201         -0.067  285.53461  48.285210      15.597  \n",
       "4        1.046          0.334         -0.133  288.75488  48.226200      15.509  \n",
       "...        ...            ...            ...        ...        ...         ...  \n",
       "9559     0.903          0.237         -0.079  297.18875  47.093819      14.082  \n",
       "9560     1.031          0.341         -0.114  286.50937  47.163219      14.757  \n",
       "9561     1.041          0.341         -0.114  294.16489  47.176281      15.385  \n",
       "9562     7.824          0.223         -1.896  296.76288  47.145142      10.998  \n",
       "9563     1.193          0.410         -0.137  297.00977  47.121021      14.826  \n",
       "\n",
       "[9564 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Data\\cumulative_2025.10.03_07.59.03.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47754838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9564 entries, 0 to 9563\n",
      "Data columns (total 50 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   loc_rowid          9564 non-null   int64  \n",
      " 1   kepid              9564 non-null   int64  \n",
      " 2   kepoi_name         9564 non-null   object \n",
      " 3   kepler_name        2747 non-null   object \n",
      " 4   koi_disposition    9564 non-null   object \n",
      " 5   koi_pdisposition   9564 non-null   object \n",
      " 6   koi_score          8054 non-null   float64\n",
      " 7   koi_fpflag_nt      9564 non-null   int64  \n",
      " 8   koi_fpflag_ss      9564 non-null   int64  \n",
      " 9   koi_fpflag_co      9564 non-null   int64  \n",
      " 10  koi_fpflag_ec      9564 non-null   int64  \n",
      " 11  koi_period         9564 non-null   float64\n",
      " 12  koi_period_err1    9110 non-null   float64\n",
      " 13  koi_period_err2    9110 non-null   float64\n",
      " 14  koi_time0bk        9564 non-null   float64\n",
      " 15  koi_time0bk_err1   9110 non-null   float64\n",
      " 16  koi_time0bk_err2   9110 non-null   float64\n",
      " 17  koi_impact         9201 non-null   float64\n",
      " 18  koi_impact_err1    9110 non-null   float64\n",
      " 19  koi_impact_err2    9110 non-null   float64\n",
      " 20  koi_duration       9564 non-null   float64\n",
      " 21  koi_duration_err1  9110 non-null   float64\n",
      " 22  koi_duration_err2  9110 non-null   float64\n",
      " 23  koi_depth          9201 non-null   float64\n",
      " 24  koi_depth_err1     9110 non-null   float64\n",
      " 25  koi_depth_err2     9110 non-null   float64\n",
      " 26  koi_prad           9201 non-null   float64\n",
      " 27  koi_prad_err1      9201 non-null   float64\n",
      " 28  koi_prad_err2      9201 non-null   float64\n",
      " 29  koi_teq            9201 non-null   float64\n",
      " 30  koi_teq_err1       0 non-null      float64\n",
      " 31  koi_teq_err2       0 non-null      float64\n",
      " 32  koi_insol          9243 non-null   float64\n",
      " 33  koi_insol_err1     9243 non-null   float64\n",
      " 34  koi_insol_err2     9243 non-null   float64\n",
      " 35  koi_model_snr      9201 non-null   float64\n",
      " 36  koi_tce_plnt_num   9218 non-null   float64\n",
      " 37  koi_tce_delivname  9218 non-null   object \n",
      " 38  koi_steff          9201 non-null   float64\n",
      " 39  koi_steff_err1     9096 non-null   float64\n",
      " 40  koi_steff_err2     9081 non-null   float64\n",
      " 41  koi_slogg          9201 non-null   float64\n",
      " 42  koi_slogg_err1     9096 non-null   float64\n",
      " 43  koi_slogg_err2     9096 non-null   float64\n",
      " 44  koi_srad           9201 non-null   float64\n",
      " 45  koi_srad_err1      9096 non-null   float64\n",
      " 46  koi_srad_err2      9096 non-null   float64\n",
      " 47  ra                 9564 non-null   float64\n",
      " 48  dec                9564 non-null   float64\n",
      " 49  koi_kepmag         9563 non-null   float64\n",
      "dtypes: float64(39), int64(6), object(5)\n",
      "memory usage: 3.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b9375",
   "metadata": {},
   "source": [
    "# ExoNet Feature Mapping (KOI Dataset → Model Usage)\n",
    "\n",
    "| KOI Column         | Description                                    | ExoNet Usage        |\n",
    "|--------------------|------------------------------------------------|---------------------|\n",
    "| **koi_period**     | Orbital Period [days]                          | Core Input Feature  |\n",
    "| **koi_duration**   | Transit Duration [hrs]                         | Core Input Feature  |\n",
    "| **koi_depth**      | Transit Depth [ppm]                            | Core Input Feature  |\n",
    "| **koi_prad**       | Planetary Radius [Earth radii]                 | Core Input Feature  |\n",
    "| **koi_model_snr**  | Transit Signal-to-Noise                        | Core Input Feature  |\n",
    "| **koi_time0bk**    | Transit Epoch [BKJD]                           | Auxiliary Input (for phase folding / alignment) |\n",
    "| **koi_impact**     | Impact Parameter                               | Auxiliary Input     |\n",
    "| **koi_steff**      | Stellar Effective Temperature [K]              | Context Feature     |\n",
    "| **koi_slogg**      | Stellar Surface Gravity [log10(cm/s²)]         | Context Feature     |\n",
    "| **koi_srad**       | Stellar Radius [Solar radii]                   | Context Feature     |\n",
    "| **koi_kepmag**     | Kepler-band Magnitude [mag]                    | Context Feature (affects noise/SNR) |\n",
    "| **koi_disposition**| Exoplanet Archive Disposition (Confirmed / Candidate / False Positive) | Label (Target) |\n",
    "| **koi_pdisposition** | Disposition Using Kepler Data                | Label / Validation Reference |\n",
    "| **koi_score**      | Disposition Score (confidence metric)          | Label Confidence / Training Weight |\n",
    "| **koi_fpflag_nt**  | Not Transit-Like False Positive Flag           | Label Metadata (helps refine negatives) |\n",
    "| **koi_fpflag_ss**  | Stellar Eclipse False Positive Flag            | Label Metadata      |\n",
    "| **koi_fpflag_co**  | Centroid Offset False Positive Flag            | Label Metadata      |\n",
    "| **koi_fpflag_ec**  | Ephemeris Match Contamination False Positive Flag | Label Metadata  |\n",
    "\n",
    "---\n",
    "\n",
    "## Notes for ExoNet\n",
    "- **Core Input Features** → go directly into the tabular branch of the model.  \n",
    "- **Auxiliary Input Features** → used during preprocessing (e.g., phase folding, alignment).  \n",
    "- **Context Features** → provide stellar/observational context for classification (help distinguish planet vs binary vs noise).  \n",
    "- **Labels / Metadata** → used for supervised training, calibration, and evaluation.  \n",
    "\n",
    "Final model pipeline:  \n",
    "- Tabular features (`period`, `duration`, `depth`, `radius`, `SNR`, + stellar context)  \n",
    "- CNN embeddings from folded light curves  \n",
    "- Fusion layer → classifier → outputs **Calibrated Probabilities (planet / candidate / false positive)**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a36e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected shape: (9564, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>koi_period</th>\n",
       "      <th>koi_duration</th>\n",
       "      <th>koi_depth</th>\n",
       "      <th>koi_prad</th>\n",
       "      <th>koi_model_snr</th>\n",
       "      <th>koi_time0bk</th>\n",
       "      <th>koi_impact</th>\n",
       "      <th>koi_steff</th>\n",
       "      <th>koi_slogg</th>\n",
       "      <th>koi_srad</th>\n",
       "      <th>koi_kepmag</th>\n",
       "      <th>koi_disposition</th>\n",
       "      <th>koi_pdisposition</th>\n",
       "      <th>koi_score</th>\n",
       "      <th>koi_fpflag_nt</th>\n",
       "      <th>koi_fpflag_ss</th>\n",
       "      <th>koi_fpflag_co</th>\n",
       "      <th>koi_fpflag_ec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.488036</td>\n",
       "      <td>2.95750</td>\n",
       "      <td>615.8</td>\n",
       "      <td>2.26</td>\n",
       "      <td>35.8</td>\n",
       "      <td>170.538750</td>\n",
       "      <td>0.146</td>\n",
       "      <td>5455.0</td>\n",
       "      <td>4.467</td>\n",
       "      <td>0.927</td>\n",
       "      <td>15.347</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.418383</td>\n",
       "      <td>4.50700</td>\n",
       "      <td>874.8</td>\n",
       "      <td>2.83</td>\n",
       "      <td>25.8</td>\n",
       "      <td>162.513840</td>\n",
       "      <td>0.586</td>\n",
       "      <td>5455.0</td>\n",
       "      <td>4.467</td>\n",
       "      <td>0.927</td>\n",
       "      <td>15.347</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.899140</td>\n",
       "      <td>1.78220</td>\n",
       "      <td>10829.0</td>\n",
       "      <td>14.60</td>\n",
       "      <td>76.3</td>\n",
       "      <td>175.850252</td>\n",
       "      <td>0.969</td>\n",
       "      <td>5853.0</td>\n",
       "      <td>4.544</td>\n",
       "      <td>0.868</td>\n",
       "      <td>15.436</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.736952</td>\n",
       "      <td>2.40641</td>\n",
       "      <td>8079.2</td>\n",
       "      <td>33.46</td>\n",
       "      <td>505.6</td>\n",
       "      <td>170.307565</td>\n",
       "      <td>1.276</td>\n",
       "      <td>5805.0</td>\n",
       "      <td>4.564</td>\n",
       "      <td>0.791</td>\n",
       "      <td>15.597</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.525592</td>\n",
       "      <td>1.65450</td>\n",
       "      <td>603.3</td>\n",
       "      <td>2.75</td>\n",
       "      <td>40.9</td>\n",
       "      <td>171.595550</td>\n",
       "      <td>0.701</td>\n",
       "      <td>6031.0</td>\n",
       "      <td>4.438</td>\n",
       "      <td>1.046</td>\n",
       "      <td>15.509</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   koi_period  koi_duration  koi_depth  koi_prad  koi_model_snr  koi_time0bk  \\\n",
       "0    9.488036       2.95750      615.8      2.26           35.8   170.538750   \n",
       "1   54.418383       4.50700      874.8      2.83           25.8   162.513840   \n",
       "2   19.899140       1.78220    10829.0     14.60           76.3   175.850252   \n",
       "3    1.736952       2.40641     8079.2     33.46          505.6   170.307565   \n",
       "4    2.525592       1.65450      603.3      2.75           40.9   171.595550   \n",
       "\n",
       "   koi_impact  koi_steff  koi_slogg  koi_srad  koi_kepmag koi_disposition  \\\n",
       "0       0.146     5455.0      4.467     0.927      15.347       CONFIRMED   \n",
       "1       0.586     5455.0      4.467     0.927      15.347       CONFIRMED   \n",
       "2       0.969     5853.0      4.544     0.868      15.436       CANDIDATE   \n",
       "3       1.276     5805.0      4.564     0.791      15.597  FALSE POSITIVE   \n",
       "4       0.701     6031.0      4.438     1.046      15.509       CONFIRMED   \n",
       "\n",
       "  koi_pdisposition  koi_score  koi_fpflag_nt  koi_fpflag_ss  koi_fpflag_co  \\\n",
       "0        CANDIDATE      1.000              0              0              0   \n",
       "1        CANDIDATE      0.969              0              0              0   \n",
       "2        CANDIDATE      0.000              0              0              0   \n",
       "3   FALSE POSITIVE      0.000              0              1              0   \n",
       "4        CANDIDATE      1.000              0              0              0   \n",
       "\n",
       "   koi_fpflag_ec  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns needed for ExoNet\n",
    "selected_columns = [\n",
    "    # Core features\n",
    "    \"koi_period\",\n",
    "    \"koi_duration\",\n",
    "    \"koi_depth\",\n",
    "    \"koi_prad\",\n",
    "    \"koi_model_snr\",\n",
    "\n",
    "    # Auxiliary features\n",
    "    \"koi_time0bk\",\n",
    "    \"koi_impact\",\n",
    "\n",
    "    # Stellar context\n",
    "    \"koi_steff\",\n",
    "    \"koi_slogg\",\n",
    "    \"koi_srad\",\n",
    "    \"koi_kepmag\",\n",
    "\n",
    "    # Labels / metadata\n",
    "    \"koi_disposition\",\n",
    "    \"koi_pdisposition\",\n",
    "    \"koi_score\",\n",
    "    \"koi_fpflag_nt\",\n",
    "    \"koi_fpflag_ss\",\n",
    "    \"koi_fpflag_co\",\n",
    "    \"koi_fpflag_ec\"\n",
    "]\n",
    "\n",
    "# Filter dataframe\n",
    "exo_df = df[selected_columns]\n",
    "\n",
    "# Quick look at the shape & first rows\n",
    "print(\"Selected shape:\", exo_df.shape)\n",
    "exo_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c1128d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3780, 16)\n",
      "Test shape: (945, 16)\n",
      "Class distribution:\n",
      " label\n",
      "1    2746\n",
      "0    1979\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "exo_df = exo_df.dropna(subset=[\"koi_disposition\"])  # must have a label\n",
    "exo_df = exo_df.fillna(df.mean(numeric_only=True))  # fill numeric NaNs with mean\n",
    "exo_df = exo_df[exo_df[\"koi_disposition\"] != \"FALSE POSITIVE\"]\n",
    "\n",
    "# 4. Encode labels (target variable)\n",
    "label_map = { \"CANDIDATE\": 0, \"CONFIRMED\": 1}\n",
    "exo_df[\"label\"] = exo_df[\"koi_disposition\"].map(label_map)\n",
    "\n",
    "# Drop raw label columns we don’t use directly\n",
    "exo_df = exo_df.drop(columns=[\"koi_disposition\", \"koi_pdisposition\"])\n",
    "\n",
    "# 5. Split features & labels\n",
    "X = exo_df.drop(columns=[\"label\"])\n",
    "y = exo_df[\"label\"]\n",
    "\n",
    "# 6. Identify numeric features to scale\n",
    "numeric_features = [\n",
    "    \"koi_period\", \"koi_duration\", \"koi_depth\", \"koi_prad\", \"koi_model_snr\",\n",
    "    \"koi_time0bk\", \"koi_impact\",\n",
    "    \"koi_steff\", \"koi_slogg\", \"koi_srad\", \"koi_kepmag\", \"koi_score\"\n",
    "]\n",
    "\n",
    "binary_features = [\"koi_fpflag_nt\", \"koi_fpflag_ss\", \"koi_fpflag_co\", \"koi_fpflag_ec\"]\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# Binary features stay as is\n",
    "\n",
    "# 7. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "print(\"Class distribution:\\n\", y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5acbc293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize class distribution for the target column \"label\"\n",
    "plt.figure(figsize=(6,4))\n",
    "y.value_counts().plot(kind='bar', color=['tomato', 'gold', 'dodgerblue'])\n",
    "plt.xticks(ticks=[0,1,2], labels=[\"FALSE POSITIVE\", \"CANDIDATE\", \"CONFIRMED\"], rotation=30)\n",
    "plt.title(\"Class Distribution for Target (label)\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0dabe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>koi_period</th>\n",
       "      <th>koi_duration</th>\n",
       "      <th>koi_depth</th>\n",
       "      <th>koi_prad</th>\n",
       "      <th>koi_model_snr</th>\n",
       "      <th>koi_time0bk</th>\n",
       "      <th>koi_impact</th>\n",
       "      <th>koi_steff</th>\n",
       "      <th>koi_slogg</th>\n",
       "      <th>koi_srad</th>\n",
       "      <th>koi_kepmag</th>\n",
       "      <th>koi_score</th>\n",
       "      <th>koi_fpflag_nt</th>\n",
       "      <th>koi_fpflag_ss</th>\n",
       "      <th>koi_fpflag_co</th>\n",
       "      <th>koi_fpflag_ec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3780.000000</td>\n",
       "      <td>3780.000000</td>\n",
       "      <td>3780.000000</td>\n",
       "      <td>3780.000000</td>\n",
       "      <td>3780.000000</td>\n",
       "      <td>3780.000000</td>\n",
       "      <td>3780.000000</td>\n",
       "      <td>3780.000000</td>\n",
       "      <td>3780.000000</td>\n",
       "      <td>3780.000000</td>\n",
       "      <td>3780.000000</td>\n",
       "      <td>3780.000000</td>\n",
       "      <td>3780.000000</td>\n",
       "      <td>3780.000000</td>\n",
       "      <td>3780.0</td>\n",
       "      <td>3780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.014298</td>\n",
       "      <td>0.006297</td>\n",
       "      <td>-0.013800</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>-0.003497</td>\n",
       "      <td>0.006232</td>\n",
       "      <td>-0.008807</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.014531</td>\n",
       "      <td>-0.009141</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>-0.001925</td>\n",
       "      <td>0.124074</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.064582</td>\n",
       "      <td>1.017868</td>\n",
       "      <td>0.673485</td>\n",
       "      <td>1.081572</td>\n",
       "      <td>1.018959</td>\n",
       "      <td>1.009571</td>\n",
       "      <td>0.869733</td>\n",
       "      <td>1.014624</td>\n",
       "      <td>0.977074</td>\n",
       "      <td>0.847045</td>\n",
       "      <td>1.001883</td>\n",
       "      <td>1.006093</td>\n",
       "      <td>7.563278</td>\n",
       "      <td>0.056262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.045483</td>\n",
       "      <td>-1.212872</td>\n",
       "      <td>-0.242777</td>\n",
       "      <td>-0.025596</td>\n",
       "      <td>-0.301788</td>\n",
       "      <td>-0.726325</td>\n",
       "      <td>-0.215777</td>\n",
       "      <td>-4.189430</td>\n",
       "      <td>-12.940346</td>\n",
       "      <td>-0.338373</td>\n",
       "      <td>-5.809355</td>\n",
       "      <td>-3.305244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.042687</td>\n",
       "      <td>-0.609232</td>\n",
       "      <td>-0.218363</td>\n",
       "      <td>-0.024928</td>\n",
       "      <td>-0.245317</td>\n",
       "      <td>-0.515017</td>\n",
       "      <td>-0.168701</td>\n",
       "      <td>-0.489836</td>\n",
       "      <td>-0.263882</td>\n",
       "      <td>-0.138522</td>\n",
       "      <td>-0.575330</td>\n",
       "      <td>-0.057563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.038485</td>\n",
       "      <td>-0.296909</td>\n",
       "      <td>-0.189938</td>\n",
       "      <td>-0.024531</td>\n",
       "      <td>-0.205456</td>\n",
       "      <td>-0.407264</td>\n",
       "      <td>-0.054256</td>\n",
       "      <td>0.171224</td>\n",
       "      <td>0.243301</td>\n",
       "      <td>-0.092128</td>\n",
       "      <td>0.220046</td>\n",
       "      <td>0.561274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.022765</td>\n",
       "      <td>0.258926</td>\n",
       "      <td>-0.126817</td>\n",
       "      <td>-0.023953</td>\n",
       "      <td>-0.104379</td>\n",
       "      <td>0.079791</td>\n",
       "      <td>0.084539</td>\n",
       "      <td>0.641735</td>\n",
       "      <td>0.568901</td>\n",
       "      <td>-0.014614</td>\n",
       "      <td>0.784144</td>\n",
       "      <td>0.580704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.848516</td>\n",
       "      <td>10.408300</td>\n",
       "      <td>7.837314</td>\n",
       "      <td>65.584268</td>\n",
       "      <td>27.913928</td>\n",
       "      <td>11.590424</td>\n",
       "      <td>34.755777</td>\n",
       "      <td>7.816660</td>\n",
       "      <td>2.832443</td>\n",
       "      <td>42.791239</td>\n",
       "      <td>2.472495</td>\n",
       "      <td>0.580704</td>\n",
       "      <td>465.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        koi_period  koi_duration    koi_depth     koi_prad  koi_model_snr  \\\n",
       "count  3780.000000   3780.000000  3780.000000  3780.000000    3780.000000   \n",
       "mean     -0.014298      0.006297    -0.013800    -0.000253      -0.003497   \n",
       "std       0.064582      1.017868     0.673485     1.081572       1.018959   \n",
       "min      -0.045483     -1.212872    -0.242777    -0.025596      -0.301788   \n",
       "25%      -0.042687     -0.609232    -0.218363    -0.024928      -0.245317   \n",
       "50%      -0.038485     -0.296909    -0.189938    -0.024531      -0.205456   \n",
       "75%      -0.022765      0.258926    -0.126817    -0.023953      -0.104379   \n",
       "max       0.848516     10.408300     7.837314    65.584268      27.913928   \n",
       "\n",
       "       koi_time0bk   koi_impact    koi_steff    koi_slogg     koi_srad  \\\n",
       "count  3780.000000  3780.000000  3780.000000  3780.000000  3780.000000   \n",
       "mean      0.006232    -0.008807     0.000305     0.014531    -0.009141   \n",
       "std       1.009571     0.869733     1.014624     0.977074     0.847045   \n",
       "min      -0.726325    -0.215777    -4.189430   -12.940346    -0.338373   \n",
       "25%      -0.515017    -0.168701    -0.489836    -0.263882    -0.138522   \n",
       "50%      -0.407264    -0.054256     0.171224     0.243301    -0.092128   \n",
       "75%       0.079791     0.084539     0.641735     0.568901    -0.014614   \n",
       "max      11.590424    34.755777     7.816660     2.832443    42.791239   \n",
       "\n",
       "        koi_kepmag    koi_score  koi_fpflag_nt  koi_fpflag_ss  koi_fpflag_co  \\\n",
       "count  3780.000000  3780.000000    3780.000000    3780.000000         3780.0   \n",
       "mean      0.004152    -0.001925       0.124074       0.003175            0.0   \n",
       "std       1.001883     1.006093       7.563278       0.056262            0.0   \n",
       "min      -5.809355    -3.305244       0.000000       0.000000            0.0   \n",
       "25%      -0.575330    -0.057563       0.000000       0.000000            0.0   \n",
       "50%       0.220046     0.561274       0.000000       0.000000            0.0   \n",
       "75%       0.784144     0.580704       0.000000       0.000000            0.0   \n",
       "max       2.472495     0.580704     465.000000       1.000000            0.0   \n",
       "\n",
       "       koi_fpflag_ec  \n",
       "count         3780.0  \n",
       "mean             0.0  \n",
       "std              0.0  \n",
       "min              0.0  \n",
       "25%              0.0  \n",
       "50%              0.0  \n",
       "75%              0.0  \n",
       "max              0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67daa4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1843    1\n",
       "2581    1\n",
       "6003    1\n",
       "9392    0\n",
       "2173    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c11e89b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8687830687830688\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fit a Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df0eaf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   CANDIDATE       0.84      0.85      0.84       396\n",
      "   CONFIRMED       0.89      0.88      0.89       549\n",
      "\n",
      "    accuracy                           0.87       945\n",
      "   macro avg       0.86      0.87      0.87       945\n",
      "weighted avg       0.87      0.87      0.87       945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"CANDIDATE\", \"CONFIRMED\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3753e0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Test accuracy: 0.7142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   CANDIDATE       0.79      0.43      0.56       396\n",
      "   CONFIRMED       0.69      0.92      0.79       549\n",
      "\n",
      "    accuracy                           0.71       945\n",
      "   macro avg       0.74      0.68      0.67       945\n",
      "weighted avg       0.73      0.71      0.69       945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Fit a Naive Bayes model\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_nb_pred = nb.predict(X_test)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "print(\"Naive Bayes Test accuracy:\", accuracy_score(y_test, y_nb_pred))\n",
    "print(classification_report(y_test, y_nb_pred, target_names=[ \"CANDIDATE\", \"CONFIRMED\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ce12941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep NN Test accuracy: 0.8391534391534392\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   CANDIDATE       0.81      0.81      0.81       396\n",
      "   CONFIRMED       0.86      0.86      0.86       549\n",
      "\n",
      "    accuracy                           0.84       945\n",
      "   macro avg       0.83      0.84      0.83       945\n",
      "weighted avg       0.84      0.84      0.84       945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Victus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a deep neural network model (multi-layer perceptron)\n",
    "dnn = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),  # 3 hidden layers, can adjust sizes\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "dnn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_dnn_pred = dnn.predict(X_test)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "print(\"Deep NN Test accuracy:\", accuracy_score(y_test, y_dnn_pred))\n",
    "print(classification_report(y_test, y_dnn_pred, target_names=[\"CANDIDATE\", \"CONFIRMED\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d96ec6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Assume you have these (replace with your actual data)\n",
    "# X_train: np.array or torch.Tensor, shape [n_samples, input_size]\n",
    "# y_train: np.array or torch.Tensor, shape [n_samples], values 0-2\n",
    "# X_test, y_test: similar\n",
    "\n",
    "# Convert to tensors if not already\n",
    "X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 16  # Adjust based on your data size and memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Compute class weights for imbalance (optional but recommended for recall)\n",
    "class_counts = np.bincount(y_train.numpy())\n",
    "class_weights = 1. / class_counts\n",
    "class_weights = torch.tensor(class_weights / class_weights.sum(), dtype=torch.float32)  # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "314416f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3780, 16])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24775b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_rate=0.3):  # <-- fixed __init__\n",
    "        super(ResidualBlock, self).__init__()  # <-- fixed __init__\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.ln = nn.LayerNorm(out_features)  # Layer norm for better stability\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Projection if dimensions differ\n",
    "        self.proj = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = self.proj(x)\n",
    "        x = F.silu(self.ln(self.fc(x)))  # Swish activation\n",
    "        x = self.dropout(x)\n",
    "        return x + residual  # Skip connection\n",
    "\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, input_size=16, num_classes=2):  # <-- fixed __init__\n",
    "        super(ResidualMLP, self).__init__()  # <-- fixed __init__\n",
    "        self.entry = nn.Linear(input_size, 256)  # Entry layer\n",
    "        \n",
    "        # Residual blocks for depth\n",
    "        self.res_block1 = ResidualBlock(256, 256)\n",
    "        self.res_block2 = ResidualBlock(256, 128)\n",
    "        self.res_block3 = ResidualBlock(128, 128)\n",
    "        self.res_block4 = ResidualBlock(128, 64)\n",
    "        \n",
    "        self.dropout_final = nn.Dropout(0.2)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.silu(self.entry(x))\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.res_block4(x)\n",
    "        \n",
    "        x = self.dropout_final(x)\n",
    "        x = self.fc_out(x)  # Logits output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a483b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1]\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(y_train)  # unique class labels\n",
    "print(\"Classes:\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aed47ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1]\n",
      "Class Weights: tensor([1.1939, 0.8603], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Ensure y_train is numpy\n",
    "if isinstance(y_train, torch.Tensor):\n",
    "    y_train_np = y_train.cpu().numpy()\n",
    "else:\n",
    "    y_train_np = np.array(y_train)\n",
    "\n",
    "# Get unique classes\n",
    "classes = np.unique(y_train_np)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train_np\n",
    ")\n",
    "\n",
    "# Convert to tensor for PyTorch\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "\n",
    "# ---- Focal Loss with class weights ----\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # can be class weights tensor\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)  # probability of true class\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "# ---- Initialize with class weights ----\n",
    "loss_fn = FocalLoss(alpha=class_weights, gamma=2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e2205d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Number of GPUs: 1\n",
      "Current Device Index: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current Device Index:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c3417b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model at epoch 1 with Test Accuracy: 78.73%\n",
      "Epoch 1/300 | Train Loss: 0.1590, Train Acc: 69.07% | Test Loss: 0.1393, Test Acc: 78.73%\n",
      "Epoch 2/300 | Train Loss: 0.1335, Train Acc: 73.60% | Test Loss: 0.1212, Test Acc: 74.39%\n",
      "✅ Saved new best model at epoch 3 with Test Accuracy: 80.42%\n",
      "Epoch 3/300 | Train Loss: 0.1259, Train Acc: 75.58% | Test Loss: 0.1162, Test Acc: 80.42%\n",
      "Epoch 4/300 | Train Loss: 0.1205, Train Acc: 76.35% | Test Loss: 0.1167, Test Acc: 79.68%\n",
      "Epoch 5/300 | Train Loss: 0.1160, Train Acc: 78.12% | Test Loss: 0.1081, Test Acc: 78.94%\n",
      "✅ Saved new best model at epoch 6 with Test Accuracy: 80.53%\n",
      "Epoch 6/300 | Train Loss: 0.1105, Train Acc: 79.31% | Test Loss: 0.1096, Test Acc: 80.53%\n",
      "✅ Saved new best model at epoch 7 with Test Accuracy: 82.86%\n",
      "Epoch 7/300 | Train Loss: 0.1112, Train Acc: 79.87% | Test Loss: 0.1033, Test Acc: 82.86%\n",
      "Epoch 8/300 | Train Loss: 0.1069, Train Acc: 80.48% | Test Loss: 0.1110, Test Acc: 82.75%\n",
      "Epoch 9/300 | Train Loss: 0.1050, Train Acc: 81.19% | Test Loss: 0.1019, Test Acc: 82.01%\n",
      "✅ Saved new best model at epoch 10 with Test Accuracy: 83.70%\n",
      "Epoch 10/300 | Train Loss: 0.1027, Train Acc: 81.80% | Test Loss: 0.1009, Test Acc: 83.70%\n",
      "Epoch 11/300 | Train Loss: 0.1005, Train Acc: 82.99% | Test Loss: 0.1041, Test Acc: 77.67%\n",
      "✅ Saved new best model at epoch 12 with Test Accuracy: 84.34%\n",
      "Epoch 12/300 | Train Loss: 0.0991, Train Acc: 82.54% | Test Loss: 0.0950, Test Acc: 84.34%\n",
      "Epoch 13/300 | Train Loss: 0.1011, Train Acc: 81.75% | Test Loss: 0.0950, Test Acc: 82.22%\n",
      "Epoch 14/300 | Train Loss: 0.0963, Train Acc: 83.54% | Test Loss: 0.0966, Test Acc: 80.95%\n",
      "✅ Saved new best model at epoch 15 with Test Accuracy: 84.87%\n",
      "Epoch 15/300 | Train Loss: 0.0987, Train Acc: 82.70% | Test Loss: 0.0952, Test Acc: 84.87%\n",
      "Epoch 16/300 | Train Loss: 0.0962, Train Acc: 82.62% | Test Loss: 0.1002, Test Acc: 83.49%\n",
      "Epoch 17/300 | Train Loss: 0.0942, Train Acc: 83.76% | Test Loss: 0.0949, Test Acc: 83.49%\n",
      "Epoch 18/300 | Train Loss: 0.0952, Train Acc: 83.31% | Test Loss: 0.0936, Test Acc: 81.38%\n",
      "Epoch 19/300 | Train Loss: 0.0942, Train Acc: 83.28% | Test Loss: 0.0999, Test Acc: 83.92%\n",
      "Epoch 20/300 | Train Loss: 0.0935, Train Acc: 83.92% | Test Loss: 0.0921, Test Acc: 82.86%\n",
      "Epoch 21/300 | Train Loss: 0.0938, Train Acc: 83.70% | Test Loss: 0.0936, Test Acc: 82.86%\n",
      "📉 Learning rate reduced from 0.001000 to 0.000500\n",
      "Epoch 22/300 | Train Loss: 0.0923, Train Acc: 84.15% | Test Loss: 0.0998, Test Acc: 79.47%\n",
      "Epoch 23/300 | Train Loss: 0.0886, Train Acc: 84.55% | Test Loss: 0.0909, Test Acc: 84.44%\n",
      "Epoch 24/300 | Train Loss: 0.0856, Train Acc: 85.77% | Test Loss: 0.0986, Test Acc: 81.38%\n",
      "Epoch 25/300 | Train Loss: 0.0862, Train Acc: 84.66% | Test Loss: 0.0906, Test Acc: 84.55%\n",
      "Epoch 26/300 | Train Loss: 0.0859, Train Acc: 85.71% | Test Loss: 0.0947, Test Acc: 82.65%\n",
      "Epoch 27/300 | Train Loss: 0.0847, Train Acc: 85.45% | Test Loss: 0.0927, Test Acc: 81.90%\n",
      "Epoch 28/300 | Train Loss: 0.0841, Train Acc: 85.48% | Test Loss: 0.0939, Test Acc: 84.13%\n",
      "📉 Learning rate reduced from 0.000500 to 0.000250\n",
      "Epoch 29/300 | Train Loss: 0.0854, Train Acc: 84.97% | Test Loss: 0.0922, Test Acc: 84.44%\n",
      "Epoch 30/300 | Train Loss: 0.0830, Train Acc: 85.69% | Test Loss: 0.0914, Test Acc: 82.33%\n",
      "Epoch 31/300 | Train Loss: 0.0816, Train Acc: 86.06% | Test Loss: 0.0924, Test Acc: 82.54%\n",
      "Epoch 32/300 | Train Loss: 0.0812, Train Acc: 86.30% | Test Loss: 0.0907, Test Acc: 84.44%\n",
      "Epoch 33/300 | Train Loss: 0.0805, Train Acc: 86.24% | Test Loss: 0.0915, Test Acc: 82.54%\n",
      "Epoch 34/300 | Train Loss: 0.0804, Train Acc: 86.16% | Test Loss: 0.0905, Test Acc: 84.55%\n",
      "Epoch 35/300 | Train Loss: 0.0798, Train Acc: 86.51% | Test Loss: 0.0919, Test Acc: 84.44%\n",
      "📉 Learning rate reduced from 0.000250 to 0.000125\n",
      "Epoch 36/300 | Train Loss: 0.0831, Train Acc: 85.71% | Test Loss: 0.0896, Test Acc: 84.76%\n",
      "Epoch 37/300 | Train Loss: 0.0779, Train Acc: 85.79% | Test Loss: 0.0908, Test Acc: 84.13%\n",
      "✅ Saved new best model at epoch 38 with Test Accuracy: 84.97%\n",
      "Epoch 38/300 | Train Loss: 0.0777, Train Acc: 86.51% | Test Loss: 0.0910, Test Acc: 84.97%\n",
      "Epoch 39/300 | Train Loss: 0.0796, Train Acc: 86.75% | Test Loss: 0.0896, Test Acc: 84.23%\n",
      "Epoch 40/300 | Train Loss: 0.0761, Train Acc: 86.80% | Test Loss: 0.0907, Test Acc: 82.96%\n",
      "Epoch 41/300 | Train Loss: 0.0779, Train Acc: 86.77% | Test Loss: 0.0905, Test Acc: 83.39%\n",
      "Epoch 42/300 | Train Loss: 0.0786, Train Acc: 87.04% | Test Loss: 0.0929, Test Acc: 83.07%\n",
      "Epoch 43/300 | Train Loss: 0.0773, Train Acc: 86.32% | Test Loss: 0.0905, Test Acc: 83.92%\n",
      "Epoch 44/300 | Train Loss: 0.0787, Train Acc: 86.85% | Test Loss: 0.0900, Test Acc: 84.23%\n",
      "📉 Learning rate reduced from 0.000125 to 0.000063\n",
      "Epoch 45/300 | Train Loss: 0.0782, Train Acc: 86.69% | Test Loss: 0.0918, Test Acc: 84.87%\n",
      "Epoch 46/300 | Train Loss: 0.0758, Train Acc: 87.30% | Test Loss: 0.0909, Test Acc: 84.66%\n",
      "Epoch 47/300 | Train Loss: 0.0757, Train Acc: 87.33% | Test Loss: 0.0909, Test Acc: 83.81%\n",
      "Epoch 48/300 | Train Loss: 0.0773, Train Acc: 87.09% | Test Loss: 0.0921, Test Acc: 84.13%\n",
      "Epoch 49/300 | Train Loss: 0.0774, Train Acc: 87.28% | Test Loss: 0.0905, Test Acc: 84.76%\n",
      "Epoch 50/300 | Train Loss: 0.0767, Train Acc: 87.09% | Test Loss: 0.0905, Test Acc: 84.55%\n",
      "Epoch 51/300 | Train Loss: 0.0750, Train Acc: 87.57% | Test Loss: 0.0924, Test Acc: 84.02%\n",
      "📉 Learning rate reduced from 0.000063 to 0.000031\n",
      "Epoch 52/300 | Train Loss: 0.0749, Train Acc: 87.20% | Test Loss: 0.0919, Test Acc: 84.55%\n",
      "Epoch 53/300 | Train Loss: 0.0753, Train Acc: 87.12% | Test Loss: 0.0921, Test Acc: 84.23%\n",
      "Epoch 54/300 | Train Loss: 0.0745, Train Acc: 87.17% | Test Loss: 0.0921, Test Acc: 84.34%\n",
      "Epoch 55/300 | Train Loss: 0.0746, Train Acc: 87.41% | Test Loss: 0.0923, Test Acc: 84.34%\n",
      "Epoch 56/300 | Train Loss: 0.0740, Train Acc: 87.06% | Test Loss: 0.0924, Test Acc: 84.44%\n",
      "Epoch 57/300 | Train Loss: 0.0763, Train Acc: 86.96% | Test Loss: 0.0917, Test Acc: 84.55%\n",
      "Epoch 58/300 | Train Loss: 0.0747, Train Acc: 87.38% | Test Loss: 0.0919, Test Acc: 84.44%\n",
      "📉 Learning rate reduced from 0.000031 to 0.000016\n",
      "Epoch 59/300 | Train Loss: 0.0752, Train Acc: 87.28% | Test Loss: 0.0925, Test Acc: 84.23%\n",
      "Epoch 60/300 | Train Loss: 0.0735, Train Acc: 87.43% | Test Loss: 0.0922, Test Acc: 84.55%\n",
      "Epoch 61/300 | Train Loss: 0.0744, Train Acc: 86.98% | Test Loss: 0.0926, Test Acc: 84.44%\n",
      "Epoch 62/300 | Train Loss: 0.0742, Train Acc: 87.46% | Test Loss: 0.0927, Test Acc: 84.55%\n",
      "Epoch 63/300 | Train Loss: 0.0738, Train Acc: 87.22% | Test Loss: 0.0922, Test Acc: 84.55%\n",
      "Epoch 64/300 | Train Loss: 0.0732, Train Acc: 87.72% | Test Loss: 0.0928, Test Acc: 84.44%\n",
      "Epoch 65/300 | Train Loss: 0.0747, Train Acc: 86.90% | Test Loss: 0.0924, Test Acc: 84.55%\n",
      "📉 Learning rate reduced from 0.000016 to 0.000008\n",
      "Epoch 66/300 | Train Loss: 0.0748, Train Acc: 87.67% | Test Loss: 0.0925, Test Acc: 84.34%\n",
      "Epoch 67/300 | Train Loss: 0.0747, Train Acc: 87.43% | Test Loss: 0.0927, Test Acc: 84.34%\n",
      "Epoch 68/300 | Train Loss: 0.0747, Train Acc: 86.96% | Test Loss: 0.0923, Test Acc: 84.76%\n",
      "Epoch 69/300 | Train Loss: 0.0748, Train Acc: 87.06% | Test Loss: 0.0924, Test Acc: 84.76%\n",
      "Epoch 70/300 | Train Loss: 0.0749, Train Acc: 87.49% | Test Loss: 0.0925, Test Acc: 84.55%\n",
      "Epoch 71/300 | Train Loss: 0.0731, Train Acc: 87.51% | Test Loss: 0.0926, Test Acc: 84.44%\n",
      "Epoch 72/300 | Train Loss: 0.0744, Train Acc: 87.25% | Test Loss: 0.0925, Test Acc: 84.87%\n",
      "📉 Learning rate reduced from 0.000008 to 0.000004\n",
      "Epoch 73/300 | Train Loss: 0.0749, Train Acc: 87.41% | Test Loss: 0.0926, Test Acc: 84.66%\n",
      "Epoch 74/300 | Train Loss: 0.0736, Train Acc: 87.65% | Test Loss: 0.0926, Test Acc: 84.76%\n",
      "Epoch 75/300 | Train Loss: 0.0737, Train Acc: 87.20% | Test Loss: 0.0927, Test Acc: 84.66%\n",
      "Epoch 76/300 | Train Loss: 0.0745, Train Acc: 87.78% | Test Loss: 0.0929, Test Acc: 84.76%\n",
      "Epoch 77/300 | Train Loss: 0.0735, Train Acc: 87.17% | Test Loss: 0.0926, Test Acc: 84.87%\n",
      "Epoch 78/300 | Train Loss: 0.0732, Train Acc: 87.49% | Test Loss: 0.0928, Test Acc: 84.55%\n",
      "Epoch 79/300 | Train Loss: 0.0737, Train Acc: 87.49% | Test Loss: 0.0928, Test Acc: 84.66%\n",
      "📉 Learning rate reduced from 0.000004 to 0.000002\n",
      "Epoch 80/300 | Train Loss: 0.0731, Train Acc: 87.72% | Test Loss: 0.0928, Test Acc: 84.66%\n",
      "Epoch 81/300 | Train Loss: 0.0739, Train Acc: 87.62% | Test Loss: 0.0928, Test Acc: 84.76%\n",
      "Epoch 82/300 | Train Loss: 0.0727, Train Acc: 87.80% | Test Loss: 0.0930, Test Acc: 84.66%\n",
      "Epoch 83/300 | Train Loss: 0.0739, Train Acc: 87.62% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 84/300 | Train Loss: 0.0726, Train Acc: 87.75% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 85/300 | Train Loss: 0.0730, Train Acc: 87.83% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 86/300 | Train Loss: 0.0734, Train Acc: 87.57% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "📉 Learning rate reduced from 0.000002 to 0.000001\n",
      "Epoch 87/300 | Train Loss: 0.0741, Train Acc: 87.88% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 88/300 | Train Loss: 0.0750, Train Acc: 87.86% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 89/300 | Train Loss: 0.0748, Train Acc: 87.43% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 90/300 | Train Loss: 0.0752, Train Acc: 87.06% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 91/300 | Train Loss: 0.0739, Train Acc: 87.54% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 92/300 | Train Loss: 0.0722, Train Acc: 88.02% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 93/300 | Train Loss: 0.0738, Train Acc: 87.49% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 94/300 | Train Loss: 0.0744, Train Acc: 87.67% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 95/300 | Train Loss: 0.0747, Train Acc: 86.96% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 96/300 | Train Loss: 0.0727, Train Acc: 87.59% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 97/300 | Train Loss: 0.0739, Train Acc: 87.86% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 98/300 | Train Loss: 0.0738, Train Acc: 87.72% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 99/300 | Train Loss: 0.0747, Train Acc: 87.38% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 100/300 | Train Loss: 0.0734, Train Acc: 87.67% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 101/300 | Train Loss: 0.0738, Train Acc: 87.01% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 102/300 | Train Loss: 0.0738, Train Acc: 87.54% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 103/300 | Train Loss: 0.0731, Train Acc: 87.78% | Test Loss: 0.0931, Test Acc: 84.66%\n",
      "Epoch 104/300 | Train Loss: 0.0729, Train Acc: 88.02% | Test Loss: 0.0930, Test Acc: 84.66%\n",
      "Epoch 105/300 | Train Loss: 0.0730, Train Acc: 87.38% | Test Loss: 0.0931, Test Acc: 84.66%\n",
      "Epoch 106/300 | Train Loss: 0.0747, Train Acc: 87.14% | Test Loss: 0.0930, Test Acc: 84.66%\n",
      "Epoch 107/300 | Train Loss: 0.0759, Train Acc: 87.46% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 108/300 | Train Loss: 0.0740, Train Acc: 87.25% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 109/300 | Train Loss: 0.0743, Train Acc: 87.12% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 110/300 | Train Loss: 0.0736, Train Acc: 87.70% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 111/300 | Train Loss: 0.0738, Train Acc: 87.59% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 112/300 | Train Loss: 0.0748, Train Acc: 87.80% | Test Loss: 0.0929, Test Acc: 84.76%\n",
      "Epoch 113/300 | Train Loss: 0.0733, Train Acc: 87.54% | Test Loss: 0.0929, Test Acc: 84.66%\n",
      "Epoch 114/300 | Train Loss: 0.0741, Train Acc: 87.28% | Test Loss: 0.0929, Test Acc: 84.87%\n",
      "Epoch 115/300 | Train Loss: 0.0724, Train Acc: 87.54% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 116/300 | Train Loss: 0.0735, Train Acc: 87.59% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 117/300 | Train Loss: 0.0730, Train Acc: 88.02% | Test Loss: 0.0930, Test Acc: 84.66%\n",
      "Epoch 118/300 | Train Loss: 0.0751, Train Acc: 87.38% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 119/300 | Train Loss: 0.0735, Train Acc: 87.65% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 120/300 | Train Loss: 0.0746, Train Acc: 87.46% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 121/300 | Train Loss: 0.0739, Train Acc: 87.88% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 122/300 | Train Loss: 0.0746, Train Acc: 87.67% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 123/300 | Train Loss: 0.0722, Train Acc: 87.59% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 124/300 | Train Loss: 0.0749, Train Acc: 87.35% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 125/300 | Train Loss: 0.0729, Train Acc: 87.49% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 126/300 | Train Loss: 0.0732, Train Acc: 87.75% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 127/300 | Train Loss: 0.0741, Train Acc: 87.33% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 128/300 | Train Loss: 0.0752, Train Acc: 87.38% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 129/300 | Train Loss: 0.0724, Train Acc: 87.33% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 130/300 | Train Loss: 0.0742, Train Acc: 87.65% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 131/300 | Train Loss: 0.0733, Train Acc: 87.49% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 132/300 | Train Loss: 0.0730, Train Acc: 87.80% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 133/300 | Train Loss: 0.0736, Train Acc: 87.80% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 134/300 | Train Loss: 0.0736, Train Acc: 87.30% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 135/300 | Train Loss: 0.0742, Train Acc: 87.57% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 136/300 | Train Loss: 0.0741, Train Acc: 87.72% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 137/300 | Train Loss: 0.0729, Train Acc: 87.59% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 138/300 | Train Loss: 0.0737, Train Acc: 87.78% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 139/300 | Train Loss: 0.0736, Train Acc: 87.49% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 140/300 | Train Loss: 0.0732, Train Acc: 87.67% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 141/300 | Train Loss: 0.0740, Train Acc: 87.12% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 142/300 | Train Loss: 0.0740, Train Acc: 87.96% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 143/300 | Train Loss: 0.0737, Train Acc: 87.65% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 144/300 | Train Loss: 0.0737, Train Acc: 87.65% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 145/300 | Train Loss: 0.0746, Train Acc: 87.06% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 146/300 | Train Loss: 0.0728, Train Acc: 87.59% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 147/300 | Train Loss: 0.0732, Train Acc: 87.38% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 148/300 | Train Loss: 0.0746, Train Acc: 87.17% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 149/300 | Train Loss: 0.0731, Train Acc: 87.41% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 150/300 | Train Loss: 0.0735, Train Acc: 87.99% | Test Loss: 0.0930, Test Acc: 84.97%\n",
      "Epoch 151/300 | Train Loss: 0.0745, Train Acc: 87.04% | Test Loss: 0.0930, Test Acc: 84.97%\n",
      "Epoch 152/300 | Train Loss: 0.0735, Train Acc: 87.30% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 153/300 | Train Loss: 0.0728, Train Acc: 87.35% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 154/300 | Train Loss: 0.0741, Train Acc: 87.22% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 155/300 | Train Loss: 0.0743, Train Acc: 87.22% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 156/300 | Train Loss: 0.0725, Train Acc: 87.65% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 157/300 | Train Loss: 0.0750, Train Acc: 86.98% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 158/300 | Train Loss: 0.0735, Train Acc: 87.46% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 159/300 | Train Loss: 0.0733, Train Acc: 88.15% | Test Loss: 0.0930, Test Acc: 84.97%\n",
      "Epoch 160/300 | Train Loss: 0.0732, Train Acc: 87.38% | Test Loss: 0.0930, Test Acc: 84.97%\n",
      "Epoch 161/300 | Train Loss: 0.0733, Train Acc: 87.33% | Test Loss: 0.0930, Test Acc: 84.97%\n",
      "Epoch 162/300 | Train Loss: 0.0736, Train Acc: 87.54% | Test Loss: 0.0930, Test Acc: 84.97%\n",
      "Epoch 163/300 | Train Loss: 0.0733, Train Acc: 87.51% | Test Loss: 0.0930, Test Acc: 84.97%\n",
      "Epoch 164/300 | Train Loss: 0.0743, Train Acc: 87.49% | Test Loss: 0.0930, Test Acc: 84.97%\n",
      "Epoch 165/300 | Train Loss: 0.0745, Train Acc: 87.22% | Test Loss: 0.0930, Test Acc: 84.97%\n",
      "Epoch 166/300 | Train Loss: 0.0743, Train Acc: 87.75% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 167/300 | Train Loss: 0.0729, Train Acc: 87.54% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 168/300 | Train Loss: 0.0724, Train Acc: 87.88% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 169/300 | Train Loss: 0.0744, Train Acc: 87.38% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 170/300 | Train Loss: 0.0735, Train Acc: 87.49% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 171/300 | Train Loss: 0.0743, Train Acc: 87.88% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 172/300 | Train Loss: 0.0728, Train Acc: 87.72% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 173/300 | Train Loss: 0.0747, Train Acc: 87.33% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 174/300 | Train Loss: 0.0749, Train Acc: 87.75% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 175/300 | Train Loss: 0.0758, Train Acc: 87.41% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 176/300 | Train Loss: 0.0730, Train Acc: 87.88% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 177/300 | Train Loss: 0.0737, Train Acc: 87.62% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 178/300 | Train Loss: 0.0733, Train Acc: 86.96% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 179/300 | Train Loss: 0.0734, Train Acc: 87.30% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 180/300 | Train Loss: 0.0737, Train Acc: 87.30% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 181/300 | Train Loss: 0.0744, Train Acc: 87.86% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 182/300 | Train Loss: 0.0745, Train Acc: 87.65% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 183/300 | Train Loss: 0.0747, Train Acc: 87.83% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 184/300 | Train Loss: 0.0740, Train Acc: 87.70% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 185/300 | Train Loss: 0.0736, Train Acc: 87.62% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 186/300 | Train Loss: 0.0737, Train Acc: 87.75% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 187/300 | Train Loss: 0.0740, Train Acc: 87.49% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 188/300 | Train Loss: 0.0725, Train Acc: 87.62% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 189/300 | Train Loss: 0.0743, Train Acc: 87.72% | Test Loss: 0.0930, Test Acc: 84.97%\n",
      "Epoch 190/300 | Train Loss: 0.0734, Train Acc: 87.80% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 191/300 | Train Loss: 0.0725, Train Acc: 88.23% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 192/300 | Train Loss: 0.0736, Train Acc: 87.51% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 193/300 | Train Loss: 0.0732, Train Acc: 87.54% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 194/300 | Train Loss: 0.0738, Train Acc: 87.70% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 195/300 | Train Loss: 0.0730, Train Acc: 87.65% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 196/300 | Train Loss: 0.0724, Train Acc: 87.57% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 197/300 | Train Loss: 0.0732, Train Acc: 87.33% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 198/300 | Train Loss: 0.0746, Train Acc: 87.35% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 199/300 | Train Loss: 0.0744, Train Acc: 87.17% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 200/300 | Train Loss: 0.0739, Train Acc: 88.07% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 201/300 | Train Loss: 0.0727, Train Acc: 87.65% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 202/300 | Train Loss: 0.0746, Train Acc: 87.67% | Test Loss: 0.0930, Test Acc: 84.76%\n",
      "Epoch 203/300 | Train Loss: 0.0749, Train Acc: 87.59% | Test Loss: 0.0930, Test Acc: 84.87%\n",
      "Epoch 204/300 | Train Loss: 0.0736, Train Acc: 87.38% | Test Loss: 0.0930, Test Acc: 84.97%\n",
      "Epoch 205/300 | Train Loss: 0.0730, Train Acc: 87.94% | Test Loss: 0.0931, Test Acc: 84.97%\n",
      "Epoch 206/300 | Train Loss: 0.0737, Train Acc: 87.86% | Test Loss: 0.0931, Test Acc: 84.97%\n",
      "Epoch 207/300 | Train Loss: 0.0736, Train Acc: 87.41% | Test Loss: 0.0931, Test Acc: 84.97%\n",
      "Epoch 208/300 | Train Loss: 0.0731, Train Acc: 87.28% | Test Loss: 0.0931, Test Acc: 84.97%\n",
      "Epoch 209/300 | Train Loss: 0.0726, Train Acc: 87.59% | Test Loss: 0.0931, Test Acc: 84.97%\n",
      "Epoch 210/300 | Train Loss: 0.0740, Train Acc: 87.78% | Test Loss: 0.0931, Test Acc: 84.97%\n",
      "Epoch 211/300 | Train Loss: 0.0748, Train Acc: 87.51% | Test Loss: 0.0931, Test Acc: 84.97%\n",
      "Epoch 212/300 | Train Loss: 0.0738, Train Acc: 87.41% | Test Loss: 0.0931, Test Acc: 84.97%\n",
      "Epoch 213/300 | Train Loss: 0.0727, Train Acc: 87.57% | Test Loss: 0.0931, Test Acc: 84.97%\n",
      "Epoch 214/300 | Train Loss: 0.0733, Train Acc: 87.41% | Test Loss: 0.0931, Test Acc: 84.97%\n",
      "Epoch 215/300 | Train Loss: 0.0731, Train Acc: 87.33% | Test Loss: 0.0931, Test Acc: 84.97%\n",
      "Epoch 216/300 | Train Loss: 0.0736, Train Acc: 87.99% | Test Loss: 0.0931, Test Acc: 84.97%\n",
      "Epoch 217/300 | Train Loss: 0.0742, Train Acc: 87.25% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 218/300 | Train Loss: 0.0732, Train Acc: 87.38% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 219/300 | Train Loss: 0.0722, Train Acc: 87.94% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 220/300 | Train Loss: 0.0729, Train Acc: 87.78% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 221/300 | Train Loss: 0.0735, Train Acc: 87.62% | Test Loss: 0.0932, Test Acc: 84.76%\n",
      "Epoch 222/300 | Train Loss: 0.0729, Train Acc: 87.67% | Test Loss: 0.0932, Test Acc: 84.76%\n",
      "Epoch 223/300 | Train Loss: 0.0737, Train Acc: 87.04% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 224/300 | Train Loss: 0.0737, Train Acc: 87.62% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 225/300 | Train Loss: 0.0730, Train Acc: 87.83% | Test Loss: 0.0931, Test Acc: 84.66%\n",
      "Epoch 226/300 | Train Loss: 0.0725, Train Acc: 87.70% | Test Loss: 0.0932, Test Acc: 84.66%\n",
      "Epoch 227/300 | Train Loss: 0.0740, Train Acc: 87.62% | Test Loss: 0.0932, Test Acc: 84.76%\n",
      "Epoch 228/300 | Train Loss: 0.0739, Train Acc: 87.99% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 229/300 | Train Loss: 0.0736, Train Acc: 87.78% | Test Loss: 0.0932, Test Acc: 84.66%\n",
      "Epoch 230/300 | Train Loss: 0.0729, Train Acc: 87.20% | Test Loss: 0.0932, Test Acc: 84.66%\n",
      "Epoch 231/300 | Train Loss: 0.0740, Train Acc: 87.38% | Test Loss: 0.0932, Test Acc: 84.66%\n",
      "Epoch 232/300 | Train Loss: 0.0746, Train Acc: 87.28% | Test Loss: 0.0931, Test Acc: 84.66%\n",
      "Epoch 233/300 | Train Loss: 0.0719, Train Acc: 87.54% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 234/300 | Train Loss: 0.0748, Train Acc: 87.91% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 235/300 | Train Loss: 0.0746, Train Acc: 87.62% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 236/300 | Train Loss: 0.0738, Train Acc: 87.30% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 237/300 | Train Loss: 0.0739, Train Acc: 87.35% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 238/300 | Train Loss: 0.0734, Train Acc: 87.62% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 239/300 | Train Loss: 0.0737, Train Acc: 87.91% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 240/300 | Train Loss: 0.0742, Train Acc: 87.88% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 241/300 | Train Loss: 0.0746, Train Acc: 87.17% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 242/300 | Train Loss: 0.0736, Train Acc: 87.78% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 243/300 | Train Loss: 0.0747, Train Acc: 87.80% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 244/300 | Train Loss: 0.0746, Train Acc: 87.43% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 245/300 | Train Loss: 0.0739, Train Acc: 87.35% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 246/300 | Train Loss: 0.0738, Train Acc: 87.35% | Test Loss: 0.0932, Test Acc: 84.76%\n",
      "Epoch 247/300 | Train Loss: 0.0732, Train Acc: 87.59% | Test Loss: 0.0932, Test Acc: 84.76%\n",
      "Epoch 248/300 | Train Loss: 0.0745, Train Acc: 87.59% | Test Loss: 0.0932, Test Acc: 84.87%\n",
      "Epoch 249/300 | Train Loss: 0.0734, Train Acc: 87.41% | Test Loss: 0.0932, Test Acc: 84.76%\n",
      "Epoch 250/300 | Train Loss: 0.0739, Train Acc: 87.43% | Test Loss: 0.0932, Test Acc: 84.87%\n",
      "Epoch 251/300 | Train Loss: 0.0743, Train Acc: 87.30% | Test Loss: 0.0932, Test Acc: 84.76%\n",
      "Epoch 252/300 | Train Loss: 0.0746, Train Acc: 87.35% | Test Loss: 0.0932, Test Acc: 84.87%\n",
      "Epoch 253/300 | Train Loss: 0.0737, Train Acc: 87.49% | Test Loss: 0.0932, Test Acc: 84.87%\n",
      "Epoch 254/300 | Train Loss: 0.0739, Train Acc: 87.12% | Test Loss: 0.0932, Test Acc: 84.87%\n",
      "Epoch 255/300 | Train Loss: 0.0746, Train Acc: 87.38% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 256/300 | Train Loss: 0.0731, Train Acc: 87.33% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 257/300 | Train Loss: 0.0730, Train Acc: 87.62% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 258/300 | Train Loss: 0.0742, Train Acc: 87.75% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 259/300 | Train Loss: 0.0737, Train Acc: 87.43% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 260/300 | Train Loss: 0.0738, Train Acc: 87.43% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 261/300 | Train Loss: 0.0734, Train Acc: 87.75% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 262/300 | Train Loss: 0.0735, Train Acc: 87.33% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 263/300 | Train Loss: 0.0735, Train Acc: 87.43% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 264/300 | Train Loss: 0.0726, Train Acc: 87.41% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 265/300 | Train Loss: 0.0745, Train Acc: 87.59% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 266/300 | Train Loss: 0.0731, Train Acc: 87.83% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 267/300 | Train Loss: 0.0736, Train Acc: 87.72% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 268/300 | Train Loss: 0.0733, Train Acc: 87.62% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 269/300 | Train Loss: 0.0737, Train Acc: 87.86% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 270/300 | Train Loss: 0.0749, Train Acc: 87.04% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 271/300 | Train Loss: 0.0728, Train Acc: 87.41% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 272/300 | Train Loss: 0.0745, Train Acc: 87.62% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 273/300 | Train Loss: 0.0726, Train Acc: 87.57% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 274/300 | Train Loss: 0.0747, Train Acc: 87.35% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 275/300 | Train Loss: 0.0736, Train Acc: 87.91% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 276/300 | Train Loss: 0.0745, Train Acc: 87.57% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 277/300 | Train Loss: 0.0737, Train Acc: 87.65% | Test Loss: 0.0931, Test Acc: 84.66%\n",
      "Epoch 278/300 | Train Loss: 0.0739, Train Acc: 87.72% | Test Loss: 0.0931, Test Acc: 84.66%\n",
      "Epoch 279/300 | Train Loss: 0.0734, Train Acc: 87.57% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 280/300 | Train Loss: 0.0744, Train Acc: 87.25% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 281/300 | Train Loss: 0.0743, Train Acc: 87.70% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 282/300 | Train Loss: 0.0734, Train Acc: 87.33% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 283/300 | Train Loss: 0.0730, Train Acc: 87.28% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 284/300 | Train Loss: 0.0728, Train Acc: 87.70% | Test Loss: 0.0932, Test Acc: 84.87%\n",
      "Epoch 285/300 | Train Loss: 0.0749, Train Acc: 87.28% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 286/300 | Train Loss: 0.0731, Train Acc: 87.35% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 287/300 | Train Loss: 0.0733, Train Acc: 87.65% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 288/300 | Train Loss: 0.0729, Train Acc: 88.04% | Test Loss: 0.0931, Test Acc: 84.87%\n",
      "Epoch 289/300 | Train Loss: 0.0741, Train Acc: 87.62% | Test Loss: 0.0931, Test Acc: 84.76%\n",
      "Epoch 290/300 | Train Loss: 0.0737, Train Acc: 87.17% | Test Loss: 0.0932, Test Acc: 84.76%\n",
      "Epoch 291/300 | Train Loss: 0.0727, Train Acc: 87.88% | Test Loss: 0.0932, Test Acc: 84.76%\n",
      "Epoch 292/300 | Train Loss: 0.0736, Train Acc: 87.35% | Test Loss: 0.0932, Test Acc: 84.76%\n",
      "Epoch 293/300 | Train Loss: 0.0721, Train Acc: 87.59% | Test Loss: 0.0932, Test Acc: 84.76%\n",
      "Epoch 294/300 | Train Loss: 0.0730, Train Acc: 87.80% | Test Loss: 0.0933, Test Acc: 84.76%\n",
      "Epoch 295/300 | Train Loss: 0.0740, Train Acc: 87.59% | Test Loss: 0.0933, Test Acc: 84.66%\n",
      "Epoch 296/300 | Train Loss: 0.0716, Train Acc: 87.91% | Test Loss: 0.0933, Test Acc: 84.66%\n",
      "Epoch 297/300 | Train Loss: 0.0738, Train Acc: 87.62% | Test Loss: 0.0933, Test Acc: 84.76%\n",
      "Epoch 298/300 | Train Loss: 0.0738, Train Acc: 87.70% | Test Loss: 0.0933, Test Acc: 84.76%\n",
      "Epoch 299/300 | Train Loss: 0.0737, Train Acc: 87.75% | Test Loss: 0.0932, Test Acc: 84.76%\n",
      "Epoch 300/300 | Train Loss: 0.0737, Train Acc: 87.59% | Test Loss: 0.0932, Test Acc: 84.76%\n",
      "\n",
      "🎯 Training finished. Best Test Accuracy: 84.97%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train.shape[1]  # number of features\n",
    "model = ResidualMLP(input_size=input_size, num_classes=2).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler (ReduceLROnPlateau equivalent)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=6, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Early stopping setup\n",
    "patience = 30\n",
    "best_accuracy = 0.0\n",
    "epochs_no_improve = 0\n",
    "best_model_path = \"best_residual_mlp_model.pth\"\n",
    "\n",
    "# Training loop\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        \n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    # --- Evaluation on test set ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += batch_y.size(0)\n",
    "            correct_test += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "\n",
    "    # --- Scheduler step ---\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(test_accuracy)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr < old_lr:\n",
    "        print(f\"📉 Learning rate reduced from {old_lr:.6f} to {new_lr:.6f}\")\n",
    "\n",
    "    # --- Save best model ---\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ Saved new best model at epoch {epoch+1} with Test Accuracy: {best_accuracy:.2f}%\")\n",
    "        epochs_no_improve = 0  # reset patience counter\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # --- Logging ---\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} | \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n",
    "        f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"\\n🎯 Training finished. Best Test Accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9267faed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7739    0.8990    0.8318       396\n",
      "           1     0.9175    0.8106    0.8607       549\n",
      "\n",
      "    accuracy                         0.8476       945\n",
      "   macro avg     0.8457    0.8548    0.8463       945\n",
      "weighted avg     0.8573    0.8476    0.8486       945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- Evaluate on test set and generate classification report ---\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Generate report\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49371989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Reverse map: from int → label\n",
    "label_map = { \"CANDIDATE\": 0, \"CONFIRMED\": 1}\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Put model in eval mode and collect predictions\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Create readable labels\n",
    "labels = [inv_label_map[i] for i in sorted(inv_label_map.keys())]\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
