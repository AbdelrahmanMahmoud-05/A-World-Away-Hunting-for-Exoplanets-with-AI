{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97065e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\victus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\victus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\victus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\victus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\victus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\victus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\victus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\victus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\victus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\victus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scikit-learn \n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8770e455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_rowid</th>\n",
       "      <th>toi</th>\n",
       "      <th>tid</th>\n",
       "      <th>tfopwg_disp</th>\n",
       "      <th>rastr</th>\n",
       "      <th>ra</th>\n",
       "      <th>decstr</th>\n",
       "      <th>dec</th>\n",
       "      <th>st_pmra</th>\n",
       "      <th>st_pmraerr1</th>\n",
       "      <th>...</th>\n",
       "      <th>st_logg</th>\n",
       "      <th>st_loggerr1</th>\n",
       "      <th>st_loggerr2</th>\n",
       "      <th>st_logglim</th>\n",
       "      <th>st_rad</th>\n",
       "      <th>st_raderr1</th>\n",
       "      <th>st_raderr2</th>\n",
       "      <th>st_radlim</th>\n",
       "      <th>toi_created</th>\n",
       "      <th>rowupdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1000.01</td>\n",
       "      <td>50365310</td>\n",
       "      <td>FP</td>\n",
       "      <td>07h29m25.85s</td>\n",
       "      <td>112.357708</td>\n",
       "      <td>-12d41m45.46s</td>\n",
       "      <td>-12.695960</td>\n",
       "      <td>-5.964</td>\n",
       "      <td>0.085</td>\n",
       "      <td>...</td>\n",
       "      <td>4.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0</td>\n",
       "      <td>2.169860</td>\n",
       "      <td>0.072573</td>\n",
       "      <td>-0.072573</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-07-24 15:58:33</td>\n",
       "      <td>2024-09-09 10:08:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1001.01</td>\n",
       "      <td>88863718</td>\n",
       "      <td>PC</td>\n",
       "      <td>08h10m19.31s</td>\n",
       "      <td>122.580465</td>\n",
       "      <td>-05d30m49.87s</td>\n",
       "      <td>-5.513852</td>\n",
       "      <td>-4.956</td>\n",
       "      <td>0.102</td>\n",
       "      <td>...</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0</td>\n",
       "      <td>2.010000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-07-24 15:58:33</td>\n",
       "      <td>2023-04-03 14:31:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1002.01</td>\n",
       "      <td>124709665</td>\n",
       "      <td>FP</td>\n",
       "      <td>06h58m54.47s</td>\n",
       "      <td>104.726966</td>\n",
       "      <td>-10d34m49.64s</td>\n",
       "      <td>-10.580455</td>\n",
       "      <td>-1.462</td>\n",
       "      <td>0.206</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5.730000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-07-24 15:58:33</td>\n",
       "      <td>2022-07-11 16:02:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1003.01</td>\n",
       "      <td>106997505</td>\n",
       "      <td>FP</td>\n",
       "      <td>07h22m14.39s</td>\n",
       "      <td>110.559945</td>\n",
       "      <td>-25d12m25.26s</td>\n",
       "      <td>-25.207017</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>0.041</td>\n",
       "      <td>...</td>\n",
       "      <td>4.15</td>\n",
       "      <td>1.64</td>\n",
       "      <td>-1.64</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-07-24 15:58:33</td>\n",
       "      <td>2022-02-23 10:10:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1004.01</td>\n",
       "      <td>238597883</td>\n",
       "      <td>FP</td>\n",
       "      <td>08h08m42.77s</td>\n",
       "      <td>122.178195</td>\n",
       "      <td>-48d48m10.12s</td>\n",
       "      <td>-48.802811</td>\n",
       "      <td>-4.496</td>\n",
       "      <td>0.069</td>\n",
       "      <td>...</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>-0.060000</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-07-24 15:58:33</td>\n",
       "      <td>2024-09-09 10:08:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7698</th>\n",
       "      <td>7699</td>\n",
       "      <td>995.01</td>\n",
       "      <td>317951248</td>\n",
       "      <td>FP</td>\n",
       "      <td>07h23m14.75s</td>\n",
       "      <td>110.811443</td>\n",
       "      <td>+05d33m46.26s</td>\n",
       "      <td>5.562850</td>\n",
       "      <td>2.061</td>\n",
       "      <td>0.405</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-07-24 15:58:33</td>\n",
       "      <td>2021-10-29 12:59:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7699</th>\n",
       "      <td>7700</td>\n",
       "      <td>996.01</td>\n",
       "      <td>142918609</td>\n",
       "      <td>FP</td>\n",
       "      <td>07h57m23.99s</td>\n",
       "      <td>119.349948</td>\n",
       "      <td>-19d30m57.65s</td>\n",
       "      <td>-19.516015</td>\n",
       "      <td>-3.900</td>\n",
       "      <td>0.848</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-07-24 15:58:33</td>\n",
       "      <td>2021-10-29 12:59:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7700</th>\n",
       "      <td>7701</td>\n",
       "      <td>997.01</td>\n",
       "      <td>341729521</td>\n",
       "      <td>FP</td>\n",
       "      <td>08h05m16.69s</td>\n",
       "      <td>121.319521</td>\n",
       "      <td>-59d34m47.27s</td>\n",
       "      <td>-59.579798</td>\n",
       "      <td>-44.770</td>\n",
       "      <td>0.044</td>\n",
       "      <td>...</td>\n",
       "      <td>4.52</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.926261</td>\n",
       "      <td>0.045789</td>\n",
       "      <td>-0.045789</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-07-24 15:58:33</td>\n",
       "      <td>2024-09-09 10:08:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7701</th>\n",
       "      <td>7702</td>\n",
       "      <td>998.01</td>\n",
       "      <td>54390047</td>\n",
       "      <td>FP</td>\n",
       "      <td>07h53m16.69s</td>\n",
       "      <td>118.319555</td>\n",
       "      <td>-14d13m07.76s</td>\n",
       "      <td>-14.218823</td>\n",
       "      <td>-1.706</td>\n",
       "      <td>0.069</td>\n",
       "      <td>...</td>\n",
       "      <td>4.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0</td>\n",
       "      <td>2.349860</td>\n",
       "      <td>0.091578</td>\n",
       "      <td>-0.091578</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-07-24 15:58:33</td>\n",
       "      <td>2024-09-09 10:08:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7702</th>\n",
       "      <td>7703</td>\n",
       "      <td>999.01</td>\n",
       "      <td>341186896</td>\n",
       "      <td>FP</td>\n",
       "      <td>07h55m27.38s</td>\n",
       "      <td>118.864086</td>\n",
       "      <td>-58d13m19.42s</td>\n",
       "      <td>-58.222060</td>\n",
       "      <td>-17.190</td>\n",
       "      <td>0.042</td>\n",
       "      <td>...</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-07-24 15:58:33</td>\n",
       "      <td>2024-09-09 10:08:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7703 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      loc_rowid      toi        tid tfopwg_disp         rastr          ra  \\\n",
       "0             1  1000.01   50365310          FP  07h29m25.85s  112.357708   \n",
       "1             2  1001.01   88863718          PC  08h10m19.31s  122.580465   \n",
       "2             3  1002.01  124709665          FP  06h58m54.47s  104.726966   \n",
       "3             4  1003.01  106997505          FP  07h22m14.39s  110.559945   \n",
       "4             5  1004.01  238597883          FP  08h08m42.77s  122.178195   \n",
       "...         ...      ...        ...         ...           ...         ...   \n",
       "7698       7699   995.01  317951248          FP  07h23m14.75s  110.811443   \n",
       "7699       7700   996.01  142918609          FP  07h57m23.99s  119.349948   \n",
       "7700       7701   997.01  341729521          FP  08h05m16.69s  121.319521   \n",
       "7701       7702   998.01   54390047          FP  07h53m16.69s  118.319555   \n",
       "7702       7703   999.01  341186896          FP  07h55m27.38s  118.864086   \n",
       "\n",
       "             decstr        dec  st_pmra  st_pmraerr1  ...  st_logg  \\\n",
       "0     -12d41m45.46s -12.695960   -5.964        0.085  ...     4.19   \n",
       "1     -05d30m49.87s  -5.513852   -4.956        0.102  ...     4.03   \n",
       "2     -10d34m49.64s -10.580455   -1.462        0.206  ...      NaN   \n",
       "3     -25d12m25.26s -25.207017   -0.939        0.041  ...     4.15   \n",
       "4     -48d48m10.12s -48.802811   -4.496        0.069  ...     4.14   \n",
       "...             ...        ...      ...          ...  ...      ...   \n",
       "7698  +05d33m46.26s   5.562850    2.061        0.405  ...      NaN   \n",
       "7699  -19d30m57.65s -19.516015   -3.900        0.848  ...      NaN   \n",
       "7700  -59d34m47.27s -59.579798  -44.770        0.044  ...     4.52   \n",
       "7701  -14d13m07.76s -14.218823   -1.706        0.069  ...     4.01   \n",
       "7702  -58d13m19.42s -58.222060  -17.190        0.042  ...     4.35   \n",
       "\n",
       "      st_loggerr1  st_loggerr2  st_logglim    st_rad  st_raderr1  st_raderr2  \\\n",
       "0            0.07        -0.07           0  2.169860    0.072573   -0.072573   \n",
       "1            0.09        -0.09           0  2.010000    0.090000   -0.090000   \n",
       "2             NaN          NaN           0  5.730000         NaN         NaN   \n",
       "3            1.64        -1.64           0       NaN         NaN         NaN   \n",
       "4            0.07        -0.07           0  2.150000    0.060000   -0.060000   \n",
       "...           ...          ...         ...       ...         ...         ...   \n",
       "7698          NaN          NaN           0       NaN         NaN         NaN   \n",
       "7699          NaN          NaN           0  2.050000         NaN         NaN   \n",
       "7700         0.08        -0.08           0  0.926261    0.045789   -0.045789   \n",
       "7701         0.07        -0.07           0  2.349860    0.091578   -0.091578   \n",
       "7702         0.09        -0.09           0  1.300000    0.050000   -0.050000   \n",
       "\n",
       "      st_radlim          toi_created            rowupdate  \n",
       "0             0  2019-07-24 15:58:33  2024-09-09 10:08:01  \n",
       "1             0  2019-07-24 15:58:33  2023-04-03 14:31:04  \n",
       "2             0  2019-07-24 15:58:33  2022-07-11 16:02:02  \n",
       "3             0  2019-07-24 15:58:33  2022-02-23 10:10:02  \n",
       "4             0  2019-07-24 15:58:33  2024-09-09 10:08:01  \n",
       "...         ...                  ...                  ...  \n",
       "7698          0  2019-07-24 15:58:33  2021-10-29 12:59:15  \n",
       "7699          0  2019-07-24 15:58:33  2021-10-29 12:59:15  \n",
       "7700          0  2019-07-24 15:58:33  2024-09-09 10:08:01  \n",
       "7701          0  2019-07-24 15:58:33  2024-09-09 10:08:01  \n",
       "7702          0  2019-07-24 15:58:33  2024-09-09 10:08:01  \n",
       "\n",
       "[7703 rows x 66 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Data\\TOI final.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "334b4de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7703 entries, 0 to 7702\n",
      "Data columns (total 66 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   loc_rowid        7703 non-null   int64  \n",
      " 1   toi              7703 non-null   float64\n",
      " 2   tid              7703 non-null   int64  \n",
      " 3   tfopwg_disp      7703 non-null   object \n",
      " 4   rastr            7703 non-null   object \n",
      " 5   ra               7703 non-null   float64\n",
      " 6   decstr           7703 non-null   object \n",
      " 7   dec              7703 non-null   float64\n",
      " 8   st_pmra          7569 non-null   float64\n",
      " 9   st_pmraerr1      7569 non-null   float64\n",
      " 10  st_pmraerr2      7569 non-null   float64\n",
      " 11  st_pmralim       7569 non-null   float64\n",
      " 12  st_pmdec         7569 non-null   float64\n",
      " 13  st_pmdecerr1     7569 non-null   float64\n",
      " 14  st_pmdecerr2     7569 non-null   float64\n",
      " 15  st_pmdeclim      7569 non-null   float64\n",
      " 16  pl_tranmid       7703 non-null   float64\n",
      " 17  pl_tranmiderr1   7692 non-null   float64\n",
      " 18  pl_tranmiderr2   7692 non-null   float64\n",
      " 19  pl_tranmidlim    7703 non-null   int64  \n",
      " 20  pl_orbper        7596 non-null   float64\n",
      " 21  pl_orbpererr1    7572 non-null   float64\n",
      " 22  pl_orbpererr2    7572 non-null   float64\n",
      " 23  pl_orbperlim     7703 non-null   int64  \n",
      " 24  pl_trandurh      7703 non-null   float64\n",
      " 25  pl_trandurherr1  7690 non-null   float64\n",
      " 26  pl_trandurherr2  7690 non-null   float64\n",
      " 27  pl_trandurhlim   7703 non-null   int64  \n",
      " 28  pl_trandep       7703 non-null   float64\n",
      " 29  pl_trandeperr1   7697 non-null   float64\n",
      " 30  pl_trandeperr2   7697 non-null   float64\n",
      " 31  pl_trandeplim    7703 non-null   int64  \n",
      " 32  pl_rade          7197 non-null   float64\n",
      " 33  pl_radeerr1      6080 non-null   float64\n",
      " 34  pl_radeerr2      6080 non-null   float64\n",
      " 35  pl_radelim       7703 non-null   int64  \n",
      " 36  pl_insol         7527 non-null   float64\n",
      " 37  pl_insolerr1     0 non-null      float64\n",
      " 38  pl_insolerr2     0 non-null      float64\n",
      " 39  pl_insollim      0 non-null      float64\n",
      " 40  pl_eqt           7392 non-null   float64\n",
      " 41  pl_eqterr1       0 non-null      float64\n",
      " 42  pl_eqterr2       0 non-null      float64\n",
      " 43  pl_eqtlim        0 non-null      float64\n",
      " 44  st_tmag          7703 non-null   float64\n",
      " 45  st_tmagerr1      7703 non-null   float64\n",
      " 46  st_tmagerr2      7703 non-null   float64\n",
      " 47  st_tmaglim       7703 non-null   int64  \n",
      " 48  st_dist          7488 non-null   float64\n",
      " 49  st_disterr1      6996 non-null   float64\n",
      " 50  st_disterr2      6996 non-null   float64\n",
      " 51  st_distlim       7703 non-null   int64  \n",
      " 52  st_teff          7542 non-null   float64\n",
      " 53  st_tefferr1      7229 non-null   float64\n",
      " 54  st_tefferr2      7229 non-null   float64\n",
      " 55  st_tefflim       7703 non-null   int64  \n",
      " 56  st_logg          6847 non-null   float64\n",
      " 57  st_loggerr1      5432 non-null   float64\n",
      " 58  st_loggerr2      5432 non-null   float64\n",
      " 59  st_logglim       7703 non-null   int64  \n",
      " 60  st_rad           7196 non-null   float64\n",
      " 61  st_raderr1       5740 non-null   float64\n",
      " 62  st_raderr2       5740 non-null   float64\n",
      " 63  st_radlim        7703 non-null   int64  \n",
      " 64  toi_created      7703 non-null   object \n",
      " 65  rowupdate        7703 non-null   object \n",
      "dtypes: float64(49), int64(12), object(5)\n",
      "memory usage: 3.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "293167e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = {\n",
    "    \"APC\": \"CANDIDATE\",\n",
    "    \"FA\": \"FALSE POSITIVE\",\n",
    "    \"FP\": \"FALSE POSITIVE\",\n",
    "    \"KP\": \"CONFIRMED\",\n",
    "    \"PC\": \"CANDIDATE\",\n",
    "    \"CP\": \"CONFIRMED\"\n",
    "}\n",
    "df[\"tfopwg_disp\"] = df[\"tfopwg_disp\"].map(target_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9dac80dc",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].mode()[0], inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].mode()[0], inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
      "C:\\Users\\Victus\\AppData\\Local\\Temp\\ipykernel_35844\\4133624849.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled[col].fillna(df_filled[col].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_filled = df.copy()\n",
    "missing_percent = df_filled.isnull().mean() * 100\n",
    "\n",
    "# Columns to fill and drop\n",
    "cols_to_fill = missing_percent[missing_percent < 50].index\n",
    "cols_to_drop = missing_percent[missing_percent >= 50].index\n",
    "\n",
    "# Fill numeric columns with median, categorical with mode\n",
    "for col in cols_to_fill:\n",
    "    if df_filled[col].dtype in ['float64', 'int64']:\n",
    "        df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
    "    else:\n",
    "        df_filled[col].fillna(df_filled[col].mode()[0], inplace=True)\n",
    "\n",
    "# Drop columns with >=50% missing\n",
    "df_filled.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6378bb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7703 entries, 0 to 7702\n",
      "Data columns (total 60 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   loc_rowid        7703 non-null   int64  \n",
      " 1   toi              7703 non-null   float64\n",
      " 2   tid              7703 non-null   int64  \n",
      " 3   tfopwg_disp      7703 non-null   object \n",
      " 4   rastr            7703 non-null   object \n",
      " 5   ra               7703 non-null   float64\n",
      " 6   decstr           7703 non-null   object \n",
      " 7   dec              7703 non-null   float64\n",
      " 8   st_pmra          7703 non-null   float64\n",
      " 9   st_pmraerr1      7703 non-null   float64\n",
      " 10  st_pmraerr2      7703 non-null   float64\n",
      " 11  st_pmralim       7703 non-null   float64\n",
      " 12  st_pmdec         7703 non-null   float64\n",
      " 13  st_pmdecerr1     7703 non-null   float64\n",
      " 14  st_pmdecerr2     7703 non-null   float64\n",
      " 15  st_pmdeclim      7703 non-null   float64\n",
      " 16  pl_tranmid       7703 non-null   float64\n",
      " 17  pl_tranmiderr1   7703 non-null   float64\n",
      " 18  pl_tranmiderr2   7703 non-null   float64\n",
      " 19  pl_tranmidlim    7703 non-null   int64  \n",
      " 20  pl_orbper        7703 non-null   float64\n",
      " 21  pl_orbpererr1    7703 non-null   float64\n",
      " 22  pl_orbpererr2    7703 non-null   float64\n",
      " 23  pl_orbperlim     7703 non-null   int64  \n",
      " 24  pl_trandurh      7703 non-null   float64\n",
      " 25  pl_trandurherr1  7703 non-null   float64\n",
      " 26  pl_trandurherr2  7703 non-null   float64\n",
      " 27  pl_trandurhlim   7703 non-null   int64  \n",
      " 28  pl_trandep       7703 non-null   float64\n",
      " 29  pl_trandeperr1   7703 non-null   float64\n",
      " 30  pl_trandeperr2   7703 non-null   float64\n",
      " 31  pl_trandeplim    7703 non-null   int64  \n",
      " 32  pl_rade          7703 non-null   float64\n",
      " 33  pl_radeerr1      7703 non-null   float64\n",
      " 34  pl_radeerr2      7703 non-null   float64\n",
      " 35  pl_radelim       7703 non-null   int64  \n",
      " 36  pl_insol         7703 non-null   float64\n",
      " 37  pl_eqt           7703 non-null   float64\n",
      " 38  st_tmag          7703 non-null   float64\n",
      " 39  st_tmagerr1      7703 non-null   float64\n",
      " 40  st_tmagerr2      7703 non-null   float64\n",
      " 41  st_tmaglim       7703 non-null   int64  \n",
      " 42  st_dist          7703 non-null   float64\n",
      " 43  st_disterr1      7703 non-null   float64\n",
      " 44  st_disterr2      7703 non-null   float64\n",
      " 45  st_distlim       7703 non-null   int64  \n",
      " 46  st_teff          7703 non-null   float64\n",
      " 47  st_tefferr1      7703 non-null   float64\n",
      " 48  st_tefferr2      7703 non-null   float64\n",
      " 49  st_tefflim       7703 non-null   int64  \n",
      " 50  st_logg          7703 non-null   float64\n",
      " 51  st_loggerr1      7703 non-null   float64\n",
      " 52  st_loggerr2      7703 non-null   float64\n",
      " 53  st_logglim       7703 non-null   int64  \n",
      " 54  st_rad           7703 non-null   float64\n",
      " 55  st_raderr1       7703 non-null   float64\n",
      " 56  st_raderr2       7703 non-null   float64\n",
      " 57  st_radlim        7703 non-null   int64  \n",
      " 58  toi_created      7703 non-null   object \n",
      " 59  rowupdate        7703 non-null   object \n",
      "dtypes: float64(43), int64(12), object(5)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_filled.info() # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f4214c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all numerical features in df_filtered using StandardScaler\n",
    "\n",
    "df_scaled = df_filled.copy() # type: ignore\n",
    "num_cols = df_scaled.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = StandardScaler()\n",
    "df_scaled[num_cols] = scaler.fit_transform(df_scaled[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6653a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfopwg_disp       3\n",
      "rastr          7390\n",
      "decstr         7400\n",
      "toi_created    1940\n",
      "rowupdate       893\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Show number of unique values in each categorical feature of df_scaled\n",
    "cat_cols = df_scaled.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "unique_counts = df_scaled[cat_cols].nunique()\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cdd97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.drop(columns=['decstr','rastr','rowupdate','toi_created'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "571ef075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_rowid</th>\n",
       "      <th>toi</th>\n",
       "      <th>tid</th>\n",
       "      <th>tfopwg_disp</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>st_pmra</th>\n",
       "      <th>st_pmraerr1</th>\n",
       "      <th>st_pmraerr2</th>\n",
       "      <th>st_pmralim</th>\n",
       "      <th>...</th>\n",
       "      <th>st_tefferr2</th>\n",
       "      <th>st_tefflim</th>\n",
       "      <th>st_logg</th>\n",
       "      <th>st_loggerr1</th>\n",
       "      <th>st_loggerr2</th>\n",
       "      <th>st_logglim</th>\n",
       "      <th>st_rad</th>\n",
       "      <th>st_raderr1</th>\n",
       "      <th>st_raderr2</th>\n",
       "      <th>st_radlim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.731826</td>\n",
       "      <td>-1.277301</td>\n",
       "      <td>-1.207012</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.650406</td>\n",
       "      <td>-0.291864</td>\n",
       "      <td>-0.070478</td>\n",
       "      <td>-0.224276</td>\n",
       "      <td>0.224276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.410982</td>\n",
       "      <td>-0.262134</td>\n",
       "      <td>0.262134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.503065</td>\n",
       "      <td>0.025507</td>\n",
       "      <td>-0.025507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.731376</td>\n",
       "      <td>-1.276836</td>\n",
       "      <td>-0.968907</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.551825</td>\n",
       "      <td>-0.140541</td>\n",
       "      <td>-0.057271</td>\n",
       "      <td>-0.197166</td>\n",
       "      <td>0.197166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.968164</td>\n",
       "      <td>-0.195091</td>\n",
       "      <td>0.195091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.399599</td>\n",
       "      <td>0.270649</td>\n",
       "      <td>-0.270649</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.730927</td>\n",
       "      <td>-1.276372</td>\n",
       "      <td>-0.747206</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.723991</td>\n",
       "      <td>-0.247291</td>\n",
       "      <td>-0.011494</td>\n",
       "      <td>-0.031322</td>\n",
       "      <td>0.031322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076551</td>\n",
       "      <td>-0.213325</td>\n",
       "      <td>0.213325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.807279</td>\n",
       "      <td>-0.151353</td>\n",
       "      <td>0.151353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.730477</td>\n",
       "      <td>-1.275907</td>\n",
       "      <td>-0.856753</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.667742</td>\n",
       "      <td>-0.555464</td>\n",
       "      <td>-0.004642</td>\n",
       "      <td>-0.294441</td>\n",
       "      <td>0.294441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.686717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.550278</td>\n",
       "      <td>5.000736</td>\n",
       "      <td>-5.000736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.102428</td>\n",
       "      <td>-0.151353</td>\n",
       "      <td>0.151353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.730027</td>\n",
       "      <td>-1.275442</td>\n",
       "      <td>-0.042828</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.555704</td>\n",
       "      <td>-1.052613</td>\n",
       "      <td>-0.051245</td>\n",
       "      <td>-0.249790</td>\n",
       "      <td>0.249790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.585102</td>\n",
       "      <td>-0.262134</td>\n",
       "      <td>0.262134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.490211</td>\n",
       "      <td>-0.151353</td>\n",
       "      <td>0.151353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loc_rowid       toi       tid  tfopwg_disp        ra       dec   st_pmra  \\\n",
       "0  -1.731826 -1.277301 -1.207012            2 -0.650406 -0.291864 -0.070478   \n",
       "1  -1.731376 -1.276836 -0.968907            0 -0.551825 -0.140541 -0.057271   \n",
       "2  -1.730927 -1.276372 -0.747206            2 -0.723991 -0.247291 -0.011494   \n",
       "3  -1.730477 -1.275907 -0.856753            2 -0.667742 -0.555464 -0.004642   \n",
       "4  -1.730027 -1.275442 -0.042828            2 -0.555704 -1.052613 -0.051245   \n",
       "\n",
       "   st_pmraerr1  st_pmraerr2  st_pmralim  ...  st_tefferr2  st_tefflim  \\\n",
       "0    -0.224276     0.224276         0.0  ...    -0.119694         0.0   \n",
       "1    -0.197166     0.197166         0.0  ...     0.139715         0.0   \n",
       "2    -0.031322     0.031322         0.0  ...     0.144216         0.0   \n",
       "3    -0.294441     0.294441         0.0  ...    -0.686717         0.0   \n",
       "4    -0.249790     0.249790         0.0  ...     0.055871         0.0   \n",
       "\n",
       "    st_logg  st_loggerr1  st_loggerr2  st_logglim    st_rad  st_raderr1  \\\n",
       "0 -0.410982    -0.262134     0.262134         0.0  0.503065    0.025507   \n",
       "1 -0.968164    -0.195091     0.195091         0.0  0.399599    0.270649   \n",
       "2  0.076551    -0.213325     0.213325         0.0  2.807279   -0.151353   \n",
       "3 -0.550278     5.000736    -5.000736         0.0 -0.102428   -0.151353   \n",
       "4 -0.585102    -0.262134     0.262134         0.0  0.490211   -0.151353   \n",
       "\n",
       "   st_raderr2  st_radlim  \n",
       "0   -0.025507        0.0  \n",
       "1   -0.270649        0.0  \n",
       "2    0.151353        0.0  \n",
       "3    0.151353        0.0  \n",
       "4    0.151353        0.0  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Identify categorical columns in df_scaled\n",
    "cat_cols = df_scaled.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "\n",
    "# Apply label encoding to each categorical column\n",
    "df_scaled_label_encoded = df_scaled.copy()\n",
    "le_dict = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    # Fill NaN with a placeholder string before encoding\n",
    "    df_scaled_label_encoded[col] = le.fit_transform(df_scaled_label_encoded[col].fillna('NaN_Label'))\n",
    "    le_dict[col] = le  # Save encoder if inverse_transform is needed later\n",
    "\n",
    "df_scaled_label_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62e65e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7703 entries, 0 to 7702\n",
      "Data columns (total 56 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   loc_rowid        7703 non-null   float64\n",
      " 1   toi              7703 non-null   float64\n",
      " 2   tid              7703 non-null   float64\n",
      " 3   tfopwg_disp      7703 non-null   int32  \n",
      " 4   ra               7703 non-null   float64\n",
      " 5   dec              7703 non-null   float64\n",
      " 6   st_pmra          7703 non-null   float64\n",
      " 7   st_pmraerr1      7703 non-null   float64\n",
      " 8   st_pmraerr2      7703 non-null   float64\n",
      " 9   st_pmralim       7703 non-null   float64\n",
      " 10  st_pmdec         7703 non-null   float64\n",
      " 11  st_pmdecerr1     7703 non-null   float64\n",
      " 12  st_pmdecerr2     7703 non-null   float64\n",
      " 13  st_pmdeclim      7703 non-null   float64\n",
      " 14  pl_tranmid       7703 non-null   float64\n",
      " 15  pl_tranmiderr1   7703 non-null   float64\n",
      " 16  pl_tranmiderr2   7703 non-null   float64\n",
      " 17  pl_tranmidlim    7703 non-null   float64\n",
      " 18  pl_orbper        7703 non-null   float64\n",
      " 19  pl_orbpererr1    7703 non-null   float64\n",
      " 20  pl_orbpererr2    7703 non-null   float64\n",
      " 21  pl_orbperlim     7703 non-null   float64\n",
      " 22  pl_trandurh      7703 non-null   float64\n",
      " 23  pl_trandurherr1  7703 non-null   float64\n",
      " 24  pl_trandurherr2  7703 non-null   float64\n",
      " 25  pl_trandurhlim   7703 non-null   float64\n",
      " 26  pl_trandep       7703 non-null   float64\n",
      " 27  pl_trandeperr1   7703 non-null   float64\n",
      " 28  pl_trandeperr2   7703 non-null   float64\n",
      " 29  pl_trandeplim    7703 non-null   float64\n",
      " 30  pl_rade          7703 non-null   float64\n",
      " 31  pl_radeerr1      7703 non-null   float64\n",
      " 32  pl_radeerr2      7703 non-null   float64\n",
      " 33  pl_radelim       7703 non-null   float64\n",
      " 34  pl_insol         7703 non-null   float64\n",
      " 35  pl_eqt           7703 non-null   float64\n",
      " 36  st_tmag          7703 non-null   float64\n",
      " 37  st_tmagerr1      7703 non-null   float64\n",
      " 38  st_tmagerr2      7703 non-null   float64\n",
      " 39  st_tmaglim       7703 non-null   float64\n",
      " 40  st_dist          7703 non-null   float64\n",
      " 41  st_disterr1      7703 non-null   float64\n",
      " 42  st_disterr2      7703 non-null   float64\n",
      " 43  st_distlim       7703 non-null   float64\n",
      " 44  st_teff          7703 non-null   float64\n",
      " 45  st_tefferr1      7703 non-null   float64\n",
      " 46  st_tefferr2      7703 non-null   float64\n",
      " 47  st_tefflim       7703 non-null   float64\n",
      " 48  st_logg          7703 non-null   float64\n",
      " 49  st_loggerr1      7703 non-null   float64\n",
      " 50  st_loggerr2      7703 non-null   float64\n",
      " 51  st_logglim       7703 non-null   float64\n",
      " 52  st_rad           7703 non-null   float64\n",
      " 53  st_raderr1       7703 non-null   float64\n",
      " 54  st_raderr2       7703 non-null   float64\n",
      " 55  st_radlim        7703 non-null   float64\n",
      "dtypes: float64(55), int32(1)\n",
      "memory usage: 3.3 MB\n"
     ]
    }
   ],
   "source": [
    "df_scaled_label_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70a8e524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toi                0.352405\n",
      "st_tmag            0.238270\n",
      "loc_rowid          0.230721\n",
      "pl_eqt             0.219080\n",
      "pl_tranmid         0.182950\n",
      "pl_insol           0.158500\n",
      "st_rad             0.083001\n",
      "st_raderr1         0.067516\n",
      "st_raderr2         0.067516\n",
      "pl_radeerr2        0.060801\n",
      "pl_radeerr1        0.060801\n",
      "st_tmagerr2        0.057016\n",
      "st_tmagerr1        0.057016\n",
      "st_loggerr2        0.055888\n",
      "st_loggerr1        0.055888\n",
      "st_teff            0.049384\n",
      "st_disterr2        0.048997\n",
      "st_disterr1        0.048997\n",
      "st_pmdecerr1       0.048763\n",
      "st_pmdecerr2       0.048763\n",
      "st_pmraerr2        0.048582\n",
      "st_pmraerr1        0.048582\n",
      "pl_orbper          0.045925\n",
      "dec                0.042892\n",
      "pl_orbpererr1      0.038532\n",
      "pl_orbpererr2      0.038532\n",
      "st_dist            0.034453\n",
      "pl_rade            0.030467\n",
      "st_logg            0.029497\n",
      "st_tefferr2        0.029164\n",
      "st_tefferr1        0.029164\n",
      "pl_trandurh        0.027542\n",
      "st_pmdec           0.024175\n",
      "pl_trandeperr2     0.020623\n",
      "pl_trandeperr1     0.020623\n",
      "st_pmra            0.014185\n",
      "pl_trandurherr1    0.007295\n",
      "pl_trandurherr2    0.007295\n",
      "ra                 0.006451\n",
      "pl_tranmiderr2     0.005011\n",
      "pl_tranmiderr1     0.005011\n",
      "tid                0.004541\n",
      "pl_trandep         0.001861\n",
      "st_pmralim              NaN\n",
      "st_pmdeclim             NaN\n",
      "pl_tranmidlim           NaN\n",
      "pl_orbperlim            NaN\n",
      "pl_trandurhlim          NaN\n",
      "pl_trandeplim           NaN\n",
      "pl_radelim              NaN\n",
      "st_tmaglim              NaN\n",
      "st_distlim              NaN\n",
      "st_tefflim              NaN\n",
      "st_logglim              NaN\n",
      "st_radlim               NaN\n",
      "Name: tfopwg_disp, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlations with \"TOI_disposition\"\n",
    "corr_with_disposition = df_scaled_label_encoded.corr(numeric_only=True)[\"tfopwg_disp\"].drop(\"tfopwg_disp\")\n",
    "corr_with_disposition = corr_with_disposition.abs()  # Use absolute values for correlation strength\n",
    "# Sort correlations in descending order\n",
    "corr_with_disposition_sorted = corr_with_disposition.sort_values(ascending=False)\n",
    "print(corr_with_disposition_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d83889d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FALSE POSITIVE' 'CANDIDATE' 'CONFIRMED']\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"tfopwg_disp\"].unique())\n",
    "print(df[\"tfopwg_disp\"].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10f0243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toi                0.068530\n",
      "loc_rowid          0.054858\n",
      "st_tmag            0.040109\n",
      "pl_tranmid         0.035751\n",
      "pl_eqt             0.035251\n",
      "st_dist            0.031589\n",
      "pl_insol           0.031234\n",
      "pl_trandurherr2    0.029427\n",
      "pl_trandep         0.027090\n",
      "pl_tranmiderr1     0.026881\n",
      "pl_tranmiderr2     0.025686\n",
      "pl_trandurherr1    0.025565\n",
      "pl_orbper          0.025173\n",
      "pl_rade            0.025022\n",
      "st_disterr2        0.024414\n",
      "pl_trandurh        0.023851\n",
      "pl_orbpererr1      0.023708\n",
      "st_disterr1        0.023354\n",
      "pl_orbpererr2      0.022458\n",
      "pl_trandeperr1     0.021441\n",
      "pl_radeerr2        0.021324\n",
      "pl_radeerr1        0.021213\n",
      "pl_trandeperr2     0.020300\n",
      "dec                0.020220\n",
      "st_rad             0.019941\n",
      "st_pmdec           0.019790\n",
      "ra                 0.019126\n",
      "tid                0.018971\n",
      "st_pmra            0.018713\n",
      "st_logg            0.018384\n",
      "st_teff            0.018364\n",
      "st_pmraerr2        0.016690\n",
      "st_pmdecerr1       0.016327\n",
      "st_pmdecerr2       0.016276\n",
      "st_pmraerr1        0.016093\n",
      "st_tefferr1        0.016027\n",
      "st_tefferr2        0.015983\n",
      "st_loggerr2        0.013722\n",
      "st_raderr1         0.013641\n",
      "st_raderr2         0.013190\n",
      "st_loggerr1        0.011507\n",
      "st_tmagerr2        0.006703\n",
      "st_tmagerr1        0.006103\n",
      "st_logglim         0.000000\n",
      "st_tefflim         0.000000\n",
      "pl_orbperlim       0.000000\n",
      "st_distlim         0.000000\n",
      "pl_trandurhlim     0.000000\n",
      "st_tmaglim         0.000000\n",
      "pl_radelim         0.000000\n",
      "st_pmralim         0.000000\n",
      "pl_trandeplim      0.000000\n",
      "st_pmdeclim        0.000000\n",
      "pl_tranmidlim      0.000000\n",
      "st_radlim          0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Prepare features and target\n",
    "X = df_scaled_label_encoded.drop(columns=\"tfopwg_disp\")\n",
    "y = df_scaled_label_encoded[\"tfopwg_disp\"]\n",
    "\n",
    "# Handle categorical variables if any\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "importances_sorted = importances.sort_values(ascending=False)\n",
    "print(importances_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77494ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['st_teff', 'st_raderr1', 'st_tmagerr2', 'pl_tranmiderr1', 'pl_insol', 'st_loggerr1', 'st_dist', 'st_pmdecerr1', 'pl_tranmid', 'pl_radeerr1', 'st_disterr2', 'st_disterr1', 'pl_trandurherr2', 'st_raderr2', 'toi', 'st_tmag', 'loc_rowid', 'st_rad', 'st_tmagerr1', 'st_pmdecerr2', 'pl_eqt', 'st_loggerr2', 'pl_radeerr2', 'pl_trandep']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7703 entries, 0 to 7702\n",
      "Data columns (total 24 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   st_teff          7703 non-null   float64\n",
      " 1   st_raderr1       7703 non-null   float64\n",
      " 2   st_tmagerr2      7703 non-null   float64\n",
      " 3   pl_tranmiderr1   7703 non-null   float64\n",
      " 4   pl_insol         7703 non-null   float64\n",
      " 5   st_loggerr1      7703 non-null   float64\n",
      " 6   st_dist          7703 non-null   float64\n",
      " 7   st_pmdecerr1     7703 non-null   float64\n",
      " 8   pl_tranmid       7703 non-null   float64\n",
      " 9   pl_radeerr1      7703 non-null   float64\n",
      " 10  st_disterr2      7703 non-null   float64\n",
      " 11  st_disterr1      7703 non-null   float64\n",
      " 12  pl_trandurherr2  7703 non-null   float64\n",
      " 13  st_raderr2       7703 non-null   float64\n",
      " 14  toi              7703 non-null   float64\n",
      " 15  st_tmag          7703 non-null   float64\n",
      " 16  loc_rowid        7703 non-null   float64\n",
      " 17  st_rad           7703 non-null   float64\n",
      " 18  st_tmagerr1      7703 non-null   float64\n",
      " 19  st_pmdecerr2     7703 non-null   float64\n",
      " 20  pl_eqt           7703 non-null   float64\n",
      " 21  st_loggerr2      7703 non-null   float64\n",
      " 22  pl_radeerr2      7703 non-null   float64\n",
      " 23  pl_trandep       7703 non-null   float64\n",
      "dtypes: float64(24)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Select top N features from both correlation and feature importance\n",
    "N = 20\n",
    "\n",
    "# Get top N features by absolute correlation with disposition\n",
    "top_corr_features = corr_with_disposition_sorted.head(N).index.tolist()\n",
    "\n",
    "# Get top N features by Random Forest importance (excluding one-hot columns for clarity)\n",
    "top_importance_features = importances_sorted.head(10).index.tolist()\n",
    "\n",
    "# Combine and deduplicate\n",
    "selected_features = list(set(top_corr_features + top_importance_features))\n",
    "\n",
    "# Filter X_train to only these features\n",
    "valuable_features_df = X[selected_features]\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n",
    "valuable_features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "17788d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7703 entries, 0 to 7702\n",
      "Data columns (total 24 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   st_teff          7703 non-null   float64\n",
      " 1   st_raderr1       7703 non-null   float64\n",
      " 2   st_tmagerr2      7703 non-null   float64\n",
      " 3   pl_tranmiderr1   7703 non-null   float64\n",
      " 4   pl_insol         7703 non-null   float64\n",
      " 5   st_loggerr1      7703 non-null   float64\n",
      " 6   st_dist          7703 non-null   float64\n",
      " 7   st_pmdecerr1     7703 non-null   float64\n",
      " 8   pl_tranmid       7703 non-null   float64\n",
      " 9   pl_radeerr1      7703 non-null   float64\n",
      " 10  st_disterr2      7703 non-null   float64\n",
      " 11  st_disterr1      7703 non-null   float64\n",
      " 12  pl_trandurherr2  7703 non-null   float64\n",
      " 13  st_raderr2       7703 non-null   float64\n",
      " 14  toi              7703 non-null   float64\n",
      " 15  st_tmag          7703 non-null   float64\n",
      " 16  loc_rowid        7703 non-null   float64\n",
      " 17  st_rad           7703 non-null   float64\n",
      " 18  st_tmagerr1      7703 non-null   float64\n",
      " 19  st_pmdecerr2     7703 non-null   float64\n",
      " 20  pl_eqt           7703 non-null   float64\n",
      " 21  st_loggerr2      7703 non-null   float64\n",
      " 22  pl_radeerr2      7703 non-null   float64\n",
      " 23  pl_trandep       7703 non-null   float64\n",
      "dtypes: float64(24)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "valuable_features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7ca60bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7703 entries, 0 to 7702\n",
      "Data columns (total 25 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   st_teff          7703 non-null   float64\n",
      " 1   st_raderr1       7703 non-null   float64\n",
      " 2   st_tmagerr2      7703 non-null   float64\n",
      " 3   pl_tranmiderr1   7703 non-null   float64\n",
      " 4   pl_insol         7703 non-null   float64\n",
      " 5   st_loggerr1      7703 non-null   float64\n",
      " 6   st_dist          7703 non-null   float64\n",
      " 7   st_pmdecerr1     7703 non-null   float64\n",
      " 8   pl_tranmid       7703 non-null   float64\n",
      " 9   pl_radeerr1      7703 non-null   float64\n",
      " 10  st_disterr2      7703 non-null   float64\n",
      " 11  st_disterr1      7703 non-null   float64\n",
      " 12  pl_trandurherr2  7703 non-null   float64\n",
      " 13  st_raderr2       7703 non-null   float64\n",
      " 14  toi              7703 non-null   float64\n",
      " 15  st_tmag          7703 non-null   float64\n",
      " 16  loc_rowid        7703 non-null   float64\n",
      " 17  st_rad           7703 non-null   float64\n",
      " 18  st_tmagerr1      7703 non-null   float64\n",
      " 19  st_pmdecerr2     7703 non-null   float64\n",
      " 20  pl_eqt           7703 non-null   float64\n",
      " 21  st_loggerr2      7703 non-null   float64\n",
      " 22  pl_radeerr2      7703 non-null   float64\n",
      " 23  pl_trandep       7703 non-null   float64\n",
      " 24  tfopwg_disp      7703 non-null   int32  \n",
      "dtypes: float64(24), int32(1)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Remove 'tfopwg_disp' from valuable_features_df if present\n",
    "if 'tfopwg_disp' in valuable_features_df.columns:\n",
    "    valuable_features_df = valuable_features_df.drop(columns=['tfopwg_disp'])\n",
    "\n",
    "# Add the target variable (koi_disposition) to valuable_features_df\n",
    "preprocessed_df = valuable_features_df.copy()\n",
    "\n",
    "preprocessed_df[\"tfopwg_disp\"] = df_scaled_label_encoded.loc[valuable_features_df.index, \"tfopwg_disp\"]\n",
    "\n",
    "preprocessed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9eea7ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st_raderr1         19.836427\n",
      "st_raderr2         19.836427\n",
      "st_loggerr1        19.498897\n",
      "st_loggerr2        19.498897\n",
      "st_tmagerr1        16.305336\n",
      "st_tmagerr2        16.305336\n",
      "pl_radeerr2        13.631053\n",
      "pl_radeerr1        13.631053\n",
      "st_pmdecerr1       11.969363\n",
      "st_pmdecerr2       11.969363\n",
      "pl_insol           11.735687\n",
      "st_teff             9.749448\n",
      "st_disterr2         8.594054\n",
      "st_disterr1         8.594054\n",
      "pl_trandep          5.335584\n",
      "pl_trandurherr2     5.322602\n",
      "pl_tranmiderr1      5.088926\n",
      "st_rad              4.284045\n",
      "pl_eqt              3.699857\n",
      "st_dist             2.661301\n",
      "st_tmag             0.415423\n",
      "pl_tranmid          0.012982\n",
      "loc_rowid           0.000000\n",
      "toi                 0.000000\n",
      "tfopwg_disp         0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate percentage of outliers in each numerical column using IQR bounds\n",
    "\n",
    "Q1 = preprocessed_df.quantile(0.25)\n",
    "Q3 = preprocessed_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outlier_percent = ((preprocessed_df < lower_bound) | (preprocessed_df > upper_bound)).sum() / len(df) * 100\n",
    "outlier_percent = outlier_percent.sort_values(ascending=False)\n",
    "print(outlier_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c336ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in full dataset:\n",
      "tfopwg_disp\n",
      "0    5141\n",
      "2    1295\n",
      "1    1267\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in training set:\n",
      "tfopwg_disp\n",
      "0    3888\n",
      "1     945\n",
      "2     944\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in test set:\n",
      "tfopwg_disp\n",
      "0    1253\n",
      "2     351\n",
      "1     322\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Show class balance for the target variable in the full dataset\n",
    "print(\"Class distribution in full dataset:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Show class balance for the training set\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Show class balance for the test set\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dbf14d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e53f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8675f435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize class distribution for the target column \"label\"\n",
    "plt.figure(figsize=(6,4))\n",
    "y.value_counts().plot(kind='bar', color=['tomato', 'gold', 'dodgerblue'])\n",
    "plt.xticks(ticks=[0,1,2], labels=[\"FALSE POSITIVE\", \"CANDIDATE\", \"CONFIRMED\"], rotation=30)\n",
    "plt.title(\"Class Distribution for Target (label)\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc12d4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5777, 55])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Convert to tensors if not already\n",
    "X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 16  # Adjust based on your data size and memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Compute class weights for imbalance (optional but recommended for recall)\n",
    "class_counts = np.bincount(y_train.numpy())\n",
    "class_weights = 1. / class_counts\n",
    "class_weights = torch.tensor(class_weights / class_weights.sum(), dtype=torch.float32)  # Normalize\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "99556936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_rate=0.3):  # <-- fixed __init__\n",
    "        super(ResidualBlock, self).__init__()  # <-- fixed __init__\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.ln = nn.LayerNorm(out_features)  # Layer norm for better stability\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Projection if dimensions differ\n",
    "        self.proj = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = self.proj(x)\n",
    "        x = F.silu(self.ln(self.fc(x)))  # Swish activation\n",
    "        x = self.dropout(x)\n",
    "        return x + residual  # Skip connection\n",
    "\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, input_size=16, num_classes=3):  # <-- fixed __init__\n",
    "        super(ResidualMLP, self).__init__()  # <-- fixed __init__\n",
    "        self.entry = nn.Linear(input_size, 256)  # Entry layer\n",
    "        \n",
    "        # Residual blocks for depth\n",
    "        self.res_block1 = ResidualBlock(256, 256)\n",
    "        self.res_block2 = ResidualBlock(256, 128)\n",
    "        self.res_block3 = ResidualBlock(128, 128)\n",
    "        self.res_block4 = ResidualBlock(128, 64)\n",
    "        \n",
    "        self.dropout_final = nn.Dropout(0.2)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.silu(self.entry(x))\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.res_block4(x)\n",
    "        \n",
    "        x = self.dropout_final(x)\n",
    "        x = self.fc_out(x)  # Logits output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a7f7f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1 2]\n",
      "Class Weights: tensor([0.4953, 2.0377, 2.0399], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Ensure y_train is numpy\n",
    "if isinstance(y_train, torch.Tensor):\n",
    "    y_train_np = y_train.cpu().numpy()\n",
    "else:\n",
    "    y_train_np = np.array(y_train)\n",
    "\n",
    "# Get unique classes\n",
    "classes = np.unique(y_train_np)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train_np\n",
    ")\n",
    "\n",
    "# Convert to tensor for PyTorch\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "\n",
    "# ---- Focal Loss with class weights ----\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # can be class weights tensor\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)  # probability of true class\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "# ---- Initialize with class weights ----\n",
    "loss_fn = FocalLoss(alpha=class_weights, gamma=2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3b57360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Number of GPUs: 1\n",
      "Current Device Index: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current Device Index:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ac94354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved new best model at epoch 1 with Test Accuracy: 40.97%\n",
      "Epoch 1/300 | Train Loss: 0.4923, Train Acc: 38.71% | Test Loss: 0.4581, Test Acc: 40.97%\n",
      "Epoch 2/300 | Train Loss: 0.3975, Train Acc: 41.32% | Test Loss: 0.4012, Test Acc: 39.20%\n",
      "Epoch 3/300 | Train Loss: 0.3578, Train Acc: 41.67% | Test Loss: 0.3893, Test Acc: 37.85%\n",
      "âœ… Saved new best model at epoch 4 with Test Accuracy: 45.95%\n",
      "Epoch 4/300 | Train Loss: 0.3494, Train Acc: 42.05% | Test Loss: 0.3878, Test Acc: 45.95%\n",
      "Epoch 5/300 | Train Loss: 0.3349, Train Acc: 42.22% | Test Loss: 0.3872, Test Acc: 42.94%\n",
      "Epoch 6/300 | Train Loss: 0.3260, Train Acc: 43.73% | Test Loss: 0.3729, Test Acc: 42.99%\n",
      "âœ… Saved new best model at epoch 7 with Test Accuracy: 50.26%\n",
      "Epoch 7/300 | Train Loss: 0.3198, Train Acc: 43.10% | Test Loss: 0.3677, Test Acc: 50.26%\n",
      "Epoch 8/300 | Train Loss: 0.3088, Train Acc: 43.17% | Test Loss: 0.3841, Test Acc: 46.78%\n",
      "âœ… Saved new best model at epoch 9 with Test Accuracy: 52.13%\n",
      "Epoch 9/300 | Train Loss: 0.3079, Train Acc: 43.83% | Test Loss: 0.3797, Test Acc: 52.13%\n",
      "Epoch 10/300 | Train Loss: 0.3032, Train Acc: 45.35% | Test Loss: 0.3612, Test Acc: 49.79%\n",
      "âœ… Saved new best model at epoch 11 with Test Accuracy: 52.49%\n",
      "Epoch 11/300 | Train Loss: 0.2996, Train Acc: 46.48% | Test Loss: 0.3682, Test Acc: 52.49%\n",
      "Epoch 12/300 | Train Loss: 0.2989, Train Acc: 46.03% | Test Loss: 0.3749, Test Acc: 44.08%\n",
      "Epoch 13/300 | Train Loss: 0.2858, Train Acc: 47.05% | Test Loss: 0.3737, Test Acc: 41.90%\n",
      "Epoch 14/300 | Train Loss: 0.2828, Train Acc: 48.12% | Test Loss: 0.3527, Test Acc: 46.37%\n",
      "Epoch 15/300 | Train Loss: 0.2741, Train Acc: 47.48% | Test Loss: 0.3816, Test Acc: 46.11%\n",
      "âœ… Saved new best model at epoch 16 with Test Accuracy: 53.74%\n",
      "Epoch 16/300 | Train Loss: 0.2802, Train Acc: 47.10% | Test Loss: 0.3779, Test Acc: 53.74%\n",
      "Epoch 17/300 | Train Loss: 0.2794, Train Acc: 47.20% | Test Loss: 0.3653, Test Acc: 40.39%\n",
      "Epoch 18/300 | Train Loss: 0.2728, Train Acc: 45.79% | Test Loss: 0.3604, Test Acc: 48.86%\n",
      "Epoch 19/300 | Train Loss: 0.2589, Train Acc: 48.49% | Test Loss: 0.3631, Test Acc: 50.00%\n",
      "Epoch 20/300 | Train Loss: 0.2561, Train Acc: 48.57% | Test Loss: 0.3711, Test Acc: 43.72%\n",
      "Epoch 21/300 | Train Loss: 0.2615, Train Acc: 48.23% | Test Loss: 0.3660, Test Acc: 51.51%\n",
      "Epoch 22/300 | Train Loss: 0.2441, Train Acc: 49.84% | Test Loss: 0.3826, Test Acc: 50.05%\n",
      "ðŸ“‰ Learning rate reduced from 0.001000 to 0.000800\n",
      "Epoch 23/300 | Train Loss: 0.2475, Train Acc: 49.85% | Test Loss: 0.3821, Test Acc: 43.72%\n",
      "Epoch 24/300 | Train Loss: 0.2314, Train Acc: 49.66% | Test Loss: 0.3714, Test Acc: 47.92%\n",
      "Epoch 25/300 | Train Loss: 0.2286, Train Acc: 50.91% | Test Loss: 0.3790, Test Acc: 45.64%\n",
      "Epoch 26/300 | Train Loss: 0.2271, Train Acc: 50.13% | Test Loss: 0.3573, Test Acc: 42.21%\n",
      "Epoch 27/300 | Train Loss: 0.2285, Train Acc: 48.61% | Test Loss: 0.3910, Test Acc: 49.17%\n",
      "Epoch 28/300 | Train Loss: 0.2187, Train Acc: 50.91% | Test Loss: 0.4163, Test Acc: 50.88%\n",
      "Epoch 29/300 | Train Loss: 0.2116, Train Acc: 51.88% | Test Loss: 0.3997, Test Acc: 47.46%\n",
      "ðŸ“‰ Learning rate reduced from 0.000800 to 0.000640\n",
      "Epoch 30/300 | Train Loss: 0.2205, Train Acc: 49.70% | Test Loss: 0.4186, Test Acc: 47.14%\n",
      "Epoch 31/300 | Train Loss: 0.2046, Train Acc: 52.29% | Test Loss: 0.4206, Test Acc: 46.26%\n",
      "Epoch 32/300 | Train Loss: 0.2023, Train Acc: 49.65% | Test Loss: 0.4091, Test Acc: 47.14%\n",
      "Epoch 33/300 | Train Loss: 0.1917, Train Acc: 51.43% | Test Loss: 0.4454, Test Acc: 49.22%\n",
      "Epoch 34/300 | Train Loss: 0.1980, Train Acc: 52.86% | Test Loss: 0.3834, Test Acc: 49.58%\n",
      "Epoch 35/300 | Train Loss: 0.1823, Train Acc: 53.23% | Test Loss: 0.4108, Test Acc: 51.19%\n",
      "Epoch 36/300 | Train Loss: 0.1885, Train Acc: 53.09% | Test Loss: 0.4251, Test Acc: 48.39%\n",
      "ðŸ“‰ Learning rate reduced from 0.000640 to 0.000512\n",
      "Epoch 37/300 | Train Loss: 0.1793, Train Acc: 54.28% | Test Loss: 0.4130, Test Acc: 46.37%\n",
      "âœ… Saved new best model at epoch 38 with Test Accuracy: 55.40%\n",
      "Epoch 38/300 | Train Loss: 0.1767, Train Acc: 54.08% | Test Loss: 0.4284, Test Acc: 55.40%\n",
      "Epoch 39/300 | Train Loss: 0.1723, Train Acc: 55.37% | Test Loss: 0.4468, Test Acc: 42.73%\n",
      "Epoch 40/300 | Train Loss: 0.1723, Train Acc: 53.02% | Test Loss: 0.4290, Test Acc: 44.86%\n",
      "Epoch 41/300 | Train Loss: 0.1697, Train Acc: 54.99% | Test Loss: 0.4472, Test Acc: 50.88%\n",
      "Epoch 42/300 | Train Loss: 0.1579, Train Acc: 56.00% | Test Loss: 0.4757, Test Acc: 51.40%\n",
      "Epoch 43/300 | Train Loss: 0.1655, Train Acc: 56.52% | Test Loss: 0.4550, Test Acc: 51.09%\n",
      "Epoch 44/300 | Train Loss: 0.1588, Train Acc: 56.81% | Test Loss: 0.4571, Test Acc: 52.13%\n",
      "ðŸ“‰ Learning rate reduced from 0.000512 to 0.000410\n",
      "Epoch 45/300 | Train Loss: 0.1644, Train Acc: 55.69% | Test Loss: 0.4375, Test Acc: 51.40%\n",
      "Epoch 46/300 | Train Loss: 0.1491, Train Acc: 56.79% | Test Loss: 0.4479, Test Acc: 49.64%\n",
      "Epoch 47/300 | Train Loss: 0.1517, Train Acc: 57.30% | Test Loss: 0.4535, Test Acc: 45.74%\n",
      "Epoch 48/300 | Train Loss: 0.1440, Train Acc: 57.66% | Test Loss: 0.4803, Test Acc: 47.87%\n",
      "Epoch 49/300 | Train Loss: 0.1497, Train Acc: 56.45% | Test Loss: 0.4824, Test Acc: 47.66%\n",
      "Epoch 50/300 | Train Loss: 0.1440, Train Acc: 58.23% | Test Loss: 0.5066, Test Acc: 48.39%\n",
      "Epoch 51/300 | Train Loss: 0.1461, Train Acc: 56.88% | Test Loss: 0.4790, Test Acc: 48.34%\n",
      "ðŸ“‰ Learning rate reduced from 0.000410 to 0.000328\n",
      "Epoch 52/300 | Train Loss: 0.1412, Train Acc: 59.04% | Test Loss: 0.4698, Test Acc: 52.18%\n",
      "Epoch 53/300 | Train Loss: 0.1281, Train Acc: 59.51% | Test Loss: 0.5142, Test Acc: 50.21%\n",
      "Epoch 54/300 | Train Loss: 0.1382, Train Acc: 58.80% | Test Loss: 0.4997, Test Acc: 50.05%\n",
      "Epoch 55/300 | Train Loss: 0.1342, Train Acc: 59.36% | Test Loss: 0.5267, Test Acc: 54.52%\n",
      "Epoch 56/300 | Train Loss: 0.1386, Train Acc: 59.79% | Test Loss: 0.5082, Test Acc: 48.49%\n",
      "Epoch 57/300 | Train Loss: 0.1269, Train Acc: 58.51% | Test Loss: 0.5488, Test Acc: 51.19%\n",
      "Epoch 58/300 | Train Loss: 0.1284, Train Acc: 59.93% | Test Loss: 0.5113, Test Acc: 47.98%\n",
      "ðŸ“‰ Learning rate reduced from 0.000328 to 0.000262\n",
      "Epoch 59/300 | Train Loss: 0.1260, Train Acc: 60.57% | Test Loss: 0.5450, Test Acc: 53.01%\n",
      "Epoch 60/300 | Train Loss: 0.1202, Train Acc: 61.29% | Test Loss: 0.5460, Test Acc: 54.98%\n",
      "Epoch 61/300 | Train Loss: 0.1235, Train Acc: 60.84% | Test Loss: 0.5194, Test Acc: 53.69%\n",
      "Epoch 62/300 | Train Loss: 0.1200, Train Acc: 61.40% | Test Loss: 0.5367, Test Acc: 53.12%\n",
      "Epoch 63/300 | Train Loss: 0.1197, Train Acc: 60.72% | Test Loss: 0.5536, Test Acc: 53.58%\n",
      "Epoch 64/300 | Train Loss: 0.1156, Train Acc: 62.51% | Test Loss: 0.5839, Test Acc: 52.54%\n",
      "Epoch 65/300 | Train Loss: 0.1109, Train Acc: 63.15% | Test Loss: 0.5902, Test Acc: 53.79%\n",
      "ðŸ“‰ Learning rate reduced from 0.000262 to 0.000210\n",
      "Epoch 66/300 | Train Loss: 0.1178, Train Acc: 62.33% | Test Loss: 0.5774, Test Acc: 50.83%\n",
      "Epoch 67/300 | Train Loss: 0.1100, Train Acc: 62.66% | Test Loss: 0.5850, Test Acc: 52.23%\n",
      "Epoch 68/300 | Train Loss: 0.1131, Train Acc: 62.75% | Test Loss: 0.5656, Test Acc: 53.17%\n",
      "âœ… Saved new best model at epoch 69 with Test Accuracy: 56.13%\n",
      "Epoch 69/300 | Train Loss: 0.1082, Train Acc: 62.77% | Test Loss: 0.5960, Test Acc: 56.13%\n",
      "âœ… Saved new best model at epoch 70 with Test Accuracy: 57.01%\n",
      "Epoch 70/300 | Train Loss: 0.1090, Train Acc: 61.62% | Test Loss: 0.6012, Test Acc: 57.01%\n",
      "Epoch 71/300 | Train Loss: 0.1065, Train Acc: 63.11% | Test Loss: 0.5972, Test Acc: 56.28%\n",
      "Epoch 72/300 | Train Loss: 0.1052, Train Acc: 63.49% | Test Loss: 0.6139, Test Acc: 55.97%\n",
      "Epoch 73/300 | Train Loss: 0.1086, Train Acc: 62.32% | Test Loss: 0.6361, Test Acc: 53.32%\n",
      "Epoch 74/300 | Train Loss: 0.1095, Train Acc: 61.26% | Test Loss: 0.6293, Test Acc: 56.80%\n",
      "Epoch 75/300 | Train Loss: 0.1045, Train Acc: 63.27% | Test Loss: 0.6472, Test Acc: 52.08%\n",
      "Epoch 76/300 | Train Loss: 0.1014, Train Acc: 63.80% | Test Loss: 0.6529, Test Acc: 55.82%\n",
      "ðŸ“‰ Learning rate reduced from 0.000210 to 0.000168\n",
      "Epoch 77/300 | Train Loss: 0.1041, Train Acc: 63.68% | Test Loss: 0.6502, Test Acc: 53.79%\n",
      "Epoch 78/300 | Train Loss: 0.1068, Train Acc: 63.39% | Test Loss: 0.6373, Test Acc: 56.39%\n",
      "âœ… Saved new best model at epoch 79 with Test Accuracy: 57.48%\n",
      "Epoch 79/300 | Train Loss: 0.0985, Train Acc: 64.22% | Test Loss: 0.6567, Test Acc: 57.48%\n",
      "Epoch 80/300 | Train Loss: 0.1019, Train Acc: 64.93% | Test Loss: 0.6513, Test Acc: 55.19%\n",
      "Epoch 81/300 | Train Loss: 0.0957, Train Acc: 66.02% | Test Loss: 0.6872, Test Acc: 56.07%\n",
      "Epoch 82/300 | Train Loss: 0.0976, Train Acc: 65.52% | Test Loss: 0.6923, Test Acc: 57.42%\n",
      "Epoch 83/300 | Train Loss: 0.0951, Train Acc: 66.37% | Test Loss: 0.6779, Test Acc: 56.18%\n",
      "Epoch 84/300 | Train Loss: 0.1010, Train Acc: 64.98% | Test Loss: 0.6440, Test Acc: 54.47%\n",
      "Epoch 85/300 | Train Loss: 0.0970, Train Acc: 64.91% | Test Loss: 0.6664, Test Acc: 53.06%\n",
      "ðŸ“‰ Learning rate reduced from 0.000168 to 0.000134\n",
      "Epoch 86/300 | Train Loss: 0.0934, Train Acc: 65.29% | Test Loss: 0.6804, Test Acc: 51.97%\n",
      "Epoch 87/300 | Train Loss: 0.0916, Train Acc: 66.12% | Test Loss: 0.6782, Test Acc: 53.53%\n",
      "âœ… Saved new best model at epoch 88 with Test Accuracy: 57.89%\n",
      "Epoch 88/300 | Train Loss: 0.0924, Train Acc: 66.26% | Test Loss: 0.7118, Test Acc: 57.89%\n",
      "Epoch 89/300 | Train Loss: 0.0953, Train Acc: 66.07% | Test Loss: 0.6951, Test Acc: 54.88%\n",
      "Epoch 90/300 | Train Loss: 0.0945, Train Acc: 66.49% | Test Loss: 0.6938, Test Acc: 56.54%\n",
      "Epoch 91/300 | Train Loss: 0.0954, Train Acc: 66.16% | Test Loss: 0.6923, Test Acc: 52.86%\n",
      "âœ… Saved new best model at epoch 92 with Test Accuracy: 58.52%\n",
      "Epoch 92/300 | Train Loss: 0.0906, Train Acc: 66.42% | Test Loss: 0.7161, Test Acc: 58.52%\n",
      "Epoch 93/300 | Train Loss: 0.0882, Train Acc: 67.04% | Test Loss: 0.7201, Test Acc: 56.18%\n",
      "Epoch 94/300 | Train Loss: 0.0908, Train Acc: 66.23% | Test Loss: 0.7160, Test Acc: 56.02%\n",
      "âœ… Saved new best model at epoch 95 with Test Accuracy: 59.50%\n",
      "Epoch 95/300 | Train Loss: 0.0905, Train Acc: 66.97% | Test Loss: 0.6969, Test Acc: 59.50%\n",
      "Epoch 96/300 | Train Loss: 0.0871, Train Acc: 67.42% | Test Loss: 0.7277, Test Acc: 56.70%\n",
      "Epoch 97/300 | Train Loss: 0.0916, Train Acc: 66.38% | Test Loss: 0.7237, Test Acc: 55.97%\n",
      "Epoch 98/300 | Train Loss: 0.0918, Train Acc: 67.77% | Test Loss: 0.7327, Test Acc: 55.87%\n",
      "Epoch 99/300 | Train Loss: 0.0904, Train Acc: 67.37% | Test Loss: 0.7352, Test Acc: 56.39%\n",
      "Epoch 100/300 | Train Loss: 0.0926, Train Acc: 66.59% | Test Loss: 0.7308, Test Acc: 56.85%\n",
      "Epoch 101/300 | Train Loss: 0.0883, Train Acc: 68.08% | Test Loss: 0.7243, Test Acc: 54.26%\n",
      "ðŸ“‰ Learning rate reduced from 0.000134 to 0.000107\n",
      "Epoch 102/300 | Train Loss: 0.0908, Train Acc: 66.80% | Test Loss: 0.7294, Test Acc: 56.54%\n",
      "Epoch 103/300 | Train Loss: 0.0864, Train Acc: 67.37% | Test Loss: 0.7382, Test Acc: 55.19%\n",
      "Epoch 104/300 | Train Loss: 0.0851, Train Acc: 67.49% | Test Loss: 0.7354, Test Acc: 55.92%\n",
      "Epoch 105/300 | Train Loss: 0.0866, Train Acc: 67.11% | Test Loss: 0.7608, Test Acc: 57.01%\n",
      "Epoch 106/300 | Train Loss: 0.0911, Train Acc: 68.03% | Test Loss: 0.7477, Test Acc: 57.17%\n",
      "Epoch 107/300 | Train Loss: 0.0852, Train Acc: 67.79% | Test Loss: 0.7549, Test Acc: 57.94%\n",
      "Epoch 108/300 | Train Loss: 0.0841, Train Acc: 68.05% | Test Loss: 0.7570, Test Acc: 56.59%\n",
      "ðŸ“‰ Learning rate reduced from 0.000107 to 0.000086\n",
      "Epoch 109/300 | Train Loss: 0.0806, Train Acc: 68.65% | Test Loss: 0.7728, Test Acc: 57.63%\n",
      "Epoch 110/300 | Train Loss: 0.0792, Train Acc: 68.89% | Test Loss: 0.7868, Test Acc: 57.22%\n",
      "Epoch 111/300 | Train Loss: 0.0802, Train Acc: 69.69% | Test Loss: 0.7776, Test Acc: 59.19%\n",
      "Epoch 112/300 | Train Loss: 0.0826, Train Acc: 69.88% | Test Loss: 0.7918, Test Acc: 58.52%\n",
      "Epoch 113/300 | Train Loss: 0.0822, Train Acc: 69.45% | Test Loss: 0.7742, Test Acc: 56.70%\n",
      "Epoch 114/300 | Train Loss: 0.0843, Train Acc: 69.10% | Test Loss: 0.7831, Test Acc: 58.26%\n",
      "Epoch 115/300 | Train Loss: 0.0792, Train Acc: 69.21% | Test Loss: 0.7859, Test Acc: 58.93%\n",
      "ðŸ“‰ Learning rate reduced from 0.000086 to 0.000069\n",
      "Epoch 116/300 | Train Loss: 0.0824, Train Acc: 69.12% | Test Loss: 0.7879, Test Acc: 57.63%\n",
      "Epoch 117/300 | Train Loss: 0.0782, Train Acc: 69.05% | Test Loss: 0.7912, Test Acc: 58.00%\n",
      "Epoch 118/300 | Train Loss: 0.0827, Train Acc: 68.93% | Test Loss: 0.7950, Test Acc: 57.68%\n",
      "Epoch 119/300 | Train Loss: 0.0755, Train Acc: 69.17% | Test Loss: 0.8095, Test Acc: 59.40%\n",
      "Epoch 120/300 | Train Loss: 0.0786, Train Acc: 70.09% | Test Loss: 0.8057, Test Acc: 59.35%\n",
      "Epoch 121/300 | Train Loss: 0.0789, Train Acc: 69.43% | Test Loss: 0.8128, Test Acc: 58.10%\n",
      "Epoch 122/300 | Train Loss: 0.0764, Train Acc: 69.55% | Test Loss: 0.8189, Test Acc: 57.68%\n",
      "ðŸ“‰ Learning rate reduced from 0.000069 to 0.000055\n",
      "Epoch 123/300 | Train Loss: 0.0771, Train Acc: 70.43% | Test Loss: 0.8201, Test Acc: 59.29%\n",
      "Epoch 124/300 | Train Loss: 0.0747, Train Acc: 69.85% | Test Loss: 0.8222, Test Acc: 59.09%\n",
      "Epoch 125/300 | Train Loss: 0.0762, Train Acc: 70.68% | Test Loss: 0.8217, Test Acc: 59.14%\n",
      "Epoch 126/300 | Train Loss: 0.0760, Train Acc: 70.26% | Test Loss: 0.8218, Test Acc: 58.26%\n",
      "Epoch 127/300 | Train Loss: 0.0798, Train Acc: 70.50% | Test Loss: 0.8158, Test Acc: 57.27%\n",
      "Epoch 128/300 | Train Loss: 0.0784, Train Acc: 69.27% | Test Loss: 0.8091, Test Acc: 58.88%\n",
      "Epoch 129/300 | Train Loss: 0.0745, Train Acc: 70.66% | Test Loss: 0.8201, Test Acc: 58.67%\n",
      "âœ… Saved new best model at epoch 130 with Test Accuracy: 59.66%\n",
      "Epoch 130/300 | Train Loss: 0.0718, Train Acc: 71.20% | Test Loss: 0.8293, Test Acc: 59.66%\n",
      "Epoch 131/300 | Train Loss: 0.0755, Train Acc: 70.61% | Test Loss: 0.8322, Test Acc: 59.09%\n",
      "Epoch 132/300 | Train Loss: 0.0760, Train Acc: 70.43% | Test Loss: 0.8343, Test Acc: 58.62%\n",
      "Epoch 133/300 | Train Loss: 0.0819, Train Acc: 70.68% | Test Loss: 0.8251, Test Acc: 58.67%\n",
      "Epoch 134/300 | Train Loss: 0.0754, Train Acc: 71.33% | Test Loss: 0.8346, Test Acc: 58.41%\n",
      "âœ… Saved new best model at epoch 135 with Test Accuracy: 59.76%\n",
      "Epoch 135/300 | Train Loss: 0.0732, Train Acc: 70.64% | Test Loss: 0.8361, Test Acc: 59.76%\n",
      "Epoch 136/300 | Train Loss: 0.0761, Train Acc: 70.71% | Test Loss: 0.8445, Test Acc: 58.62%\n",
      "Epoch 137/300 | Train Loss: 0.0765, Train Acc: 70.83% | Test Loss: 0.8469, Test Acc: 59.45%\n",
      "Epoch 138/300 | Train Loss: 0.0727, Train Acc: 71.40% | Test Loss: 0.8502, Test Acc: 59.24%\n",
      "Epoch 139/300 | Train Loss: 0.0744, Train Acc: 70.82% | Test Loss: 0.8567, Test Acc: 58.83%\n",
      "Epoch 140/300 | Train Loss: 0.0762, Train Acc: 71.07% | Test Loss: 0.8524, Test Acc: 58.52%\n",
      "Epoch 141/300 | Train Loss: 0.0739, Train Acc: 71.06% | Test Loss: 0.8467, Test Acc: 58.20%\n",
      "âœ… Saved new best model at epoch 142 with Test Accuracy: 60.85%\n",
      "Epoch 142/300 | Train Loss: 0.0700, Train Acc: 71.59% | Test Loss: 0.8666, Test Acc: 60.85%\n",
      "Epoch 143/300 | Train Loss: 0.0739, Train Acc: 71.33% | Test Loss: 0.8649, Test Acc: 59.61%\n",
      "Epoch 144/300 | Train Loss: 0.0722, Train Acc: 71.47% | Test Loss: 0.8634, Test Acc: 60.23%\n",
      "Epoch 145/300 | Train Loss: 0.0732, Train Acc: 71.56% | Test Loss: 0.8589, Test Acc: 59.61%\n",
      "Epoch 146/300 | Train Loss: 0.0733, Train Acc: 71.61% | Test Loss: 0.8563, Test Acc: 59.40%\n",
      "Epoch 147/300 | Train Loss: 0.0730, Train Acc: 71.25% | Test Loss: 0.8646, Test Acc: 59.55%\n",
      "Epoch 148/300 | Train Loss: 0.0747, Train Acc: 71.51% | Test Loss: 0.8543, Test Acc: 58.98%\n",
      "ðŸ“‰ Learning rate reduced from 0.000055 to 0.000044\n",
      "Epoch 149/300 | Train Loss: 0.0705, Train Acc: 72.15% | Test Loss: 0.8631, Test Acc: 60.28%\n",
      "Epoch 150/300 | Train Loss: 0.0694, Train Acc: 72.41% | Test Loss: 0.8660, Test Acc: 59.92%\n",
      "Epoch 151/300 | Train Loss: 0.0707, Train Acc: 72.62% | Test Loss: 0.8670, Test Acc: 60.02%\n",
      "Epoch 152/300 | Train Loss: 0.0724, Train Acc: 72.11% | Test Loss: 0.8608, Test Acc: 59.97%\n",
      "Epoch 153/300 | Train Loss: 0.0714, Train Acc: 71.78% | Test Loss: 0.8664, Test Acc: 59.92%\n",
      "Epoch 154/300 | Train Loss: 0.0706, Train Acc: 71.70% | Test Loss: 0.8716, Test Acc: 60.54%\n",
      "Epoch 155/300 | Train Loss: 0.0677, Train Acc: 72.79% | Test Loss: 0.8840, Test Acc: 59.97%\n",
      "ðŸ“‰ Learning rate reduced from 0.000044 to 0.000035\n",
      "Epoch 156/300 | Train Loss: 0.0703, Train Acc: 72.20% | Test Loss: 0.8804, Test Acc: 60.70%\n",
      "Epoch 157/300 | Train Loss: 0.0702, Train Acc: 72.55% | Test Loss: 0.8767, Test Acc: 60.70%\n",
      "Epoch 158/300 | Train Loss: 0.0687, Train Acc: 72.93% | Test Loss: 0.8777, Test Acc: 59.87%\n",
      "Epoch 159/300 | Train Loss: 0.0755, Train Acc: 72.10% | Test Loss: 0.8657, Test Acc: 59.14%\n",
      "Epoch 160/300 | Train Loss: 0.0689, Train Acc: 72.63% | Test Loss: 0.8819, Test Acc: 60.59%\n",
      "Epoch 161/300 | Train Loss: 0.0734, Train Acc: 72.98% | Test Loss: 0.8772, Test Acc: 59.97%\n",
      "Epoch 162/300 | Train Loss: 0.0725, Train Acc: 72.46% | Test Loss: 0.8774, Test Acc: 60.12%\n",
      "ðŸ“‰ Learning rate reduced from 0.000035 to 0.000028\n",
      "Epoch 163/300 | Train Loss: 0.0719, Train Acc: 71.89% | Test Loss: 0.8801, Test Acc: 60.18%\n",
      "Epoch 164/300 | Train Loss: 0.0691, Train Acc: 72.46% | Test Loss: 0.8779, Test Acc: 59.61%\n",
      "Epoch 165/300 | Train Loss: 0.0708, Train Acc: 72.49% | Test Loss: 0.8831, Test Acc: 60.59%\n",
      "Epoch 166/300 | Train Loss: 0.0704, Train Acc: 72.36% | Test Loss: 0.8804, Test Acc: 59.92%\n",
      "Epoch 167/300 | Train Loss: 0.0725, Train Acc: 72.06% | Test Loss: 0.8764, Test Acc: 60.33%\n",
      "Epoch 168/300 | Train Loss: 0.0662, Train Acc: 73.57% | Test Loss: 0.8800, Test Acc: 60.54%\n",
      "Epoch 169/300 | Train Loss: 0.0692, Train Acc: 71.82% | Test Loss: 0.8868, Test Acc: 60.85%\n",
      "ðŸ“‰ Learning rate reduced from 0.000028 to 0.000023\n",
      "Epoch 170/300 | Train Loss: 0.0646, Train Acc: 73.26% | Test Loss: 0.8920, Test Acc: 60.44%\n",
      "Epoch 171/300 | Train Loss: 0.0692, Train Acc: 72.68% | Test Loss: 0.8908, Test Acc: 60.44%\n",
      "âœ… Saved new best model at epoch 172 with Test Accuracy: 60.90%\n",
      "Epoch 172/300 | Train Loss: 0.0666, Train Acc: 72.81% | Test Loss: 0.8969, Test Acc: 60.90%\n",
      "Epoch 173/300 | Train Loss: 0.0700, Train Acc: 73.78% | Test Loss: 0.9007, Test Acc: 60.49%\n",
      "Epoch 174/300 | Train Loss: 0.0651, Train Acc: 73.67% | Test Loss: 0.9005, Test Acc: 59.50%\n",
      "Epoch 175/300 | Train Loss: 0.0686, Train Acc: 72.43% | Test Loss: 0.9005, Test Acc: 60.12%\n",
      "Epoch 176/300 | Train Loss: 0.0663, Train Acc: 73.53% | Test Loss: 0.9029, Test Acc: 59.87%\n",
      "Epoch 177/300 | Train Loss: 0.0681, Train Acc: 72.68% | Test Loss: 0.9008, Test Acc: 60.33%\n",
      "Epoch 178/300 | Train Loss: 0.0718, Train Acc: 73.20% | Test Loss: 0.8986, Test Acc: 59.81%\n",
      "ðŸ“‰ Learning rate reduced from 0.000023 to 0.000018\n",
      "Epoch 179/300 | Train Loss: 0.0653, Train Acc: 73.00% | Test Loss: 0.9012, Test Acc: 60.07%\n",
      "Epoch 180/300 | Train Loss: 0.0681, Train Acc: 73.22% | Test Loss: 0.9031, Test Acc: 59.66%\n",
      "Epoch 181/300 | Train Loss: 0.0729, Train Acc: 73.41% | Test Loss: 0.9027, Test Acc: 60.38%\n",
      "Epoch 182/300 | Train Loss: 0.0711, Train Acc: 72.79% | Test Loss: 0.8976, Test Acc: 60.12%\n",
      "Epoch 183/300 | Train Loss: 0.0665, Train Acc: 72.67% | Test Loss: 0.9076, Test Acc: 60.80%\n",
      "Epoch 184/300 | Train Loss: 0.0654, Train Acc: 73.86% | Test Loss: 0.9118, Test Acc: 60.18%\n",
      "Epoch 185/300 | Train Loss: 0.0673, Train Acc: 73.12% | Test Loss: 0.9096, Test Acc: 60.38%\n",
      "ðŸ“‰ Learning rate reduced from 0.000018 to 0.000014\n",
      "Epoch 186/300 | Train Loss: 0.0669, Train Acc: 72.88% | Test Loss: 0.9084, Test Acc: 60.38%\n",
      "Epoch 187/300 | Train Loss: 0.0668, Train Acc: 73.55% | Test Loss: 0.9092, Test Acc: 60.12%\n",
      "Epoch 188/300 | Train Loss: 0.0676, Train Acc: 73.78% | Test Loss: 0.9061, Test Acc: 60.07%\n",
      "Epoch 189/300 | Train Loss: 0.0674, Train Acc: 73.46% | Test Loss: 0.9062, Test Acc: 60.49%\n",
      "Epoch 190/300 | Train Loss: 0.0668, Train Acc: 73.13% | Test Loss: 0.9109, Test Acc: 60.49%\n",
      "Epoch 191/300 | Train Loss: 0.0660, Train Acc: 72.91% | Test Loss: 0.9143, Test Acc: 60.90%\n",
      "Epoch 192/300 | Train Loss: 0.0703, Train Acc: 73.12% | Test Loss: 0.9132, Test Acc: 60.49%\n",
      "ðŸ“‰ Learning rate reduced from 0.000014 to 0.000012\n",
      "Epoch 193/300 | Train Loss: 0.0655, Train Acc: 73.15% | Test Loss: 0.9151, Test Acc: 60.28%\n",
      "Epoch 194/300 | Train Loss: 0.0670, Train Acc: 73.91% | Test Loss: 0.9138, Test Acc: 59.61%\n",
      "Epoch 195/300 | Train Loss: 0.0648, Train Acc: 73.45% | Test Loss: 0.9164, Test Acc: 60.28%\n",
      "Epoch 196/300 | Train Loss: 0.0664, Train Acc: 73.10% | Test Loss: 0.9152, Test Acc: 59.92%\n",
      "Epoch 197/300 | Train Loss: 0.0680, Train Acc: 73.53% | Test Loss: 0.9124, Test Acc: 60.70%\n",
      "Epoch 198/300 | Train Loss: 0.0647, Train Acc: 73.81% | Test Loss: 0.9175, Test Acc: 60.75%\n",
      "Epoch 199/300 | Train Loss: 0.0653, Train Acc: 73.69% | Test Loss: 0.9168, Test Acc: 60.49%\n",
      "ðŸ“‰ Learning rate reduced from 0.000012 to 0.000009\n",
      "Epoch 200/300 | Train Loss: 0.0651, Train Acc: 73.20% | Test Loss: 0.9171, Test Acc: 60.64%\n",
      "Epoch 201/300 | Train Loss: 0.0646, Train Acc: 74.09% | Test Loss: 0.9177, Test Acc: 60.59%\n",
      "Epoch 202/300 | Train Loss: 0.0631, Train Acc: 73.81% | Test Loss: 0.9193, Test Acc: 60.59%\n",
      "Epoch 203/300 | Train Loss: 0.0662, Train Acc: 73.29% | Test Loss: 0.9215, Test Acc: 60.59%\n",
      "Epoch 204/300 | Train Loss: 0.0648, Train Acc: 73.69% | Test Loss: 0.9239, Test Acc: 60.70%\n",
      "Epoch 205/300 | Train Loss: 0.0685, Train Acc: 73.95% | Test Loss: 0.9192, Test Acc: 60.38%\n",
      "Epoch 206/300 | Train Loss: 0.0646, Train Acc: 73.29% | Test Loss: 0.9218, Test Acc: 60.90%\n",
      "ðŸ“‰ Learning rate reduced from 0.000009 to 0.000007\n",
      "Epoch 207/300 | Train Loss: 0.0662, Train Acc: 73.60% | Test Loss: 0.9195, Test Acc: 60.49%\n",
      "Epoch 208/300 | Train Loss: 0.0654, Train Acc: 73.57% | Test Loss: 0.9213, Test Acc: 60.44%\n",
      "Epoch 209/300 | Train Loss: 0.0685, Train Acc: 73.53% | Test Loss: 0.9199, Test Acc: 60.75%\n",
      "âœ… Saved new best model at epoch 210 with Test Accuracy: 60.96%\n",
      "Epoch 210/300 | Train Loss: 0.0654, Train Acc: 74.17% | Test Loss: 0.9215, Test Acc: 60.96%\n",
      "Epoch 211/300 | Train Loss: 0.0692, Train Acc: 73.52% | Test Loss: 0.9209, Test Acc: 60.85%\n",
      "Epoch 212/300 | Train Loss: 0.0667, Train Acc: 73.74% | Test Loss: 0.9207, Test Acc: 60.75%\n",
      "Epoch 213/300 | Train Loss: 0.0655, Train Acc: 73.55% | Test Loss: 0.9218, Test Acc: 60.80%\n",
      "Epoch 214/300 | Train Loss: 0.0664, Train Acc: 73.86% | Test Loss: 0.9220, Test Acc: 60.38%\n",
      "Epoch 215/300 | Train Loss: 0.0639, Train Acc: 73.36% | Test Loss: 0.9240, Test Acc: 60.75%\n",
      "Epoch 216/300 | Train Loss: 0.0671, Train Acc: 73.74% | Test Loss: 0.9242, Test Acc: 60.70%\n",
      "ðŸ“‰ Learning rate reduced from 0.000007 to 0.000006\n",
      "Epoch 217/300 | Train Loss: 0.0665, Train Acc: 73.60% | Test Loss: 0.9249, Test Acc: 60.75%\n",
      "Epoch 218/300 | Train Loss: 0.0650, Train Acc: 74.05% | Test Loss: 0.9234, Test Acc: 60.38%\n",
      "Epoch 219/300 | Train Loss: 0.0675, Train Acc: 73.58% | Test Loss: 0.9234, Test Acc: 60.70%\n",
      "Epoch 220/300 | Train Loss: 0.0651, Train Acc: 74.07% | Test Loss: 0.9252, Test Acc: 60.90%\n",
      "Epoch 221/300 | Train Loss: 0.0653, Train Acc: 73.13% | Test Loss: 0.9242, Test Acc: 60.49%\n",
      "Epoch 222/300 | Train Loss: 0.0665, Train Acc: 73.78% | Test Loss: 0.9230, Test Acc: 60.44%\n",
      "Epoch 223/300 | Train Loss: 0.0686, Train Acc: 73.03% | Test Loss: 0.9225, Test Acc: 60.80%\n",
      "âœ… Saved new best model at epoch 224 with Test Accuracy: 61.21%\n",
      "Epoch 224/300 | Train Loss: 0.0652, Train Acc: 73.78% | Test Loss: 0.9249, Test Acc: 61.21%\n",
      "Epoch 225/300 | Train Loss: 0.0679, Train Acc: 73.12% | Test Loss: 0.9249, Test Acc: 60.80%\n",
      "Epoch 226/300 | Train Loss: 0.0649, Train Acc: 73.60% | Test Loss: 0.9245, Test Acc: 60.80%\n",
      "Epoch 227/300 | Train Loss: 0.0637, Train Acc: 73.00% | Test Loss: 0.9267, Test Acc: 61.01%\n",
      "Epoch 228/300 | Train Loss: 0.0694, Train Acc: 72.77% | Test Loss: 0.9231, Test Acc: 60.64%\n",
      "Epoch 229/300 | Train Loss: 0.0634, Train Acc: 73.90% | Test Loss: 0.9248, Test Acc: 60.70%\n",
      "Epoch 230/300 | Train Loss: 0.0649, Train Acc: 73.81% | Test Loss: 0.9243, Test Acc: 60.59%\n",
      "ðŸ“‰ Learning rate reduced from 0.000006 to 0.000005\n",
      "Epoch 231/300 | Train Loss: 0.0665, Train Acc: 73.38% | Test Loss: 0.9264, Test Acc: 60.64%\n",
      "Epoch 232/300 | Train Loss: 0.0643, Train Acc: 73.71% | Test Loss: 0.9258, Test Acc: 60.85%\n",
      "Epoch 233/300 | Train Loss: 0.0660, Train Acc: 73.17% | Test Loss: 0.9274, Test Acc: 60.80%\n",
      "Epoch 234/300 | Train Loss: 0.0639, Train Acc: 73.33% | Test Loss: 0.9265, Test Acc: 60.49%\n",
      "Epoch 235/300 | Train Loss: 0.0657, Train Acc: 73.57% | Test Loss: 0.9270, Test Acc: 60.80%\n",
      "Epoch 236/300 | Train Loss: 0.0633, Train Acc: 73.97% | Test Loss: 0.9290, Test Acc: 60.90%\n",
      "Epoch 237/300 | Train Loss: 0.0669, Train Acc: 73.41% | Test Loss: 0.9279, Test Acc: 60.80%\n",
      "ðŸ“‰ Learning rate reduced from 0.000005 to 0.000004\n",
      "Epoch 238/300 | Train Loss: 0.0648, Train Acc: 73.45% | Test Loss: 0.9282, Test Acc: 60.64%\n",
      "Epoch 239/300 | Train Loss: 0.0674, Train Acc: 73.71% | Test Loss: 0.9283, Test Acc: 60.64%\n",
      "Epoch 240/300 | Train Loss: 0.0682, Train Acc: 73.78% | Test Loss: 0.9280, Test Acc: 60.64%\n",
      "Epoch 241/300 | Train Loss: 0.0659, Train Acc: 73.86% | Test Loss: 0.9280, Test Acc: 60.75%\n",
      "Epoch 242/300 | Train Loss: 0.0648, Train Acc: 74.16% | Test Loss: 0.9282, Test Acc: 60.54%\n",
      "Epoch 243/300 | Train Loss: 0.0666, Train Acc: 73.76% | Test Loss: 0.9277, Test Acc: 60.49%\n",
      "Epoch 244/300 | Train Loss: 0.0649, Train Acc: 73.52% | Test Loss: 0.9273, Test Acc: 60.54%\n",
      "ðŸ“‰ Learning rate reduced from 0.000004 to 0.000003\n",
      "Epoch 245/300 | Train Loss: 0.0636, Train Acc: 73.62% | Test Loss: 0.9281, Test Acc: 60.70%\n",
      "Epoch 246/300 | Train Loss: 0.0630, Train Acc: 73.88% | Test Loss: 0.9293, Test Acc: 60.75%\n",
      "Epoch 247/300 | Train Loss: 0.0642, Train Acc: 73.48% | Test Loss: 0.9292, Test Acc: 60.75%\n",
      "Epoch 248/300 | Train Loss: 0.0648, Train Acc: 73.97% | Test Loss: 0.9300, Test Acc: 60.80%\n",
      "Epoch 249/300 | Train Loss: 0.0635, Train Acc: 74.28% | Test Loss: 0.9306, Test Acc: 60.75%\n",
      "Epoch 250/300 | Train Loss: 0.0662, Train Acc: 73.81% | Test Loss: 0.9306, Test Acc: 60.70%\n",
      "Epoch 251/300 | Train Loss: 0.0648, Train Acc: 74.00% | Test Loss: 0.9302, Test Acc: 60.85%\n",
      "ðŸ“‰ Learning rate reduced from 0.000003 to 0.000002\n",
      "Epoch 252/300 | Train Loss: 0.0630, Train Acc: 74.03% | Test Loss: 0.9310, Test Acc: 60.85%\n",
      "Epoch 253/300 | Train Loss: 0.0675, Train Acc: 73.69% | Test Loss: 0.9305, Test Acc: 60.90%\n",
      "Epoch 254/300 | Train Loss: 0.0667, Train Acc: 74.10% | Test Loss: 0.9302, Test Acc: 60.85%\n",
      "Epoch 255/300 | Train Loss: 0.0615, Train Acc: 74.21% | Test Loss: 0.9301, Test Acc: 60.85%\n",
      "Epoch 256/300 | Train Loss: 0.0645, Train Acc: 73.58% | Test Loss: 0.9308, Test Acc: 60.80%\n",
      "Epoch 257/300 | Train Loss: 0.0641, Train Acc: 73.78% | Test Loss: 0.9315, Test Acc: 61.01%\n",
      "Epoch 258/300 | Train Loss: 0.0628, Train Acc: 74.28% | Test Loss: 0.9312, Test Acc: 60.96%\n",
      "ðŸ“‰ Learning rate reduced from 0.000002 to 0.000002\n",
      "Epoch 259/300 | Train Loss: 0.0671, Train Acc: 74.36% | Test Loss: 0.9317, Test Acc: 61.01%\n",
      "Epoch 260/300 | Train Loss: 0.0629, Train Acc: 73.93% | Test Loss: 0.9317, Test Acc: 60.85%\n",
      "Epoch 261/300 | Train Loss: 0.0646, Train Acc: 74.05% | Test Loss: 0.9318, Test Acc: 60.75%\n",
      "Epoch 262/300 | Train Loss: 0.0679, Train Acc: 73.84% | Test Loss: 0.9312, Test Acc: 60.85%\n",
      "Epoch 263/300 | Train Loss: 0.0636, Train Acc: 73.97% | Test Loss: 0.9319, Test Acc: 60.80%\n",
      "Epoch 264/300 | Train Loss: 0.0633, Train Acc: 74.05% | Test Loss: 0.9321, Test Acc: 60.80%\n",
      "Epoch 265/300 | Train Loss: 0.0649, Train Acc: 74.19% | Test Loss: 0.9324, Test Acc: 60.90%\n",
      "ðŸ“‰ Learning rate reduced from 0.000002 to 0.000002\n",
      "Epoch 266/300 | Train Loss: 0.0645, Train Acc: 74.33% | Test Loss: 0.9325, Test Acc: 60.90%\n",
      "Epoch 267/300 | Train Loss: 0.0637, Train Acc: 74.49% | Test Loss: 0.9329, Test Acc: 60.90%\n",
      "Epoch 268/300 | Train Loss: 0.0650, Train Acc: 74.23% | Test Loss: 0.9322, Test Acc: 60.90%\n",
      "Epoch 269/300 | Train Loss: 0.0660, Train Acc: 73.95% | Test Loss: 0.9320, Test Acc: 60.85%\n",
      "Epoch 270/300 | Train Loss: 0.0662, Train Acc: 73.38% | Test Loss: 0.9319, Test Acc: 60.85%\n",
      "Epoch 271/300 | Train Loss: 0.0663, Train Acc: 73.62% | Test Loss: 0.9315, Test Acc: 60.80%\n",
      "Epoch 272/300 | Train Loss: 0.0614, Train Acc: 73.93% | Test Loss: 0.9320, Test Acc: 60.80%\n",
      "ðŸ“‰ Learning rate reduced from 0.000002 to 0.000001\n",
      "Epoch 273/300 | Train Loss: 0.0631, Train Acc: 74.29% | Test Loss: 0.9323, Test Acc: 60.80%\n",
      "Epoch 274/300 | Train Loss: 0.0643, Train Acc: 74.07% | Test Loss: 0.9324, Test Acc: 60.80%\n",
      "Epoch 275/300 | Train Loss: 0.0663, Train Acc: 74.12% | Test Loss: 0.9320, Test Acc: 60.75%\n",
      "Epoch 276/300 | Train Loss: 0.0648, Train Acc: 73.53% | Test Loss: 0.9322, Test Acc: 60.80%\n",
      "Epoch 277/300 | Train Loss: 0.0645, Train Acc: 74.52% | Test Loss: 0.9322, Test Acc: 60.80%\n",
      "Epoch 278/300 | Train Loss: 0.0619, Train Acc: 74.14% | Test Loss: 0.9322, Test Acc: 60.85%\n",
      "Epoch 279/300 | Train Loss: 0.0620, Train Acc: 73.90% | Test Loss: 0.9326, Test Acc: 60.85%\n",
      "ðŸ“‰ Learning rate reduced from 0.000001 to 0.000001\n",
      "Epoch 280/300 | Train Loss: 0.0637, Train Acc: 74.14% | Test Loss: 0.9325, Test Acc: 60.90%\n",
      "Epoch 281/300 | Train Loss: 0.0639, Train Acc: 74.23% | Test Loss: 0.9331, Test Acc: 60.85%\n",
      "Epoch 282/300 | Train Loss: 0.0673, Train Acc: 73.46% | Test Loss: 0.9333, Test Acc: 60.80%\n",
      "Epoch 283/300 | Train Loss: 0.0614, Train Acc: 74.55% | Test Loss: 0.9340, Test Acc: 60.80%\n",
      "Epoch 284/300 | Train Loss: 0.0657, Train Acc: 74.14% | Test Loss: 0.9337, Test Acc: 60.80%\n",
      "Epoch 285/300 | Train Loss: 0.0653, Train Acc: 73.74% | Test Loss: 0.9338, Test Acc: 60.85%\n",
      "Epoch 286/300 | Train Loss: 0.0678, Train Acc: 73.88% | Test Loss: 0.9335, Test Acc: 60.90%\n",
      "Epoch 287/300 | Train Loss: 0.0623, Train Acc: 74.52% | Test Loss: 0.9336, Test Acc: 60.85%\n",
      "Epoch 288/300 | Train Loss: 0.0667, Train Acc: 73.76% | Test Loss: 0.9333, Test Acc: 60.90%\n",
      "Epoch 289/300 | Train Loss: 0.0653, Train Acc: 74.17% | Test Loss: 0.9330, Test Acc: 60.85%\n",
      "Epoch 290/300 | Train Loss: 0.0641, Train Acc: 74.10% | Test Loss: 0.9328, Test Acc: 60.85%\n",
      "Epoch 291/300 | Train Loss: 0.0639, Train Acc: 73.65% | Test Loss: 0.9329, Test Acc: 60.80%\n",
      "Epoch 292/300 | Train Loss: 0.0651, Train Acc: 73.41% | Test Loss: 0.9328, Test Acc: 60.80%\n",
      "Epoch 293/300 | Train Loss: 0.0636, Train Acc: 74.29% | Test Loss: 0.9330, Test Acc: 60.85%\n",
      "Epoch 294/300 | Train Loss: 0.0660, Train Acc: 73.98% | Test Loss: 0.9327, Test Acc: 60.80%\n",
      "Epoch 295/300 | Train Loss: 0.0610, Train Acc: 74.12% | Test Loss: 0.9331, Test Acc: 60.85%\n",
      "Epoch 296/300 | Train Loss: 0.0640, Train Acc: 73.95% | Test Loss: 0.9334, Test Acc: 60.90%\n",
      "Epoch 297/300 | Train Loss: 0.0637, Train Acc: 74.66% | Test Loss: 0.9335, Test Acc: 60.96%\n",
      "Epoch 298/300 | Train Loss: 0.0638, Train Acc: 73.95% | Test Loss: 0.9337, Test Acc: 61.01%\n",
      "Epoch 299/300 | Train Loss: 0.0642, Train Acc: 74.14% | Test Loss: 0.9338, Test Acc: 60.90%\n",
      "Epoch 300/300 | Train Loss: 0.0686, Train Acc: 73.41% | Test Loss: 0.9336, Test Acc: 60.90%\n",
      "\n",
      "ðŸŽ¯ Training finished. Best Test Accuracy: 61.21%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train.shape[1]  # number of features\n",
    "model = ResidualMLP(input_size=input_size, num_classes=3).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler (ReduceLROnPlateau equivalent)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.8, patience=6, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Early stopping setup\n",
    "patience = 20\n",
    "best_accuracy = 0.0\n",
    "epochs_no_improve = 0\n",
    "best_model_path = \"TESS_best_residual_mlp_model.pth\"\n",
    "\n",
    "# Training loop\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        \n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    # --- Evaluation on test set ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += batch_y.size(0)\n",
    "            correct_test += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "\n",
    "    # --- Scheduler step ---\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(test_accuracy)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr < old_lr:\n",
    "        print(f\"ðŸ“‰ Learning rate reduced from {old_lr:.6f} to {new_lr:.6f}\")\n",
    "\n",
    "    # --- Save best model ---\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"âœ… Saved new best model at epoch {epoch+1} with Test Accuracy: {best_accuracy:.2f}%\")\n",
    "        epochs_no_improve = 0  # reset patience counter\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # --- Logging ---\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} | \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n",
    "        f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Training finished. Best Test Accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa18b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- Evaluate on test set and generate classification report ---\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Generate report\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6251d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Reverse map: from int â†’ label\n",
    "label_map = {\"FALSE POSITIVE\": 0, \"CANDIDATE\": 1, \"CONFIRMED\": 2}\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Put model in eval mode and collect predictions\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Create readable labels\n",
    "labels = [inv_label_map[i] for i in sorted(inv_label_map.keys())]\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7e98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
