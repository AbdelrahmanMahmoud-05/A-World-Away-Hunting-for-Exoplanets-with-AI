{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ac232977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.calibration import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "77dd3ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4004 entries, 0 to 4003\n",
      "Data columns (total 95 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   loc_rowid        4004 non-null   int64  \n",
      " 1   pl_name          4004 non-null   object \n",
      " 2   hostname         4004 non-null   object \n",
      " 3   default_flag     4004 non-null   int64  \n",
      " 4   disposition      4004 non-null   object \n",
      " 5   disp_refname     4004 non-null   object \n",
      " 6   sy_snum          4004 non-null   int64  \n",
      " 7   sy_pnum          4004 non-null   int64  \n",
      " 8   discoverymethod  4004 non-null   object \n",
      " 9   disc_year        4004 non-null   int64  \n",
      " 10  disc_facility    4004 non-null   object \n",
      " 11  soltype          4004 non-null   object \n",
      " 12  pl_controv_flag  4004 non-null   int64  \n",
      " 13  pl_refname       4004 non-null   object \n",
      " 14  pl_orbper        3960 non-null   float64\n",
      " 15  pl_orbpererr1    3071 non-null   float64\n",
      " 16  pl_orbpererr2    3071 non-null   float64\n",
      " 17  pl_orbperlim     3960 non-null   float64\n",
      " 18  pl_orbsmax       820 non-null    float64\n",
      " 19  pl_orbsmaxerr1   813 non-null    float64\n",
      " 20  pl_orbsmaxerr2   813 non-null    float64\n",
      " 21  pl_orbsmaxlim    820 non-null    float64\n",
      " 22  pl_rade          3173 non-null   float64\n",
      " 23  pl_radeerr1      2887 non-null   float64\n",
      " 24  pl_radeerr2      2887 non-null   float64\n",
      " 25  pl_radelim       3173 non-null   float64\n",
      " 26  pl_radj          3173 non-null   float64\n",
      " 27  pl_radjerr1      2887 non-null   float64\n",
      " 28  pl_radjerr2      2887 non-null   float64\n",
      " 29  pl_radjlim       3173 non-null   float64\n",
      " 30  pl_bmasse        435 non-null    float64\n",
      " 31  pl_bmasseerr1    393 non-null    float64\n",
      " 32  pl_bmasseerr2    393 non-null    float64\n",
      " 33  pl_bmasselim     435 non-null    float64\n",
      " 34  pl_bmassj        435 non-null    float64\n",
      " 35  pl_bmassjerr1    393 non-null    float64\n",
      " 36  pl_bmassjerr2    393 non-null    float64\n",
      " 37  pl_bmassjlim     435 non-null    float64\n",
      " 38  pl_bmassprov     435 non-null    object \n",
      " 39  pl_orbeccen      429 non-null    float64\n",
      " 40  pl_orbeccenerr1  226 non-null    float64\n",
      " 41  pl_orbeccenerr2  226 non-null    float64\n",
      " 42  pl_orbeccenlim   429 non-null    float64\n",
      " 43  pl_insol         630 non-null    float64\n",
      " 44  pl_insolerr1     447 non-null    float64\n",
      " 45  pl_insolerr2     447 non-null    float64\n",
      " 46  pl_insollim      630 non-null    float64\n",
      " 47  pl_eqt           854 non-null    float64\n",
      " 48  pl_eqterr1       685 non-null    float64\n",
      " 49  pl_eqterr2       685 non-null    float64\n",
      " 50  pl_eqtlim        854 non-null    float64\n",
      " 51  ttv_flag         4004 non-null   int64  \n",
      " 52  st_refname       3988 non-null   object \n",
      " 53  st_spectype      437 non-null    object \n",
      " 54  st_teff          2892 non-null   float64\n",
      " 55  st_tefferr1      2584 non-null   float64\n",
      " 56  st_tefferr2      2578 non-null   float64\n",
      " 57  st_tefflim       2892 non-null   float64\n",
      " 58  st_rad           3874 non-null   float64\n",
      " 59  st_raderr1       3242 non-null   float64\n",
      " 60  st_raderr2       3236 non-null   float64\n",
      " 61  st_radlim        3874 non-null   float64\n",
      " 62  st_mass          2102 non-null   float64\n",
      " 63  st_masserr1      1946 non-null   float64\n",
      " 64  st_masserr2      1940 non-null   float64\n",
      " 65  st_masslim       2102 non-null   float64\n",
      " 66  st_met           1698 non-null   float64\n",
      " 67  st_meterr1       1667 non-null   float64\n",
      " 68  st_meterr2       1667 non-null   float64\n",
      " 69  st_metlim        1698 non-null   float64\n",
      " 70  st_metratio      1695 non-null   object \n",
      " 71  st_logg          2356 non-null   float64\n",
      " 72  st_loggerr1      2143 non-null   float64\n",
      " 73  st_loggerr2      2143 non-null   float64\n",
      " 74  st_logglim       2356 non-null   float64\n",
      " 75  sy_refname       4004 non-null   object \n",
      " 76  rastr            4004 non-null   object \n",
      " 77  ra               4004 non-null   float64\n",
      " 78  decstr           4004 non-null   object \n",
      " 79  dec              4004 non-null   float64\n",
      " 80  sy_dist          3879 non-null   float64\n",
      " 81  sy_disterr1      3748 non-null   float64\n",
      " 82  sy_disterr2      3748 non-null   float64\n",
      " 83  sy_vmag          3962 non-null   float64\n",
      " 84  sy_vmagerr1      3962 non-null   float64\n",
      " 85  sy_vmagerr2      3962 non-null   float64\n",
      " 86  sy_kmag          3981 non-null   float64\n",
      " 87  sy_kmagerr1      3973 non-null   float64\n",
      " 88  sy_kmagerr2      3973 non-null   float64\n",
      " 89  sy_gaiamag       3948 non-null   float64\n",
      " 90  sy_gaiamagerr1   3948 non-null   float64\n",
      " 91  sy_gaiamagerr2   3948 non-null   float64\n",
      " 92  rowupdate        4004 non-null   object \n",
      " 93  pl_pubdate       4004 non-null   object \n",
      " 94  releasedate      4004 non-null   object \n",
      "dtypes: float64(70), int64(7), object(18)\n",
      "memory usage: 2.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. Load dataset\n",
    "# -------------------------------\n",
    "df = pd.read_csv('Data\\k2pandc final.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ac386b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. ]Drop name and ID columns\n",
    "# -------------------------------\n",
    "cols_to_drop = [\n",
    "    'loc_rowid','pl_name','hostname','disp_refname','pl_refname','st_refname',\n",
    "    'sy_refname','rowupdate','pl_pubdate','releasedate',\n",
    "    'pl_orbsmax','pl_orbsmaxerr1','pl_orbsmaxerr2','pl_orbsmaxlim',\n",
    "    'pl_bmasse','pl_bmasseerr1','pl_bmasseerr2','pl_bmasselim',\n",
    "    'pl_bmassj','pl_bmassjerr1','pl_bmassjerr2','pl_bmassjlim','pl_bmassprov',\n",
    "    'pl_orbeccen','pl_orbeccenerr1','pl_orbeccenerr2','pl_orbeccenlim',\n",
    "    'pl_insol','pl_insolerr1','pl_insolerr2','pl_insollim',\n",
    "    'pl_eqt','pl_eqterr1','pl_eqterr2','pl_eqtlim',\n",
    "    'st_spectype','st_mass','st_masserr1','st_masserr2','st_masslim',\n",
    "    'st_met','st_meterr1','st_meterr2','st_metlim','st_metratio',\n",
    "    'pl_radj','pl_radjerr1','pl_radjerr2','pl_radjlim',\n",
    "    'pl_orbpererr1','pl_orbpererr2','pl_orbperlim',\n",
    "    'st_tefferr1','st_tefferr2','st_tefflim',\n",
    "    'st_raderr1','st_raderr2','st_radlim',\n",
    "    'sy_vmagerr1','sy_vmagerr2','sy_kmagerr1','sy_kmagerr2',\n",
    "    'sy_gaiamagerr1','sy_gaiamagerr2'\n",
    "]\n",
    "df = df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "9786f101",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2.1 Handle missing values (improved)\n",
    "# -------------------------------\n",
    "missing_pct = df.isnull().mean() * 100\n",
    "\n",
    "# Drop columns with >50% missing values\n",
    "cols_to_drop = missing_pct[missing_pct > 50].index\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Update num_cols and cat_cols after dropping columns\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "cat_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "\n",
    "# Fill numerical columns with median\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "# Fill categorical columns with mode\n",
    "for col in cat_cols:\n",
    "    mode = df[col].mode()\n",
    "    df[col] = df[col].fillna(mode[0] if not mode.empty else 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b61a7aad",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4004 entries, 0 to 4003\n",
      "Data columns (total 31 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   default_flag     4004 non-null   int64  \n",
      " 1   disposition      4004 non-null   object \n",
      " 2   sy_snum          4004 non-null   int64  \n",
      " 3   sy_pnum          4004 non-null   int64  \n",
      " 4   discoverymethod  4004 non-null   object \n",
      " 5   disc_year        4004 non-null   int64  \n",
      " 6   disc_facility    4004 non-null   object \n",
      " 7   soltype          4004 non-null   object \n",
      " 8   pl_controv_flag  4004 non-null   int64  \n",
      " 9   pl_orbper        4004 non-null   float64\n",
      " 10  pl_rade          4004 non-null   float64\n",
      " 11  pl_radeerr1      4004 non-null   float64\n",
      " 12  pl_radeerr2      4004 non-null   float64\n",
      " 13  pl_radelim       4004 non-null   float64\n",
      " 14  ttv_flag         4004 non-null   int64  \n",
      " 15  st_teff          4004 non-null   float64\n",
      " 16  st_rad           4004 non-null   float64\n",
      " 17  st_logg          4004 non-null   float64\n",
      " 18  st_loggerr1      4004 non-null   float64\n",
      " 19  st_loggerr2      4004 non-null   float64\n",
      " 20  st_logglim       4004 non-null   float64\n",
      " 21  rastr            4004 non-null   object \n",
      " 22  ra               4004 non-null   float64\n",
      " 23  decstr           4004 non-null   object \n",
      " 24  dec              4004 non-null   float64\n",
      " 25  sy_dist          4004 non-null   float64\n",
      " 26  sy_disterr1      4004 non-null   float64\n",
      " 27  sy_disterr2      4004 non-null   float64\n",
      " 28  sy_vmag          4004 non-null   float64\n",
      " 29  sy_kmag          4004 non-null   float64\n",
      " 30  sy_gaiamag       4004 non-null   float64\n",
      "dtypes: float64(19), int64(6), object(6)\n",
      "memory usage: 969.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "75259bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Scale numerical features\n",
    "# -------------------------------\n",
    "df_scaled = df.copy()\n",
    "num_cols = df_scaled.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = StandardScaler()\n",
    "df_scaled[num_cols] = scaler.fit_transform(df_scaled[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8f607fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. Encode categorical features\n",
    "# -------------------------------\n",
    "##########\n",
    "df_encoded = df_scaled.copy()\n",
    "cat_cols = df_encoded.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "\n",
    "le_dict = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col].fillna('NaN_Label'))\n",
    "    le_dict[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e3d39068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5. Feature selection (correlation + importance)\n",
    "# -------------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = df_encoded.drop(columns=['disposition'])\n",
    "y = df_encoded['disposition']\n",
    "\n",
    "X = pd.get_dummies(X)  # one-hot if needed\n",
    "X_train, _, y_train, _ = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Correlation\n",
    "corr_with_target = df_encoded.corr(numeric_only=True)['disposition'].drop('disposition').abs()\n",
    "top_corr = corr_with_target.sort_values(ascending=False).head(7).index.tolist()\n",
    "\n",
    "# Random Forest importance\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "top_importance = importances.head(7).index.tolist()\n",
    "\n",
    "# Combine\n",
    "selected_features = list(set(top_corr + top_importance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8acda4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 6. Build preprocessed dataframe\n",
    "# -------------------------------\n",
    "X = df_encoded.drop(columns=['disposition'])\n",
    "valuable_features_df = X[selected_features]\n",
    "\n",
    "# Add target\n",
    "preprocessed_df = valuable_features_df.copy()\n",
    "preprocessed_df['disposition'] = df_encoded['disposition']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c1750bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4004 entries, 0 to 4003\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   pl_radeerr2   4004 non-null   float64\n",
      " 1   sy_vmag       4004 non-null   float64\n",
      " 2   sy_pnum       4004 non-null   float64\n",
      " 3   dec           4004 non-null   float64\n",
      " 4   soltype       4004 non-null   int32  \n",
      " 5   sy_kmag       4004 non-null   float64\n",
      " 6   sy_disterr1   4004 non-null   float64\n",
      " 7   sy_disterr2   4004 non-null   float64\n",
      " 8   default_flag  4004 non-null   float64\n",
      " 9   disc_year     4004 non-null   float64\n",
      " 10  sy_gaiamag    4004 non-null   float64\n",
      " 11  disposition   4004 non-null   int32  \n",
      "dtypes: float64(10), int32(2)\n",
      "memory usage: 344.2 KB\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "1983b5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disposition\n",
       "1    2315\n",
       "0    1374\n",
       "2     293\n",
       "3      22\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df['disposition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9b50ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = preprocessed_df[preprocessed_df['disposition'] != 3].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b6228e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disposition\n",
       "1    2315\n",
       "0    1374\n",
       "2     293\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df['disposition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "121ee947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = preprocessed_df.drop(columns=['disposition'])\n",
    "y = preprocessed_df['disposition']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "43a40552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1 2]\n",
      "Class Weights: tensor([0.9663, 0.5733, 4.5242], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Ensure y_train is numpy\n",
    "if isinstance(y_train, torch.Tensor):\n",
    "    y_train_np = y_train.cpu().numpy()\n",
    "else:\n",
    "    y_train_np = np.array(y_train)\n",
    "\n",
    "# Get unique classes\n",
    "classes = np.unique(y_train_np)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train_np\n",
    ")\n",
    "\n",
    "# Convert to tensor for PyTorch\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "\n",
    "# ---- Focal Loss with class weights ----\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # can be class weights tensor\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)  # probability of true class\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "# ---- Initialize with class weights ----\n",
    "loss_fn = FocalLoss(alpha=class_weights, gamma=2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "410d6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Convert to tensors if not already\n",
    "X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 32  # Adjust based on your data size and memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Compute class weights for imbalance (optional but recommended for recall)\n",
    "class_counts = np.bincount(y_train.numpy())\n",
    "class_weights = 1. / class_counts\n",
    "class_weights = torch.tensor(class_weights / class_weights.sum(), dtype=torch.float32)  # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "508fa912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2986, 11])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7a451748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_rate=0.3):  # <-- fixed __init__\n",
    "        super(ResidualBlock, self).__init__()  # <-- fixed __init__\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.ln = nn.LayerNorm(out_features)  # Layer norm for better stability\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Projection if dimensions differ\n",
    "        self.proj = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = self.proj(x)\n",
    "        x = F.silu(self.ln(self.fc(x)))  # Swish activation\n",
    "        x = self.dropout(x)\n",
    "        return x + residual  # Skip connection\n",
    "\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, input_size=16, num_classes=3):  # <-- fixed __init__\n",
    "        super(ResidualMLP, self).__init__()  # <-- fixed __init__\n",
    "        self.entry = nn.Linear(input_size, 256)  # Entry layer\n",
    "        \n",
    "        # Residual blocks for depth\n",
    "        self.res_block1 = ResidualBlock(256, 256)\n",
    "        self.res_block2 = ResidualBlock(256, 128)\n",
    "        self.res_block3 = ResidualBlock(128, 128)\n",
    "        self.res_block4 = ResidualBlock(128, 64)\n",
    "        \n",
    "        self.dropout_final = nn.Dropout(0.2)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.silu(self.entry(x))\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.res_block4(x)\n",
    "        \n",
    "        x = self.dropout_final(x)\n",
    "        x = self.fc_out(x)  # Logits output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "133dd06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(y_train)  # unique class labels\n",
    "print(\"Classes:\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "6903ae6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1 2]\n",
      "Class Weights: tensor([0.9663, 0.5733, 4.5242], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Ensure y_train is numpy\n",
    "if isinstance(y_train, torch.Tensor):\n",
    "    y_train_np = y_train.cpu().numpy()\n",
    "else:\n",
    "    y_train_np = np.array(y_train)\n",
    "\n",
    "# Get unique classes\n",
    "classes = np.unique(y_train_np)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train_np\n",
    ")\n",
    "\n",
    "# Convert to tensor for PyTorch\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "\n",
    "# ---- Focal Loss with class weights ----\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # can be class weights tensor\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)  # probability of true class\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "# ---- Initialize with class weights ----\n",
    "loss_fn = FocalLoss(alpha=class_weights, gamma=2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "87958dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Number of GPUs: 1\n",
      "Current Device Index: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current Device Index:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e5b06266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pl_radeerr2</th>\n",
       "      <th>sy_vmag</th>\n",
       "      <th>sy_pnum</th>\n",
       "      <th>dec</th>\n",
       "      <th>soltype</th>\n",
       "      <th>sy_kmag</th>\n",
       "      <th>sy_disterr1</th>\n",
       "      <th>sy_disterr2</th>\n",
       "      <th>default_flag</th>\n",
       "      <th>disc_year</th>\n",
       "      <th>sy_gaiamag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.152988</td>\n",
       "      <td>-1.229216</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>1.274274</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.953914</td>\n",
       "      <td>-0.164254</td>\n",
       "      <td>0.188935</td>\n",
       "      <td>-0.906452</td>\n",
       "      <td>-0.761754</td>\n",
       "      <td>-1.142112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.157520</td>\n",
       "      <td>-1.229216</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>1.274274</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.953914</td>\n",
       "      <td>-0.164254</td>\n",
       "      <td>0.188935</td>\n",
       "      <td>-0.906452</td>\n",
       "      <td>-0.761754</td>\n",
       "      <td>-1.142112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.157353</td>\n",
       "      <td>-1.229216</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>1.274274</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.953914</td>\n",
       "      <td>-0.164254</td>\n",
       "      <td>0.188935</td>\n",
       "      <td>1.103202</td>\n",
       "      <td>-0.761754</td>\n",
       "      <td>-1.142112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.159915</td>\n",
       "      <td>-0.764653</td>\n",
       "      <td>-0.815954</td>\n",
       "      <td>-0.498382</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.059170</td>\n",
       "      <td>-0.170415</td>\n",
       "      <td>0.197710</td>\n",
       "      <td>1.103202</td>\n",
       "      <td>0.222931</td>\n",
       "      <td>-0.831989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.156657</td>\n",
       "      <td>-0.764653</td>\n",
       "      <td>-0.815954</td>\n",
       "      <td>-0.498382</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.059170</td>\n",
       "      <td>-0.170415</td>\n",
       "      <td>0.197710</td>\n",
       "      <td>-0.906452</td>\n",
       "      <td>0.222931</td>\n",
       "      <td>-0.831989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>-0.719784</td>\n",
       "      <td>-1.297472</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>0.348257</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.405518</td>\n",
       "      <td>-0.164471</td>\n",
       "      <td>0.189295</td>\n",
       "      <td>-0.906452</td>\n",
       "      <td>-0.761754</td>\n",
       "      <td>-1.281380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>0.160793</td>\n",
       "      <td>-1.535574</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>-0.489871</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.199202</td>\n",
       "      <td>-0.173274</td>\n",
       "      <td>0.201801</td>\n",
       "      <td>1.103202</td>\n",
       "      <td>0.222931</td>\n",
       "      <td>-1.702106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>0.160474</td>\n",
       "      <td>-1.535574</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>-0.489871</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.199202</td>\n",
       "      <td>-0.173274</td>\n",
       "      <td>0.201801</td>\n",
       "      <td>-0.906452</td>\n",
       "      <td>0.222931</td>\n",
       "      <td>-1.702106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>0.160793</td>\n",
       "      <td>-1.535574</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>-0.489871</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.199202</td>\n",
       "      <td>-0.173274</td>\n",
       "      <td>0.201801</td>\n",
       "      <td>-0.906452</td>\n",
       "      <td>0.222931</td>\n",
       "      <td>-1.702106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>0.077437</td>\n",
       "      <td>-1.535574</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>-0.489871</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.199202</td>\n",
       "      <td>-0.173274</td>\n",
       "      <td>0.201801</td>\n",
       "      <td>-0.906452</td>\n",
       "      <td>0.222931</td>\n",
       "      <td>-1.702106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3982 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pl_radeerr2   sy_vmag   sy_pnum       dec  soltype   sy_kmag  \\\n",
       "0        0.152988 -1.229216 -0.091546  1.274274        1 -0.953914   \n",
       "1        0.157520 -1.229216 -0.091546  1.274274        1 -0.953914   \n",
       "2        0.157353 -1.229216 -0.091546  1.274274        1 -0.953914   \n",
       "3        0.159915 -0.764653 -0.815954 -0.498382        0 -1.059170   \n",
       "4        0.156657 -0.764653 -0.815954 -0.498382        0 -1.059170   \n",
       "...           ...       ...       ...       ...      ...       ...   \n",
       "3977    -0.719784 -1.297472 -0.091546  0.348257        0 -1.405518   \n",
       "3978     0.160793 -1.535574 -0.091546 -0.489871        1 -2.199202   \n",
       "3979     0.160474 -1.535574 -0.091546 -0.489871        1 -2.199202   \n",
       "3980     0.160793 -1.535574 -0.091546 -0.489871        1 -2.199202   \n",
       "3981     0.077437 -1.535574 -0.091546 -0.489871        2 -2.199202   \n",
       "\n",
       "      sy_disterr1  sy_disterr2  default_flag  disc_year  sy_gaiamag  \n",
       "0       -0.164254     0.188935     -0.906452  -0.761754   -1.142112  \n",
       "1       -0.164254     0.188935     -0.906452  -0.761754   -1.142112  \n",
       "2       -0.164254     0.188935      1.103202  -0.761754   -1.142112  \n",
       "3       -0.170415     0.197710      1.103202   0.222931   -0.831989  \n",
       "4       -0.170415     0.197710     -0.906452   0.222931   -0.831989  \n",
       "...           ...          ...           ...        ...         ...  \n",
       "3977    -0.164471     0.189295     -0.906452  -0.761754   -1.281380  \n",
       "3978    -0.173274     0.201801      1.103202   0.222931   -1.702106  \n",
       "3979    -0.173274     0.201801     -0.906452   0.222931   -1.702106  \n",
       "3980    -0.173274     0.201801     -0.906452   0.222931   -1.702106  \n",
       "3981    -0.173274     0.201801     -0.906452   0.222931   -1.702106  \n",
       "\n",
       "[3982 rows x 11 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "42bb2b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model at epoch 1 | Test Acc: 91.77%\n",
      "Epoch 1/500 | Train Loss: 0.3067, Train Acc: 88.71% | Test Loss: 0.2066, Test Acc: 91.77%\n",
      "✅ Saved new best model at epoch 2 | Test Acc: 92.07%\n",
      "Epoch 2/500 | Train Loss: 0.2209, Train Acc: 92.06% | Test Loss: 0.2067, Test Acc: 92.07%\n",
      "⏳ No improvement for 1/40 epochs\n",
      "Epoch 3/500 | Train Loss: 0.2097, Train Acc: 92.26% | Test Loss: 0.2130, Test Acc: 91.97%\n",
      "✅ Saved new best model at epoch 4 | Test Acc: 93.37%\n",
      "Epoch 4/500 | Train Loss: 0.1904, Train Acc: 92.67% | Test Loss: 0.1856, Test Acc: 93.37%\n",
      "✅ Saved new best model at epoch 5 | Test Acc: 94.38%\n",
      "Epoch 5/500 | Train Loss: 0.1754, Train Acc: 93.24% | Test Loss: 0.2061, Test Acc: 94.38%\n",
      "⏳ No improvement for 1/40 epochs\n",
      "Epoch 6/500 | Train Loss: 0.1770, Train Acc: 93.00% | Test Loss: 0.1853, Test Acc: 93.88%\n",
      "⏳ No improvement for 2/40 epochs\n",
      "Epoch 7/500 | Train Loss: 0.1692, Train Acc: 93.30% | Test Loss: 0.1631, Test Acc: 94.38%\n",
      "✅ Saved new best model at epoch 8 | Test Acc: 95.28%\n",
      "Epoch 8/500 | Train Loss: 0.1605, Train Acc: 94.07% | Test Loss: 0.1605, Test Acc: 95.28%\n",
      "⏳ No improvement for 1/40 epochs\n",
      "Epoch 9/500 | Train Loss: 0.1483, Train Acc: 94.47% | Test Loss: 0.1726, Test Acc: 93.47%\n",
      "⏳ No improvement for 2/40 epochs\n",
      "Epoch 10/500 | Train Loss: 0.1659, Train Acc: 92.97% | Test Loss: 0.1914, Test Acc: 92.27%\n",
      "⏳ No improvement for 3/40 epochs\n",
      "Epoch 11/500 | Train Loss: 0.1626, Train Acc: 93.64% | Test Loss: 0.1541, Test Acc: 93.98%\n",
      "⏳ No improvement for 4/40 epochs\n",
      "Epoch 12/500 | Train Loss: 0.1500, Train Acc: 94.11% | Test Loss: 0.1460, Test Acc: 94.18%\n",
      "⏳ No improvement for 5/40 epochs\n",
      "Epoch 13/500 | Train Loss: 0.1362, Train Acc: 94.94% | Test Loss: 0.1488, Test Acc: 94.68%\n",
      "⏳ No improvement for 6/40 epochs\n",
      "Epoch 14/500 | Train Loss: 0.1427, Train Acc: 94.57% | Test Loss: 0.1755, Test Acc: 94.08%\n",
      "📉 Learning rate reduced from 0.005000 to 0.004000\n",
      "⏳ No improvement for 7/40 epochs\n",
      "Epoch 15/500 | Train Loss: 0.1642, Train Acc: 94.01% | Test Loss: 0.1423, Test Acc: 94.78%\n",
      "⏳ No improvement for 8/40 epochs\n",
      "Epoch 16/500 | Train Loss: 0.1315, Train Acc: 95.08% | Test Loss: 0.1402, Test Acc: 95.08%\n",
      "⏳ No improvement for 9/40 epochs\n",
      "Epoch 17/500 | Train Loss: 0.1238, Train Acc: 95.14% | Test Loss: 0.1442, Test Acc: 94.08%\n",
      "⏳ No improvement for 10/40 epochs\n",
      "Epoch 18/500 | Train Loss: 0.1273, Train Acc: 95.21% | Test Loss: 0.1476, Test Acc: 94.68%\n",
      "⏳ No improvement for 11/40 epochs\n",
      "Epoch 19/500 | Train Loss: 0.1260, Train Acc: 95.55% | Test Loss: 0.1530, Test Acc: 94.38%\n",
      "⏳ No improvement for 12/40 epochs\n",
      "Epoch 20/500 | Train Loss: 0.1222, Train Acc: 95.34% | Test Loss: 0.1436, Test Acc: 94.98%\n",
      "⏳ No improvement for 13/40 epochs\n",
      "Epoch 21/500 | Train Loss: 0.1197, Train Acc: 95.45% | Test Loss: 0.1911, Test Acc: 92.87%\n",
      "📉 Learning rate reduced from 0.004000 to 0.003200\n",
      "⏳ No improvement for 14/40 epochs\n",
      "Epoch 22/500 | Train Loss: 0.1226, Train Acc: 95.58% | Test Loss: 0.1605, Test Acc: 93.78%\n",
      "⏳ No improvement for 15/40 epochs\n",
      "Epoch 23/500 | Train Loss: 0.1242, Train Acc: 95.45% | Test Loss: 0.1483, Test Acc: 94.58%\n",
      "⏳ No improvement for 16/40 epochs\n",
      "Epoch 24/500 | Train Loss: 0.1263, Train Acc: 95.18% | Test Loss: 0.1340, Test Acc: 95.18%\n",
      "⏳ No improvement for 17/40 epochs\n",
      "Epoch 25/500 | Train Loss: 0.1168, Train Acc: 95.38% | Test Loss: 0.1371, Test Acc: 94.98%\n",
      "⏳ No improvement for 18/40 epochs\n",
      "Epoch 26/500 | Train Loss: 0.1151, Train Acc: 95.41% | Test Loss: 0.1358, Test Acc: 94.88%\n",
      "⏳ No improvement for 19/40 epochs\n",
      "Epoch 27/500 | Train Loss: 0.1110, Train Acc: 96.15% | Test Loss: 0.1462, Test Acc: 94.38%\n",
      "⏳ No improvement for 20/40 epochs\n",
      "Epoch 28/500 | Train Loss: 0.1117, Train Acc: 95.91% | Test Loss: 0.1724, Test Acc: 95.18%\n",
      "📉 Learning rate reduced from 0.003200 to 0.002560\n",
      "⏳ No improvement for 21/40 epochs\n",
      "Epoch 29/500 | Train Loss: 0.1232, Train Acc: 95.34% | Test Loss: 0.1925, Test Acc: 94.88%\n",
      "⏳ No improvement for 22/40 epochs\n",
      "Epoch 30/500 | Train Loss: 0.1161, Train Acc: 95.48% | Test Loss: 0.1559, Test Acc: 94.58%\n",
      "⏳ No improvement for 23/40 epochs\n",
      "Epoch 31/500 | Train Loss: 0.1100, Train Acc: 96.08% | Test Loss: 0.1593, Test Acc: 94.68%\n",
      "⏳ No improvement for 24/40 epochs\n",
      "Epoch 32/500 | Train Loss: 0.1056, Train Acc: 96.08% | Test Loss: 0.1810, Test Acc: 94.18%\n",
      "⏳ No improvement for 25/40 epochs\n",
      "Epoch 33/500 | Train Loss: 0.1082, Train Acc: 96.12% | Test Loss: 0.1492, Test Acc: 94.88%\n",
      "⏳ No improvement for 26/40 epochs\n",
      "Epoch 34/500 | Train Loss: 0.1034, Train Acc: 96.01% | Test Loss: 0.1450, Test Acc: 94.68%\n",
      "⏳ No improvement for 27/40 epochs\n",
      "Epoch 35/500 | Train Loss: 0.1081, Train Acc: 96.22% | Test Loss: 0.1771, Test Acc: 95.08%\n",
      "📉 Learning rate reduced from 0.002560 to 0.002048\n",
      "⏳ No improvement for 28/40 epochs\n",
      "Epoch 36/500 | Train Loss: 0.1077, Train Acc: 96.18% | Test Loss: 0.1639, Test Acc: 94.58%\n",
      "⏳ No improvement for 29/40 epochs\n",
      "Epoch 37/500 | Train Loss: 0.1032, Train Acc: 96.05% | Test Loss: 0.1646, Test Acc: 95.08%\n",
      "⏳ No improvement for 30/40 epochs\n",
      "Epoch 38/500 | Train Loss: 0.1029, Train Acc: 96.01% | Test Loss: 0.1603, Test Acc: 94.18%\n",
      "⏳ No improvement for 31/40 epochs\n",
      "Epoch 39/500 | Train Loss: 0.1019, Train Acc: 95.88% | Test Loss: 0.1658, Test Acc: 94.58%\n",
      "⏳ No improvement for 32/40 epochs\n",
      "Epoch 40/500 | Train Loss: 0.0963, Train Acc: 96.45% | Test Loss: 0.1780, Test Acc: 95.28%\n",
      "⏳ No improvement for 33/40 epochs\n",
      "Epoch 41/500 | Train Loss: 0.0984, Train Acc: 96.48% | Test Loss: 0.1597, Test Acc: 94.58%\n",
      "⏳ No improvement for 34/40 epochs\n",
      "Epoch 42/500 | Train Loss: 0.0962, Train Acc: 96.58% | Test Loss: 0.1629, Test Acc: 94.48%\n",
      "📉 Learning rate reduced from 0.002048 to 0.001638\n",
      "⏳ No improvement for 35/40 epochs\n",
      "Epoch 43/500 | Train Loss: 0.0987, Train Acc: 96.28% | Test Loss: 0.1807, Test Acc: 94.88%\n",
      "⏳ No improvement for 36/40 epochs\n",
      "Epoch 44/500 | Train Loss: 0.0934, Train Acc: 96.48% | Test Loss: 0.1642, Test Acc: 94.88%\n",
      "⏳ No improvement for 37/40 epochs\n",
      "Epoch 45/500 | Train Loss: 0.1001, Train Acc: 96.45% | Test Loss: 0.1885, Test Acc: 94.58%\n",
      "⏳ No improvement for 38/40 epochs\n",
      "Epoch 46/500 | Train Loss: 0.1045, Train Acc: 96.22% | Test Loss: 0.1489, Test Acc: 95.08%\n",
      "⏳ No improvement for 39/40 epochs\n",
      "Epoch 47/500 | Train Loss: 0.0998, Train Acc: 96.45% | Test Loss: 0.1639, Test Acc: 94.68%\n",
      "⏳ No improvement for 40/40 epochs\n",
      "🚨 Early stopping triggered at epoch 48\n",
      "🎯 Best Test Accuracy: 95.28%\n",
      "\n",
      "🎯 Training finished.\n",
      "Best Test Accuracy: 95.28%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train.shape[1]  # number of features\n",
    "model = ResidualMLP(input_size=input_size, num_classes=3).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.005, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.8, patience=6, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Early stopping setup\n",
    "patience = 40\n",
    "best_accuracy = 0.0\n",
    "epochs_no_improve = 0\n",
    "best_model_path = \"K2_best_residual_mlp_model.pth\"\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training parameters\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    model.eval()\n",
    "    test_loss, correct_test, total_test = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += batch_y.size(0)\n",
    "            correct_test += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "\n",
    "    # --- Scheduler step (manual print) ---\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(test_accuracy)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr < old_lr:\n",
    "        print(f\"📉 Learning rate reduced from {old_lr:.6f} to {new_lr:.6f}\")\n",
    "\n",
    "    # --- Early stopping check ---\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"✅ Saved new best model at epoch {epoch+1} | Test Acc: {best_accuracy:.2f}%\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"⏳ No improvement for {epochs_no_improve}/{patience} epochs\")\n",
    "    \n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"🚨 Early stopping triggered at epoch {epoch+1}\")\n",
    "        print(f\"🎯 Best Test Accuracy: {best_accuracy:.2f}%\")\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        break\n",
    "\n",
    "    # --- Logging ---\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} | \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n",
    "        f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\"\n",
    "    )\n",
    "\n",
    "print(\"\\n🎯 Training finished.\")\n",
    "print(f\"Best Test Accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4ff6c252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9114    0.9564    0.9333       344\n",
      "           1     0.9914    1.0000    0.9957       579\n",
      "           2     0.8039    0.5616    0.6613        73\n",
      "\n",
      "    accuracy                         0.9528       996\n",
      "   macro avg     0.9022    0.8393    0.8634       996\n",
      "weighted avg     0.9500    0.9528    0.9497       996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- Evaluate on test set and generate classification report ---\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Generate report\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6491681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
